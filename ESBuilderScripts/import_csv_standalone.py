# import_csv_standalone.py
import pandas as pd
import numpy as np
from elasticsearch import Elasticsearch
from tqdm import tqdm
import hashlib
import json

# é…ç½®ï¼ˆå…³é”®ï¼šé€‚é…ä½ çš„CSVåˆ—åï¼‰
CSV_FILE_PATH = "./translation_pairs.csv"  # ä½ çš„CSVè·¯å¾„
ES_HOST = "http://localhost:9200"
INDEX_NAME = "zh_en_translation_memory"
BATCH_SIZE = 10000
# æ˜ å°„ï¼šä½ çš„CSVåˆ—å â†’ è„šæœ¬éœ€è¦çš„åˆ—å
SOURCE_COL = "source_text"  # æºæ–‡æœ¬ï¼ˆè‹±æ–‡ï¼‰
TARGET_COL = "target_text"  # ç›®æ ‡æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰
ID_KEY = SOURCE_COL  # ä»¥æºæ–‡æœ¬åˆ—ä¸ºåŸºå‡†ç”ŸæˆID

# 1. è¿æ¥ES
es = Elasticsearch(ES_HOST, 
        # timeout=30,
        )
if not es.ping():
    raise Exception("æ— æ³•è¿æ¥åˆ°Elasticsearchï¼Œè¯·æ£€æŸ¥å®¹å™¨æ˜¯å¦è¿è¡Œ")

# 2. æ–‡æœ¬æ¸…æ´—å‡½æ•°ï¼ˆæç®€ç‰ˆï¼Œå’Œé¡¹ç›®é€»è¾‘å¯¹é½ï¼‰
def clean_text(text):
    if pd.isna(text):
        return ""
    import re
    text = re.sub(r'\s+', ' ', text).strip()  # å¤šä¸ªç©ºæ ¼è½¬ä¸€ä¸ª
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)  # ä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼
    return text

def is_noise(text):
    if pd.isna(text) or len(text) < 3:
        return True
    import re
    if re.match(r'^\d+$', text):  # çº¯æ•°å­—åˆ¤å®šä¸ºå™ªéŸ³
        return True
    return False

# 3. è¯»å–å¹¶è¿‡æ»¤CSV
try:
    # è¯»å–CSVï¼ˆç¼–ç ä¸å¯¹çš„è¯æ”¹æˆencoding="gbk"ï¼‰
    df = pd.read_csv(CSV_FILE_PATH, encoding="utf-8")
    print(f"âœ… æˆåŠŸè¯»å–CSVï¼Œå…± {len(df)} è¡Œæ•°æ®")
except Exception as e:
    print(f"è¯»å–CSVå¤±è´¥ï¼š{e}")
    exit(1)

# éªŒè¯æ ¸å¿ƒåˆ—æ˜¯å¦å­˜åœ¨
required_cols = {SOURCE_COL, TARGET_COL}
if not required_cols.intersection(df.columns):
    raise ValueError(
        f"CSVå¿…é¡»åŒ…å« '{SOURCE_COL}' å’Œ '{TARGET_COL}' åˆ—ï¼Œå½“å‰åˆ—ï¼š{df.columns}\n"
        "å¦‚æœåˆ—åå¯¹åº”é”™è¯¯ï¼Œè¯·ä¿®æ”¹è„šæœ¬é‡Œçš„ SOURCE_COL/TARGET_COL é…ç½®ï¼"
    )

# é‡å‘½ååˆ—ï¼ˆé€‚é…åç»­é€»è¾‘ï¼ŒæŠŠsource_textâ†’enï¼Œtarget_textâ†’zhï¼‰
df.rename(columns={SOURCE_COL: "en", TARGET_COL: "zh"}, inplace=True)
ID_KEY = "en"  # é‡å‘½ååIDåŸºå‡†åˆ—æ”¹ä¸ºen

# è¿‡æ»¤æ— æ•ˆæ•°æ®
df = df.dropna(subset=["en", "zh"], how="all")  # ç§»é™¤ä¸­è‹±æ–‡éƒ½ä¸ºç©ºçš„è¡Œ
df = df.drop_duplicates(subset=[ID_KEY])  # åŸºäºè‹±æ–‡åˆ—å»é‡
df[ID_KEY] = df[ID_KEY].apply(clean_text)  # æ¸…æ´—è‹±æ–‡æ–‡æœ¬
df = df[~df[ID_KEY].map(is_noise)]  # ç§»é™¤å™ªéŸ³æ–‡æœ¬
df["id_key"] = ID_KEY  # è®°å½•IDåŸºå‡†åˆ—

# è¿‡æ»¤åæ•°æ®æ£€æŸ¥
if len(df) == 0:
    print("âš ï¸ è¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥CSVå†…å®¹ï¼")
    exit(0)
print(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(df)} æ¡æœ‰æ•ˆæ•°æ®")

# 4. æ‰¹é‡å¯¼å…¥ES
def upsert_doc(record):
    """ç”ŸæˆESçš„upsertæŒ‡ä»¤ï¼ˆå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰"""
    # åŸºäºè‹±æ–‡åˆ—ç”Ÿæˆå”¯ä¸€ID
    doc_id = hashlib.sha1(record[ID_KEY].encode('utf8')).hexdigest()
    # ç§»é™¤ç©ºå€¼å­—æ®µ
    record = {k: v for k, v in record.items() if not pd.isna(v)}
    # ç”ŸæˆæŒ‡ä»¤
    yield json.dumps({"update": {"_index": INDEX_NAME, "_id": doc_id, "retry_on_conflict": 3}})
    yield json.dumps({"doc": record, "doc_as_upsert": True})

# åˆ†æ‰¹å¯¼å…¥ï¼ˆé¿å…ä¸€æ¬¡æ€§å¯¼å…¥è¿‡å¤šæ•°æ®ï¼‰
batch_idx = np.array_split(range(len(df)), max(int(len(df)/BATCH_SIZE), 1))
for idx in tqdm(batch_idx, desc="å¯¼å…¥CSVæ•°æ®åˆ°ES"):
    batch_df = df.iloc[idx]
    bulk_data = []
    for _, row in batch_df.iterrows():
        bulk_data.extend(list(upsert_doc(row.to_dict())))
    # æ‰§è¡Œæ‰¹é‡å¯¼å…¥
    try:
        response = es.bulk(body=bulk_data, index=INDEX_NAME)
        if response.get("errors"):
            print(f"âš ï¸ è¯¥æ‰¹æ¬¡å­˜åœ¨å¯¼å…¥é”™è¯¯ï¼š{response['errors']}")
    except Exception as e:
        print(f"âŒ è¯¥æ‰¹æ¬¡å¯¼å…¥å¤±è´¥ï¼š{e}")
        continue

print(f"\nğŸ‰ æ•°æ®å¯¼å…¥å®Œæˆï¼å…±å¯¼å…¥ {len(df)} æ¡ä¸­è‹±æ–‡ç¿»è¯‘æ•°æ®åˆ°ESç´¢å¼• {INDEX_NAME}")
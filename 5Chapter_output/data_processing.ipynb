{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c865b4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLTKæ•°æ®é›†è·¯å¾„å·²æŒ‡å®šä¸ºï¼šC:\\Users\\UTU\\AppData\\Roaming\\nltk_data\n",
      "âœ… æ‰€æœ‰NLTKæ•°æ®é›†å‡å­˜åœ¨ï¼\n",
      "âœ… spaCyæ¨¡å‹en_core_web_småŠ è½½æˆåŠŸ\n",
      "=====================================\n",
      "å¼€å§‹å¤„ç†ã€ŠAI: A Modern Approachã€‹4thç‰ˆ - ä»…æå–å‰äº”ç« ï¼\n",
      "ç›®æ ‡ç« èŠ‚ï¼š[1, 2, 3, 4, 5]\n",
      "è¾“å…¥æ–‡ä»¶ï¼š./AIBookEnglish.txt\n",
      "=====================================\n",
      "\n",
      "[15:14:04] æ­¥éª¤3ï¼šè¯»å–TXTæ–‡ä»¶ï¼š./AIBookEnglish.txtï¼Œä»…æå–å‰äº”ç« ï¼ˆ[1, 2, 3, 4, 5]ï¼‰\n",
      "[15:14:04] æ­¥éª¤3ï¼šâœ… å·²è·³è¿‡å‰ç½®å†…å®¹ï¼Œä»CHAPTER 1å¼€å§‹å¤„ç†\n",
      "[15:14:04] æ­¥éª¤3ï¼šğŸ“š å‰äº”ç« æ‰€åœ¨æ–‡æœ¬æœ‰æ•ˆå­—ç¬¦æ•°ï¼š3209012ï¼ˆéœ€â‰¥20000ï¼‰\n",
      "[15:14:04] æ­¥éª¤4ï¼šâš ï¸  ç« èŠ‚1æ­£åˆ™åŒ¹é…å†…å®¹ä¸è¶³ï¼Œè§¦å‘å…œåº•æˆªå–\n",
      "[15:14:04] æ­¥éª¤4ï¼šâœ… ç« èŠ‚1æœ€ç»ˆï¼š1ä¸ªæ®µè½ï¼Œ105113å­—ç¬¦\n",
      "[15:14:04] æ­¥éª¤4ï¼šâš ï¸  ç« èŠ‚2æ­£åˆ™åŒ¹é…å†…å®¹ä¸è¶³ï¼Œè§¦å‘å…œåº•æˆªå–\n",
      "[15:14:04] æ­¥éª¤4ï¼šâœ… ç« èŠ‚2æœ€ç»ˆï¼š1ä¸ªæ®µè½ï¼Œ72852å­—ç¬¦\n",
      "[15:14:04] æ­¥éª¤4ï¼šâš ï¸  ç« èŠ‚3æ­£åˆ™åŒ¹é…å†…å®¹ä¸è¶³ï¼Œè§¦å‘å…œåº•æˆªå–\n",
      "[15:14:04] æ­¥éª¤4ï¼šâœ… ç« èŠ‚3æœ€ç»ˆï¼š1ä¸ªæ®µè½ï¼Œ126113å­—ç¬¦\n",
      "[15:14:04] æ­¥éª¤4ï¼šâš ï¸  ç« èŠ‚4æ­£åˆ™åŒ¹é…å†…å®¹ä¸è¶³ï¼Œè§¦å‘å…œåº•æˆªå–\n",
      "[15:14:04] æ­¥éª¤4ï¼šâœ… ç« èŠ‚4æœ€ç»ˆï¼š1ä¸ªæ®µè½ï¼Œ92808å­—ç¬¦\n",
      "[15:14:04] æ­¥éª¤4ï¼šâš ï¸  ç« èŠ‚5æ­£åˆ™åŒ¹é…å†…å®¹ä¸è¶³ï¼Œè§¦å‘å…œåº•æˆªå–\n",
      "[15:14:05] æ­¥éª¤4ï¼šâœ… ç« èŠ‚5æœ€ç»ˆï¼š1ä¸ªæ®µè½ï¼Œ77976å­—ç¬¦\n",
      "[15:14:05] æ­¥éª¤4ï¼šâœ… å‰äº”ç« å…¨éƒ¨æå–æˆåŠŸï¼\n",
      "\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚1ï¼ˆChapter 1: INTRODUCTIONï¼‰...\n",
      "\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚2ï¼ˆChapter 2: INTELLIGENT AGENTSï¼‰...\n",
      "\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚3ï¼ˆChapter 3: SOLVING PROBLEMS BY SEARCHINGï¼‰...\n",
      "\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚4ï¼ˆChapter 4: SEARCH IN COMPLEX ENVIRONMENTSï¼‰...\n",
      "\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚5ï¼ˆChapter 5: CONSTRAINT SATISFACTION PROBLEMSï¼‰...\n",
      "\n",
      "æ­¥éª¤3/6ï¼šä¿å­˜å‰äº”ç« é¢„å¤„ç†æˆæœ...\n",
      "âœ… å‰äº”ç« å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ï¼špreprocess_chap1-5.json\n",
      "âœ… å‰äº”ç« æœ¯è¯­æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼šterminology_chap1-5.csv\n",
      "âœ… å‰äº”ç« å‘½åå®ä½“æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼šnamed_entity_chap1-5.csv\n",
      "âœ… å‰äº”ç« é£æ ¼åˆ†æè¡¨å·²ä¿å­˜è‡³ï¼šstyle_analysis_chap1-5.csv\n",
      "âœ… å‰äº”ç« æ–‡åŒ–è´Ÿè½½è¯è¡¨å·²ä¿å­˜è‡³ï¼šcultural_word_chap1-5.csv\n",
      "âœ… å‰äº”ç« ç¿»è¯‘å•å…ƒè¡¨å·²ä¿å­˜è‡³ï¼štranslation_units_chap1-5.csv\n",
      "\n",
      "ğŸ‰ å‰äº”ç« é¢„å¤„ç†å®Œæˆï¼å…¨å±€ç»Ÿè®¡ï¼š\n",
      "  - å¤„ç†ç« èŠ‚ï¼š[1, 2, 3, 4, 5]\n",
      "  - æ€»å­—ç¬¦æ•°ï¼š474862\n",
      "  - ç¿»è¯‘å•å…ƒæ•°ï¼ˆæ®µè½ï¼‰ï¼š5\n",
      "  - æå–æœ¯è¯­æ•°ï¼š6104\n",
      "  - å‘½åå®ä½“æ•°ï¼š2525\n",
      "  - æ–‡åŒ–è´Ÿè½½è¯æ•°ï¼š1\n",
      "\n",
      "ğŸ“ è¾“å‡ºæ–‡ä»¶æ¸…å•ï¼š\n",
      "  - preprocess_chap1-5.jsonï¼ˆå®Œæ•´ç»“æ„åŒ–æ•°æ®ï¼‰\n",
      "  - terminology_chap1-5.csvï¼ˆæœ¯è¯­æ ¡éªŒè¡¨ï¼‰\n",
      "  - named_entity_chap1-5.csvï¼ˆå‘½åå®ä½“æ ¡éªŒè¡¨ï¼‰\n",
      "  - style_analysis_chap1-5.csvï¼ˆé£æ ¼åˆ†æè¡¨ï¼‰\n",
      "  - cultural_word_chap1-5.csvï¼ˆæ–‡åŒ–è´Ÿè½½è¯è¡¨ï¼‰\n",
      "  - translation_units_chap1-5.csvï¼ˆç¿»è¯‘å•å…ƒè¡¨ï¼‰\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒé…ç½®ï¼šä»…æå–å‰äº”ç«  + ç²¾ç¡®ç« èŠ‚åæ˜ å°„ --------------------------\n",
    "INPUT_TXT = \"./AIBookEnglish.txt\"  # æ›¿æ¢ä¸ºä½ çš„TXTæ–‡ä»¶å®é™…è·¯å¾„\n",
    "TARGET_CHAPTERS = [1, 2, 3, 4, 5]  # å›ºå®šæå–å‰äº”ç« \n",
    "MIN_TEXT_LENGTH = 20000  # å•ç« æœ€å°æœ‰æ•ˆå­—ç¬¦æ•°\n",
    "# å‰äº”ç« ç²¾ç¡®ç« èŠ‚åæ˜ å°„ï¼ˆæ¥è‡ªæ–‡æ¡£Contentsï¼‰\n",
    "CHAPTER_TITLES = {\n",
    "    1: \"INTRODUCTION\",\n",
    "    2: \"INTELLIGENT AGENTS\",\n",
    "    3: \"SOLVING PROBLEMS BY SEARCHING\",\n",
    "    4: \"SEARCH IN COMPLEX ENVIRONMENTS\",\n",
    "    5: \"CONSTRAINT SATISFACTION PROBLEMS\"\n",
    "}\n",
    "# è¾“å‡ºæ–‡ä»¶ï¼ˆå‡æ ‡æ³¨chap1-5ï¼Œé¿å…æ··æ·†ï¼‰\n",
    "PREPROCESS_JSON = \"preprocess_chap1-5.json\"\n",
    "TERM_TABLE = \"terminology_chap1-5.csv\"\n",
    "NER_TABLE = \"named_entity_chap1-5.csv\"\n",
    "STYLE_METADATA = \"style_analysis_chap1-5.csv\"\n",
    "CULTURAL_WORD_TABLE = \"cultural_word_chap1-5.csv\"\n",
    "TRANSLATION_UNITS = \"translation_units_chap1-5.csv\"\n",
    "\n",
    "# -------------------------- NLTKé…ç½®ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "NLTK_DATA_DIR = \"C:\\\\Users\\\\UTU\\\\AppData\\\\Roaming\\\\nltk_data\"\n",
    "os.environ[\"NLTK_DATA\"] = NLTK_DATA_DIR\n",
    "nltk.data.path = [NLTK_DATA_DIR]\n",
    "print(f\"âœ… NLTKæ•°æ®é›†è·¯å¾„å·²æŒ‡å®šä¸ºï¼š{NLTK_DATA_DIR}\")\n",
    "\n",
    "required_datasets = {\n",
    "    \"stopwords\": os.path.join(NLTK_DATA_DIR, \"corpora\", \"stopwords\", \"english\"),\n",
    "    \"wordnet\": os.path.join(NLTK_DATA_DIR, \"corpora\", \"wordnet\"),\n",
    "    \"punkt\": os.path.join(NLTK_DATA_DIR, \"tokenizers\", \"punkt\", \"PY3\", \"english.pickle\"),\n",
    "    \"averaged_perceptron_tagger\": os.path.join(NLTK_DATA_DIR, \"taggers\", \"averaged_perceptron_tagger\", \"averaged_perceptron_tagger.pickle\")\n",
    "}\n",
    "\n",
    "for data_name, data_path in required_datasets.items():\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"âŒ æœªæ‰¾åˆ°{NLTK_DATA_DIR}ä¸‹çš„{data_name}æ•°æ®é›†ï¼\\n\"\n",
    "            f\"è¯·ç¡®è®¤{data_path}æ–‡ä»¶/æ–‡ä»¶å¤¹å­˜åœ¨\"\n",
    "        )\n",
    "print(\"âœ… æ‰€æœ‰NLTKæ•°æ®é›†å‡å­˜åœ¨ï¼\")\n",
    "\n",
    "# -------------------------- åˆå§‹åŒ–æ ¸å¿ƒä¾èµ–ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "try:\n",
    "    nltk.pos_tag(word_tokenize(\"test sentence\"))\n",
    "except Exception as e:\n",
    "    raise Exception(f\"âŒ è¯æ€§æ ‡æ³¨æ•°æ®é›†åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ï¼š\\n{NLTK_DATA_DIR}\\\\taggers\\\\averaged_perceptron_tagger\\\\averaged_perceptron_tagger.pickle\")\n",
    "\n",
    "# åˆå§‹åŒ–spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"âœ… spaCyæ¨¡å‹en_core_web_småŠ è½½æˆåŠŸ\")\n",
    "except OSError:\n",
    "    raise Exception(\"âŒ spaCyæ¨¡å‹æœªå®‰è£…ï¼è¯·å…ˆè¿è¡Œï¼špython -m spacy download en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"âŒ spaCyåˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}\")\n",
    "\n",
    "# -------------------------- è¾…åŠ©å‡½æ•°ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "def roman_numeral(n):\n",
    "    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n",
    "    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n",
    "    roman_num = ''\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        for _ in range(n // val[i]):\n",
    "            roman_num += syb[i]\n",
    "            n -= val[i]\n",
    "        i += 1\n",
    "    return roman_num\n",
    "\n",
    "def clean_and_normalize_text(text):\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r' ([.,;:!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,;:!?])', r'\\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def split_into_logical_paragraphs(text, chap_num):\n",
    "    clean_text = clean_and_normalize_text(text)\n",
    "    raw_paragraphs = re.split(r'\\n\\s*\\n+', clean_text)\n",
    "    logical_paras = []\n",
    "    para_idx = 0\n",
    "    for raw_para in raw_paragraphs:\n",
    "        para = raw_para.strip()\n",
    "        if len(para) < 5:\n",
    "            continue\n",
    "        sentences = sent_tokenize(para)\n",
    "        if len(sentences) == 1 and len(para.split()) < 10 and logical_paras:\n",
    "            last_para = logical_paras[-1]\n",
    "            merged_content = last_para[\"content\"] + \" \" + para\n",
    "            merged_sentences = sent_tokenize(merged_content)\n",
    "            logical_paras[-1] = {\n",
    "                \"para_id\": last_para[\"para_id\"],\n",
    "                \"content\": merged_content,\n",
    "                \"sentence_count\": len(merged_sentences),\n",
    "                \"prev_para_id\": last_para[\"prev_para_id\"],\n",
    "                \"next_para_id\": f\"chap{chap_num}_para{para_idx+2}\" if (para_idx+2) < len(raw_paragraphs) else \"None\",\n",
    "                \"char_count\": len(merged_content.replace(' ', ''))\n",
    "            }\n",
    "            continue\n",
    "        para_id = f\"chap{chap_num}_para{para_idx+1}\"\n",
    "        prev_para_id = f\"chap{chap_num}_para{para_idx}\" if para_idx > 0 else \"None\"\n",
    "        next_para_id = f\"chap{chap_num}_para{para_idx+2}\" if (para_idx+2) <= len(raw_paragraphs) else \"None\"\n",
    "        logical_paras.append({\n",
    "            \"para_id\": para_id,\n",
    "            \"content\": para,\n",
    "            \"sentence_count\": len(sentences),\n",
    "            \"prev_para_id\": prev_para_id,\n",
    "            \"next_para_id\": next_para_id,\n",
    "            \"char_count\": len(para.replace(' ', ''))\n",
    "        })\n",
    "        para_idx += 1\n",
    "    return logical_paras\n",
    "\n",
    "def log_step(step, msg):\n",
    "    timestamp = time.strftime('%H:%M:%S')\n",
    "    print(f\"[{timestamp}] æ­¥éª¤{step}ï¼š{msg}\")\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒä¿®æ”¹ï¼šTXTæå–å‡½æ•°ï¼ˆå«ç« èŠ‚åæ ¡éªŒï¼‰ --------------------------\n",
    "def extract_txt_chapters(txt_path, target_chapters, MIN_TEXT_LENGTH=20000):\n",
    "    log_step(3, f\"è¯»å–TXTæ–‡ä»¶ï¼š{txt_path}ï¼Œä»…æå–å‰äº”ç« ï¼ˆ{target_chapters}ï¼‰\")\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(txt_path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "            full_text = f.read()\n",
    "    \n",
    "    # è·³è¿‡å‰ç½®å†…å®¹ï¼Œä¸¥æ ¼åŒ¹é…å…¨å¤§å†™CHAPTER 1\n",
    "    chapter1_match = re.search(r'^CHAPTER\\s+1', full_text, re.MULTILINE)\n",
    "    if chapter1_match:\n",
    "        full_text = full_text[chapter1_match.start():]\n",
    "        log_step(3, \"âœ… å·²è·³è¿‡å‰ç½®å†…å®¹ï¼Œä»CHAPTER 1å¼€å§‹å¤„ç†\")\n",
    "    else:\n",
    "        raise ValueError(\"âŒ æœªæ‰¾åˆ°CHAPTER 1ï¼Œæ£€æŸ¥æ–‡æ¡£æ ¼å¼æ˜¯å¦æ­£ç¡®\")\n",
    "\n",
    "    total_len = len(full_text.replace(' ', ''))\n",
    "    log_step(3, f\"ğŸ“š å‰äº”ç« æ‰€åœ¨æ–‡æœ¬æœ‰æ•ˆå­—ç¬¦æ•°ï¼š{total_len}ï¼ˆéœ€â‰¥{MIN_TEXT_LENGTH}ï¼‰\")\n",
    "    if total_len < MIN_TEXT_LENGTH:\n",
    "        raise ValueError(f\"æ–‡æœ¬è§„æ¨¡ä¸è¾¾æ ‡ï¼å½“å‰{total_len}å­—ç¬¦ï¼Œéœ€â‰¥{MIN_TEXT_LENGTH}å­—ç¬¦\")\n",
    "\n",
    "    chapters_data = defaultdict(dict)\n",
    "    for chap_num in target_chapters:\n",
    "        chap_content = \"\"\n",
    "        chap_title = f\"Chapter {chap_num}: {CHAPTER_TITLES.get(chap_num, 'Unknown')}\"\n",
    "\n",
    "        # ä¼˜åŒ–åçš„æ­£åˆ™ï¼šä¸¥æ ¼åŒ¹é…å…¨å¤§å†™CHAPTERï¼Œå®Œæ•´æ•è·å†…å®¹\n",
    "        patterns = [\n",
    "            rf'^CHAPTER\\s+{chap_num}\\s*([^\\n]*)\\n+([\\s\\S]*?)(?=^CHAPTER\\s+{chap_num+1}|$)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
    "            if match:\n",
    "                sub_title = match.group(1).strip().upper()\n",
    "                temp_content = match.group(2).strip()\n",
    "                # æ ¡éªŒå†…å®¹æœ‰æ•ˆæ€§ï¼šé¿å…ç¢ç‰‡å†…å®¹\n",
    "                if len(temp_content.replace(' ', '')) < 5000:\n",
    "                    continue\n",
    "                chap_content = temp_content\n",
    "\n",
    "                # æ ‡é¢˜æ ¡éªŒé€»è¾‘\n",
    "                if chap_num in CHAPTER_TITLES:\n",
    "                    expected_title = CHAPTER_TITLES[chap_num].upper()\n",
    "                    if sub_title != expected_title:\n",
    "                        chap_title = f\"Chapter {chap_num}: {CHAPTER_TITLES[chap_num]}\"\n",
    "                        log_step(4, f\"âš ï¸  ä¿®æ­£ç« èŠ‚{chap_num}æ ‡é¢˜ï¼š{sub_title} â†’ {CHAPTER_TITLES[chap_num]}\")\n",
    "                    else:\n",
    "                        chap_title = f\"Chapter {chap_num}: {sub_title.title()}\"\n",
    "                else:\n",
    "                    chap_title = f\"Chapter {chap_num}: {sub_title}\" if sub_title else f\"Chapter {chap_num}\"\n",
    "                \n",
    "                log_step(4, f\"âœ… åŒ¹é…ç« èŠ‚{chap_num}ï¼šæ ‡é¢˜={chap_title}ï¼Œå†…å®¹é•¿åº¦={len(chap_content.replace(' ', ''))}å­—ç¬¦\")\n",
    "                break\n",
    "\n",
    "        # å…œåº•æˆªå–ï¼ˆä¸¥æ ¼åŒ¹é…å…¨å¤§å†™CHAPTERï¼‰\n",
    "        if not chap_content or len(chap_content.replace(' ', '')) < 5000:\n",
    "            log_step(4, f\"âš ï¸  ç« èŠ‚{chap_num}æ­£åˆ™åŒ¹é…å†…å®¹ä¸è¶³ï¼Œè§¦å‘å…œåº•æˆªå–\")\n",
    "            current_chap_mark = re.search(rf'^CHAPTER\\s+{chap_num}', full_text, re.MULTILINE)\n",
    "            next_chap_mark = re.search(rf'^CHAPTER\\s+{chap_num+1}', full_text, re.MULTILINE)\n",
    "            if current_chap_mark:\n",
    "                start_pos = current_chap_mark.end()\n",
    "                end_pos = next_chap_mark.start() if next_chap_mark else len(full_text)\n",
    "                chap_content = full_text[start_pos:end_pos].strip()\n",
    "                chap_title = f\"Chapter {chap_num}: {CHAPTER_TITLES.get(chap_num, f'Chapter {chap_num}')}\"\n",
    "\n",
    "        # æ®µè½åˆ†å‰²\n",
    "        paragraphs = split_into_logical_paragraphs(chap_content, chap_num)\n",
    "        chapters_data[chap_num] = {\n",
    "            \"title\": chap_title,\n",
    "            \"raw_content\": chap_content,\n",
    "            \"paragraphs\": paragraphs,\n",
    "            \"para_count\": len(paragraphs),\n",
    "            \"char_count\": len(chap_content.replace(' ', ''))\n",
    "        }\n",
    "        log_step(4, f\"âœ… ç« èŠ‚{chap_num}æœ€ç»ˆï¼š{len(paragraphs)}ä¸ªæ®µè½ï¼Œ{len(chap_content.replace(' ', ''))}å­—ç¬¦\")\n",
    "\n",
    "    # æ ¡éªŒå‰äº”ç« æå–æˆåŠŸ\n",
    "    missing_chaps = [chap for chap in target_chapters if chap not in chapters_data]\n",
    "    if missing_chaps:\n",
    "        raise ValueError(f\"âŒ ä»¥ä¸‹ç« èŠ‚æœªæå–åˆ°ï¼š{missing_chaps}\")\n",
    "    log_step(4, f\"âœ… å‰äº”ç« å…¨éƒ¨æå–æˆåŠŸï¼\")\n",
    "    return chapters_data\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½ï¼šè¯­åŸŸé£æ ¼è¯†åˆ«ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "def analyze_style_and_domain(para_content, para_id):\n",
    "    ai_domain_keywords = {\n",
    "        \"machine learning\", \"neural network\", \"reinforcement learning\", \"bayesian network\",\n",
    "        \"convolutional neural network\", \"attention mechanism\", \"backpropagation\",\n",
    "        \"markov decision process\", \"perceptron\", \"rational agent\"\n",
    "    }\n",
    "    formal_style_keywords = {\"furthermore\", \"nevertheless\", \"consequently\", \"therefore\", \"moreover\", \"however\"}\n",
    "    \n",
    "    sentences = sent_tokenize(para_content)\n",
    "    sent_count = len(sentences)\n",
    "    words = word_tokenize(para_content.lower())\n",
    "    word_count = len(words)\n",
    "    avg_sent_length = round(word_count / sent_count, 2) if sent_count > 0 else 0\n",
    "    \n",
    "    passive_count = len(re.findall(r'\\bis\\s+\\w+ed\\b|\\bwas\\s+\\w+ed\\b|\\bwere\\s+\\w+ed\\b', para_content.lower()))\n",
    "    passive_ratio = round(passive_count / word_count, 3) if word_count > 0 else 0\n",
    "    \n",
    "    text_word_set = set(words)\n",
    "    domain_match = text_word_set & ai_domain_keywords\n",
    "    domain = \"AI_Professional\" if len(domain_match) >= 3 else \"AI_Popular\"\n",
    "    \n",
    "    formal_match = text_word_set & formal_style_keywords\n",
    "    formality_score = 7.0 + len(formal_match)*0.5 + (1 if passive_ratio >= 0.05 else 0) + (1 if avg_sent_length >= 25 else 0)\n",
    "    formality_score = min(formality_score, 10.0)\n",
    "    style = \"Formal_Academic\" if formality_score >= 8.0 else \"Semi_Formal_Academic\"\n",
    "    \n",
    "    return {\n",
    "        \"para_id\": para_id,\n",
    "        \"domain\": domain,\n",
    "        \"style\": style,\n",
    "        \"formality_score\": round(formality_score, 1),\n",
    "        \"avg_sentence_length\": avg_sent_length,\n",
    "        \"passive_voice_ratio\": passive_ratio,\n",
    "        \"domain_keywords\": list(domain_match),\n",
    "        \"formal_keywords\": list(formal_match),\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sent_count\n",
    "    }\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½ï¼šæœ¯è¯­æå–ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "def extract_terminology(para_content):\n",
    "    tokens = word_tokenize(para_content.lower())\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens \n",
    "        if token not in STOP_WORDS and not token.isdigit() and len(token) >= 3\n",
    "    ]\n",
    "    \n",
    "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "    valid_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\n",
    "    valid_tokens = [lemmatizer.lemmatize(token) for token, pos in pos_tags if pos in valid_pos_tags]\n",
    "    \n",
    "    ai_core_terms = {\n",
    "        \"rational agent\", \"bayesian network\", \"machine learning\", \"neural network\",\n",
    "        \"reinforcement learning\", \"markov decision process\", \"perceptron\",\n",
    "        \"attention mechanism\", \"backpropagation\", \"convolutional neural network\"\n",
    "    }\n",
    "    for term in ai_core_terms:\n",
    "        valid_tokens.append(term)\n",
    "    \n",
    "    return Counter(valid_tokens)\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½ï¼šå‘½åå®ä½“æå–ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "def extract_named_entities(para_content, para_id):\n",
    "    doc = nlp(para_content)\n",
    "    named_entities = []\n",
    "    target_entity_types = [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\", \"PRODUCT\", \"EVENT\"]\n",
    "    \n",
    "    sentences = sent_tokenize(para_content)\n",
    "    sent_positions = []\n",
    "    current_pos = 0\n",
    "    for sent in sentences:\n",
    "        sent_start = current_pos\n",
    "        sent_end = current_pos + len(sent)\n",
    "        sent_positions.append({\n",
    "            \"sentence\": sent,\n",
    "            \"start\": sent_start,\n",
    "            \"end\": sent_end\n",
    "        })\n",
    "        current_pos = sent_end + 1\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in target_entity_types:\n",
    "            entity_sentence = \"\"\n",
    "            for sent in sent_positions:\n",
    "                if sent[\"start\"] <= ent.start_char <= sent[\"end\"]:\n",
    "                    entity_sentence = sent[\"sentence\"]\n",
    "                    break\n",
    "            \n",
    "            named_entities.append({\n",
    "                \"para_id\": para_id,\n",
    "                \"entity_text\": ent.text,\n",
    "                \"entity_type\": ent.label_,\n",
    "                \"start_char\": ent.start_char,\n",
    "                \"end_char\": ent.end_char,\n",
    "                \"context\": entity_sentence\n",
    "            })\n",
    "    return named_entities\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½ï¼šæ–‡åŒ–è´Ÿè½½è¯æå–ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "def extract_cultural_words(para_content, para_id):\n",
    "    cultural_word_lib = {\n",
    "        \"state-of-the-art\": {\"semantic_explanation\": \"AIé¢†åŸŸæŒ‡å½“å‰æœ€é«˜æŠ€æœ¯æ°´å¹³\", \"translation_strategy\": \"æ„è¯‘â€˜å‰æ²¿æŠ€æœ¯â€™\"},\n",
    "        \"holy grail\": {\"semantic_explanation\": \"æ¯”å–»é¢†åŸŸç»ˆæç›®æ ‡\", \"translation_strategy\": \"æ„è¯‘â€˜åœ£æ¯ï¼ˆç»ˆæç›®æ ‡ï¼‰â€™\"},\n",
    "        \"trial and error\": {\"semantic_explanation\": \"AIè®­ç»ƒä¸­çš„è¯•é”™æ–¹æ³•\", \"translation_strategy\": \"å›ºå®šè¯‘æ³•â€˜è¯•é”™æ³•â€™\"},\n",
    "        \"game-changer\": {\"semantic_explanation\": \"é¢ è¦†æ€§æŠ€æœ¯/æ–¹æ³•\", \"translation_strategy\": \"æ„è¯‘â€˜å˜é©æ€§æŠ€æœ¯â€™\"}\n",
    "    }\n",
    "    \n",
    "    extracted = []\n",
    "    for word, info in cultural_word_lib.items():\n",
    "        if re.search(r'\\b' + re.escape(word) + r'\\b', para_content, re.IGNORECASE):\n",
    "            match = re.search(r'\\b' + re.escape(word) + r'\\b', para_content, re.IGNORECASE)\n",
    "            \n",
    "            sentences = sent_tokenize(para_content)\n",
    "            sent_positions = []\n",
    "            current_pos = 0\n",
    "            for sent in sentences:\n",
    "                sent_start = current_pos\n",
    "                sent_end = current_pos + len(sent)\n",
    "                sent_positions.append((sent_start, sent_end, sent))\n",
    "                current_pos = sent_end + 1\n",
    "            \n",
    "            entity_sentence = para_content[max(0, match.start() - 50):min(len(para_content), match.end() + 50)]\n",
    "            for sent_start, sent_end, sent in sent_positions:\n",
    "                if sent_start <= match.start() <= sent_end:\n",
    "                    entity_sentence = sent\n",
    "                    break\n",
    "            \n",
    "            extracted.append({\n",
    "                \"para_id\": para_id,\n",
    "                \"cultural_word\": word,\n",
    "                \"semantic_explanation\": info[\"semantic_explanation\"],\n",
    "                \"translation_strategy\": info[\"translation_strategy\"],\n",
    "                \"context\": entity_sentence\n",
    "            })\n",
    "    return extracted\n",
    "\n",
    "# -------------------------- è¾…åŠ©å‡½æ•°ï¼šsetè½¬listï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ --------------------------\n",
    "def convert_set_to_list(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_set_to_list(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "        return [convert_set_to_list(item) for item in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½ï¼šä¿å­˜é¢„å¤„ç†æˆæœï¼ˆæ ‡æ³¨å‰äº”ç« ï¼‰ --------------------------\n",
    "def save_preprocess_results(final_data):\n",
    "    data_to_save = copy.deepcopy(final_data)\n",
    "    data_to_save = convert_set_to_list(data_to_save)\n",
    "    \n",
    "    # 1. å®Œæ•´ç»“æ„åŒ–æ•°æ®\n",
    "    with open(PREPROCESS_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_to_save, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… å‰äº”ç« å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ï¼š{PREPROCESS_JSON}\")\n",
    "\n",
    "    # 2. æœ¯è¯­æ ¡éªŒè¡¨\n",
    "    with open(TERM_TABLE, 'w', encoding='utf-8', newline='') as f:\n",
    "        fieldnames = [\"æœ¯è¯­\", \"å‡ºç°æ¬¡æ•°\", \"é¢†åŸŸåˆ†ç±»\", \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        ai_core_terms = final_data[\"ai_core_terms\"]\n",
    "        for term, count in sorted(final_data[\"global_terminology\"].items(), key=lambda x: -x[1]):\n",
    "            domain = \"AIä¸“ä¸šæœ¯è¯­\" if term in ai_core_terms else \"é€šç”¨æœ¯è¯­\"\n",
    "            writer.writerow({\n",
    "                \"æœ¯è¯­\": term, \"å‡ºç°æ¬¡æ•°\": count, \"é¢†åŸŸåˆ†ç±»\": domain,\n",
    "                \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\": \"\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\": \"\"\n",
    "            })\n",
    "    print(f\"âœ… å‰äº”ç« æœ¯è¯­æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼š{TERM_TABLE}\")\n",
    "\n",
    "    # 3. å‘½åå®ä½“æ ¡éªŒè¡¨\n",
    "    all_entities = []\n",
    "    entity_counter = Counter()\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            for ent in para[\"named_entities\"]:\n",
    "                all_entities.append(ent)\n",
    "                entity_counter[(ent[\"entity_text\"], ent[\"entity_type\"])] += 1\n",
    "    unique_entities = []\n",
    "    seen = set()\n",
    "    for ent in all_entities:\n",
    "        key = (ent[\"entity_text\"], ent[\"entity_type\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            ent[\"å‡ºç°æ¬¡æ•°\"] = entity_counter[key]\n",
    "            unique_entities.append(ent)\n",
    "    with open(NER_TABLE, 'w', encoding='utf-8', newline='') as f:\n",
    "        fieldnames = [\"å®ä½“åŸæ–‡\", \"å®ä½“ç±»å‹\", \"å‡ºç°æ¬¡æ•°\", \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\", \"ä¸Šä¸‹æ–‡\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for ent in unique_entities:\n",
    "            writer.writerow({\n",
    "                \"å®ä½“åŸæ–‡\": ent[\"entity_text\"], \"å®ä½“ç±»å‹\": ent[\"entity_type\"], \"å‡ºç°æ¬¡æ•°\": ent[\"å‡ºç°æ¬¡æ•°\"],\n",
    "                \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\": \"\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\": \"\", \"ä¸Šä¸‹æ–‡\": ent[\"context\"]\n",
    "            })\n",
    "    print(f\"âœ… å‰äº”ç« å‘½åå®ä½“æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼š{NER_TABLE}\")\n",
    "\n",
    "    # 4. é£æ ¼åˆ†æè¡¨\n",
    "    style_data = []\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            style_meta = para[\"style_metadata\"]\n",
    "            style_data.append({\n",
    "                \"æ®µè½ID\": style_meta[\"para_id\"], \"ç« èŠ‚å·\": chap[\"chapter_num\"],\n",
    "                \"é¢†åŸŸ\": style_meta[\"domain\"], \"è¯­ä½“é£æ ¼\": style_meta[\"style\"],\n",
    "                \"æ­£å¼åº¦è¯„åˆ†\": style_meta[\"formality_score\"], \"å¹³å‡å¥é•¿\": style_meta[\"avg_sentence_length\"],\n",
    "                \"è¢«åŠ¨è¯­æ€å æ¯”\": style_meta[\"passive_voice_ratio\"], \"é¢†åŸŸå…³é”®è¯\": \",\".join(style_meta[\"domain_keywords\"])\n",
    "            })\n",
    "    with open(STYLE_METADATA, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=style_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(style_data)\n",
    "    print(f\"âœ… å‰äº”ç« é£æ ¼åˆ†æè¡¨å·²ä¿å­˜è‡³ï¼š{STYLE_METADATA}\")\n",
    "\n",
    "    # 5. æ–‡åŒ–è´Ÿè½½è¯è¡¨\n",
    "    all_cultural = []\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            all_cultural.extend(para[\"cultural_words\"])\n",
    "    with open(CULTURAL_WORD_TABLE, 'w', encoding='utf-8', newline='') as f:\n",
    "        fieldnames = [\"æ–‡åŒ–è´Ÿè½½è¯\", \"è¯­ä¹‰è§£é‡Š\", \"ç¿»è¯‘ç­–ç•¥\", \"ä¸Šä¸‹æ–‡\", \"æœ€ç»ˆè¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\", \"æ ¡éªŒç»“æœ\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for item in all_cultural:\n",
    "            csv_row = {\n",
    "                \"æ–‡åŒ–è´Ÿè½½è¯\": item[\"cultural_word\"],\n",
    "                \"è¯­ä¹‰è§£é‡Š\": item[\"semantic_explanation\"],\n",
    "                \"ç¿»è¯‘ç­–ç•¥\": item[\"translation_strategy\"],\n",
    "                \"ä¸Šä¸‹æ–‡\": item[\"context\"],\n",
    "                \"æœ€ç»ˆè¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\": \"\",\n",
    "                \"æ ¡éªŒç»“æœ\": \"\"\n",
    "            }\n",
    "            writer.writerow(csv_row)\n",
    "    print(f\"âœ… å‰äº”ç« æ–‡åŒ–è´Ÿè½½è¯è¡¨å·²ä¿å­˜è‡³ï¼š{CULTURAL_WORD_TABLE}\")\n",
    "\n",
    "    # 6. ç¿»è¯‘å•å…ƒè¡¨\n",
    "    translation_units = []\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            para_terms = [t for t, _ in extract_terminology(para[\"content\"]).most_common(3)]\n",
    "            para_ents = [ent[\"entity_text\"] for ent in para[\"named_entities\"]]\n",
    "            translation_units.append({\n",
    "                \"å•å…ƒID\": para[\"para_id\"], \"ç« èŠ‚å·\": chap[\"chapter_num\"],\n",
    "                \"åŸæ–‡\": para[\"content\"], \"ä¸Šä¸‹æ–‡å…³è”\": f\"å‰ï¼š{para['prev_para_id']} | åï¼š{para['next_para_id']}\",\n",
    "                \"é¢†åŸŸ\": para[\"style_metadata\"][\"domain\"], \"è¯­ä½“è¦æ±‚\": para[\"style_metadata\"][\"style\"],\n",
    "                \"å…³é”®æœ¯è¯­\": \",\".join(para_terms), \"å‘½åå®ä½“\": \",\".join(para_ents) if para_ents else \"æ— \",\n",
    "                \"Agentè¯‘æ–‡\": \"\", \"äººå·¥ä¿®æ­£\": \"\"\n",
    "            })\n",
    "    with open(TRANSLATION_UNITS, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=translation_units[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(translation_units)\n",
    "    print(f\"âœ… å‰äº”ç« ç¿»è¯‘å•å…ƒè¡¨å·²ä¿å­˜è‡³ï¼š{TRANSLATION_UNITS}\")\n",
    "\n",
    "# -------------------------- ä¸»å‡½æ•°ï¼ˆä»…å¤„ç†å‰äº”ç« ï¼‰ --------------------------\n",
    "def main():\n",
    "    print(f\"=====================================\")\n",
    "    print(f\"å¼€å§‹å¤„ç†ã€ŠAI: A Modern Approachã€‹4thç‰ˆ - ä»…æå–å‰äº”ç« ï¼\")\n",
    "    print(f\"ç›®æ ‡ç« èŠ‚ï¼š{TARGET_CHAPTERS}\")\n",
    "    print(f\"è¾“å…¥æ–‡ä»¶ï¼š{INPUT_TXT}\")\n",
    "    print(f\"=====================================\\n\")\n",
    "    \n",
    "    # æå–å‰äº”ç« æ•°æ®\n",
    "    chapters_data = extract_txt_chapters(INPUT_TXT, TARGET_CHAPTERS)\n",
    "    \n",
    "    # åˆå§‹åŒ–å…¨å±€æ•°æ®ç»“æ„\n",
    "    final_data = {\n",
    "        \"book_info\": \"Artificial Intelligence: A Modern Approach (4th Ed, English) - å‰äº”ç« \",\n",
    "        \"target_chapters\": TARGET_CHAPTERS,\n",
    "        \"global_metadata\": {\n",
    "            \"total_characters\": 0, \"total_paragraphs\": 0, \"total_terms\": 0,\n",
    "            \"total_entities\": 0, \"total_cultural_words\": 0,\n",
    "            \"dominant_domain\": \"AI_Professional\", \"dominant_style\": \"Formal_Academic\"\n",
    "        },\n",
    "        \"chapters\": [],\n",
    "        \"global_terminology\": {},\n",
    "        \"ai_core_terms\": {\"rational agent\", \"bayesian network\", \"machine learning\", \"neural network\",\n",
    "                          \"reinforcement learning\", \"markov decision process\", \"perceptron\",\n",
    "                          \"attention mechanism\", \"backpropagation\", \"convolutional neural network\"}\n",
    "    }\n",
    "    \n",
    "    # å¤„ç†æ¯ä¸ªç« èŠ‚çš„å¤šç»´åº¦ä¿¡æ¯\n",
    "    global_terms = Counter()\n",
    "    global_entity_count = 0\n",
    "    global_cultural_count = 0\n",
    "    \n",
    "    for chap_num in TARGET_CHAPTERS:\n",
    "        chap_data = chapters_data[chap_num]\n",
    "        print(f\"\\næ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚{chap_num}ï¼ˆ{chap_data['title']}ï¼‰...\")\n",
    "        processed_paras = []\n",
    "        chap_content = chap_data[\"raw_content\"]\n",
    "        \n",
    "        for para in chap_data[\"paragraphs\"]:\n",
    "            para_id = para[\"para_id\"]\n",
    "            para_text = para[\"content\"]\n",
    "            \n",
    "            # æå–é£æ ¼ã€æœ¯è¯­ã€å®ä½“ã€æ–‡åŒ–è´Ÿè½½è¯\n",
    "            style_meta = analyze_style_and_domain(para_text, para_id)\n",
    "            para_terms = extract_terminology(para_text)\n",
    "            para_ents = extract_named_entities(para_text, para_id)\n",
    "            para_cultural = extract_cultural_words(para_text, para_id)\n",
    "            \n",
    "            # æ±‡æ€»å…¨å±€æ•°æ®\n",
    "            global_terms.update(para_terms)\n",
    "            global_entity_count += len(para_ents)\n",
    "            global_cultural_count += len(para_cultural)\n",
    "            \n",
    "            processed_paras.append({\n",
    "                \"para_id\": para_id, \"content\": para_text,\n",
    "                \"sentence_count\": para[\"sentence_count\"], \"char_count\": para[\"char_count\"],\n",
    "                \"prev_para_id\": para[\"prev_para_id\"], \"next_para_id\": para[\"next_para_id\"],\n",
    "                \"style_metadata\": style_meta, \"terminology\": dict(para_terms),\n",
    "                \"named_entities\": para_ents, \"cultural_words\": para_cultural\n",
    "            })\n",
    "        \n",
    "        # ä¿å­˜ç« èŠ‚æ•°æ®\n",
    "        final_data[\"chapters\"].append({\n",
    "            \"chapter_num\": chap_num, \"title\": chap_data[\"title\"],\n",
    "            \"paragraphs\": processed_paras, \"para_count\": len(processed_paras),\n",
    "            \"char_count\": chap_data[\"char_count\"]\n",
    "        })\n",
    "        final_data[\"global_metadata\"][\"total_characters\"] += chap_data[\"char_count\"]\n",
    "        final_data[\"global_metadata\"][\"total_paragraphs\"] += len(processed_paras)\n",
    "    \n",
    "    # å¡«å……å…¨å±€å…ƒæ•°æ®\n",
    "    final_data[\"global_terminology\"] = dict(global_terms)\n",
    "    final_data[\"global_metadata\"][\"total_terms\"] = len(global_terms)\n",
    "    final_data[\"global_metadata\"][\"total_entities\"] = global_entity_count\n",
    "    final_data[\"global_metadata\"][\"total_cultural_words\"] = global_cultural_count\n",
    "    \n",
    "    # ä¿å­˜æ‰€æœ‰æˆæœ\n",
    "    print(f\"\\næ­¥éª¤3/6ï¼šä¿å­˜å‰äº”ç« é¢„å¤„ç†æˆæœ...\")\n",
    "    save_preprocess_results(final_data)\n",
    "    \n",
    "    # è¾“å‡ºç»Ÿè®¡æ‘˜è¦\n",
    "    print(f\"\\nğŸ‰ å‰äº”ç« é¢„å¤„ç†å®Œæˆï¼å…¨å±€ç»Ÿè®¡ï¼š\")\n",
    "    print(f\"  - å¤„ç†ç« èŠ‚ï¼š{TARGET_CHAPTERS}\")\n",
    "    print(f\"  - æ€»å­—ç¬¦æ•°ï¼š{final_data['global_metadata']['total_characters']}\")\n",
    "    print(f\"  - ç¿»è¯‘å•å…ƒæ•°ï¼ˆæ®µè½ï¼‰ï¼š{final_data['global_metadata']['total_paragraphs']}\")\n",
    "    print(f\"  - æå–æœ¯è¯­æ•°ï¼š{final_data['global_metadata']['total_terms']}\")\n",
    "    print(f\"  - å‘½åå®ä½“æ•°ï¼š{final_data['global_metadata']['total_entities']}\")\n",
    "    print(f\"  - æ–‡åŒ–è´Ÿè½½è¯æ•°ï¼š{final_data['global_metadata']['total_cultural_words']}\")\n",
    "    print(f\"\\nğŸ“ è¾“å‡ºæ–‡ä»¶æ¸…å•ï¼š\")\n",
    "    print(f\"  - {PREPROCESS_JSON}ï¼ˆå®Œæ•´ç»“æ„åŒ–æ•°æ®ï¼‰\")\n",
    "    print(f\"  - {TERM_TABLE}ï¼ˆæœ¯è¯­æ ¡éªŒè¡¨ï¼‰\")\n",
    "    print(f\"  - {NER_TABLE}ï¼ˆå‘½åå®ä½“æ ¡éªŒè¡¨ï¼‰\")\n",
    "    print(f\"  - {STYLE_METADATA}ï¼ˆé£æ ¼åˆ†æè¡¨ï¼‰\")\n",
    "    print(f\"  - {CULTURAL_WORD_TABLE}ï¼ˆæ–‡åŒ–è´Ÿè½½è¯è¡¨ï¼‰\")\n",
    "    print(f\"  - {TRANSLATION_UNITS}ï¼ˆç¿»è¯‘å•å…ƒè¡¨ï¼‰\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6f695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼ˆutf-8ç¼–ç ï¼‰\n",
      "âœ… å·²è·³è¿‡å‰ç½®å†…å®¹ï¼Œä»CHAPTER 1å¼€å§‹å¤„ç†\n",
      "\n",
      "âœ… åŒ¹é…ç« èŠ‚1ï¼šæ ‡é¢˜=Chapter 1: Introductionï¼Œæœ‰æ•ˆå­—ç¬¦æ•°=126262\n",
      "  â†’ ç§»é™¤äº†0ä¸ªé‡å¤/æ— æ•ˆæ®µè½\n",
      "âœ… ç« èŠ‚1æœ€ç»ˆï¼š355ä¸ªå”¯ä¸€æ®µè½ï¼Œæ— é‡å¤å¥å­\n",
      "\n",
      "âš ï¸  ç« èŠ‚2æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œè§¦å‘ç²¾å‡†å…œåº•\n",
      "  â†’ ç§»é™¤äº†1ä¸ªé‡å¤/æ— æ•ˆæ®µè½\n",
      "âœ… ç« èŠ‚2æœ€ç»ˆï¼š193ä¸ªå”¯ä¸€æ®µè½ï¼Œæ— é‡å¤å¥å­\n",
      "\n",
      "âš ï¸  ç« èŠ‚3æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œè§¦å‘ç²¾å‡†å…œåº•\n",
      "  â†’ ç§»é™¤äº†4ä¸ªé‡å¤/æ— æ•ˆæ®µè½\n",
      "âœ… ç« èŠ‚3æœ€ç»ˆï¼š371ä¸ªå”¯ä¸€æ®µè½ï¼Œæ— é‡å¤å¥å­\n",
      "\n",
      "âš ï¸  ç« èŠ‚4æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œè§¦å‘ç²¾å‡†å…œåº•\n",
      "  â†’ ç§»é™¤äº†0ä¸ªé‡å¤/æ— æ•ˆæ®µè½\n",
      "âœ… ç« èŠ‚4æœ€ç»ˆï¼š257ä¸ªå”¯ä¸€æ®µè½ï¼Œæ— é‡å¤å¥å­\n",
      "\n",
      "âš ï¸  ç« èŠ‚5æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œè§¦å‘ç²¾å‡†å…œåº•\n",
      "  â†’ ç§»é™¤äº†0ä¸ªé‡å¤/æ— æ•ˆæ®µè½\n",
      "âœ… ç« èŠ‚5æœ€ç»ˆï¼š214ä¸ªå”¯ä¸€æ®µè½ï¼Œæ— é‡å¤å¥å­\n",
      "\n",
      "ğŸ‰ æ— é‡å¤JSONæ–‡ä»¶å·²ä¿å­˜è‡³ï¼špreprocess_chap1-5.json\n",
      "ğŸ“Š å…¨å±€ç»Ÿè®¡ï¼š\n",
      "  - ç« èŠ‚1ï¼š355æ®µ | 126262å­—ç¬¦\n",
      "  - ç« èŠ‚2ï¼š193æ®µ | 72733å­—ç¬¦\n",
      "  - ç« èŠ‚3ï¼š371æ®µ | 125808å­—ç¬¦\n",
      "  - ç« èŠ‚4ï¼š257æ®µ | 92580å­—ç¬¦\n",
      "  - ç« èŠ‚5ï¼š214æ®µ | 77771å­—ç¬¦\n",
      "  - æ€»è®¡ï¼š1390ä¸ªå”¯ä¸€æ®µè½ | 495154ä¸ªæœ‰æ•ˆå­—ç¬¦\n"
     ]
    }
   ],
   "source": [
    "# ä»…è¾“å‡ºjsonæ–‡ä»¶çš„ä¿®æ­£ä»£ç \n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒé…ç½® --------------------------\n",
    "INPUT_TXT = \"./AIBookEnglish.txt\"  # ä½ çš„è‹±æ–‡TXTè·¯å¾„\n",
    "OUTPUT_JSON = \"preprocess_chap1-5.json\"  # è¾“å‡ºæ— é‡å¤çš„JSON\n",
    "TARGET_CHAPTERS = [1, 2, 3, 4, 5]\n",
    "CHAPTER_TITLES = {  # å‰äº”ç« ç²¾ç¡®æ ‡é¢˜\n",
    "    1: \"INTRODUCTION\",\n",
    "    2: \"INTELLIGENT AGENTS\",\n",
    "    3: \"SOLVING PROBLEMS BY SEARCHING\",\n",
    "    4: \"SEARCH IN COMPLEX ENVIRONMENTS\",\n",
    "    5: \"CONSTRAINT SATISFACTION PROBLEMS\"\n",
    "}\n",
    "MAX_CHAP_CHAR_COUNT = 150000  # å•ä¸ªç« èŠ‚æœ€å¤§å­—ç¬¦æ•°ï¼ˆé¿å…è¿‡åº¦æ•è·ï¼‰\n",
    "MIN_PARA_LENGTH = 10  # æœ€å°æ®µè½é•¿åº¦ï¼ˆè¿‡æ»¤æ— æ•ˆç©ºç™½æ®µè½ï¼‰\n",
    "\n",
    "# -------------------------- è¾…åŠ©å‡½æ•°ï¼šå»é‡ + ä¼˜åŒ–æ®µè½åˆ†å‰² --------------------------\n",
    "def remove_duplicate_paragraphs(paragraphs):\n",
    "    \"\"\"ç§»é™¤å®Œå…¨é‡å¤çš„æ®µè½ï¼Œä¿ç•™é¦–æ¬¡å‡ºç°çš„æ®µè½\"\"\"\n",
    "    seen_content = set()\n",
    "    unique_paragraphs = []\n",
    "    duplicate_count = 0\n",
    "    for para in paragraphs:\n",
    "        # æå–æ®µè½æ ¸å¿ƒå†…å®¹ï¼ˆå»ç©ºæ ¼/æ¢è¡Œï¼Œç”¨äºå»é‡åˆ¤æ–­ï¼‰\n",
    "        core_content = para[\"content\"].replace(\" \", \"\").replace(\"\\n\", \"\").strip()\n",
    "        if not core_content:  # è¿‡æ»¤ç©ºæ®µè½\n",
    "            continue\n",
    "        if len(core_content) < MIN_PARA_LENGTH:  # è¿‡æ»¤æçŸ­æ— æ•ˆæ®µè½\n",
    "            continue\n",
    "        if core_content not in seen_content:\n",
    "            seen_content.add(core_content)\n",
    "            unique_paragraphs.append(para)\n",
    "        else:\n",
    "            duplicate_count += 1\n",
    "    print(f\"  â†’ ç§»é™¤äº†{duplicate_count}ä¸ªé‡å¤/æ— æ•ˆæ®µè½\")\n",
    "    return unique_paragraphs\n",
    "\n",
    "def split_into_unique_paragraphs(content, chap_num):\n",
    "    \"\"\"åˆ†å‰²æ®µè½ + è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤å¥å­\"\"\"\n",
    "    # 1. æŒ‰ç©ºè¡Œåˆ†å‰²åŸå§‹æ®µè½ï¼Œæ¸…ç†æ ¼å¼\n",
    "    raw_paragraphs = re.split(r'\\n{2,}', content)\n",
    "    processed_paras = []\n",
    "    para_idx = 0\n",
    "\n",
    "    for raw_para in raw_paragraphs:\n",
    "        para_content = raw_para.strip()\n",
    "        # è¿‡æ»¤æ— æ•ˆå†…å®¹\n",
    "        if not para_content or len(para_content.replace(\" \", \"\")) < MIN_PARA_LENGTH:\n",
    "            continue\n",
    "\n",
    "        # ç”Ÿæˆæ®µè½IDå’Œä¸Šä¸‹æ–‡å…³è”\n",
    "        para_id = f\"chap{chap_num}_para{para_idx + 1}\"\n",
    "        prev_para_id = f\"chap{chap_num}_para{para_idx}\" if para_idx > 0 else None\n",
    "        next_para_id = f\"chap{chap_num}_para{para_idx + 2}\" if (para_idx + 2) <= len(raw_paragraphs) else None\n",
    "\n",
    "        # ç»Ÿè®¡æ®µè½åŸºæœ¬ä¿¡æ¯\n",
    "        sentence_count = len(re.split(r'[.!?]+(?=\\s|$)', para_content))  # å¥å­æ•°\n",
    "        char_count = len(para_content.replace(\" \", \"\"))  # æœ‰æ•ˆå­—ç¬¦æ•°\n",
    "\n",
    "        processed_paras.append({\n",
    "            \"para_id\": para_id,\n",
    "            \"content\": para_content,\n",
    "            \"sentence_count\": sentence_count,\n",
    "            \"char_count\": char_count,\n",
    "            \"prev_para_id\": prev_para_id,\n",
    "            \"next_para_id\": next_para_id\n",
    "        })\n",
    "        para_idx += 1\n",
    "\n",
    "    # 2. ç§»é™¤é‡å¤æ®µè½\n",
    "    unique_paras = remove_duplicate_paragraphs(processed_paras)\n",
    "    return unique_paras\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒå‡½æ•°ï¼šç²¾å‡†æå–å‰äº”ç« ï¼ˆæ— é‡å¤ï¼‰ --------------------------\n",
    "def extract_unique_chapters(txt_path):\n",
    "    \"\"\"ç²¾å‡†æå–ï¼Œé¿å…è¿‡åº¦æ•è·ï¼Œç”Ÿæˆæ— é‡å¤å†…å®¹çš„ç« èŠ‚æ•°æ®\"\"\"\n",
    "    # 1. è¯»å–æ–‡ä»¶ï¼ˆå…¼å®¹ç¼–ç ï¼‰\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "        print(\"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼ˆutf-8ç¼–ç ï¼‰\")\n",
    "    except UnicodeDecodeError:\n",
    "        with open(txt_path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "            full_text = f.read()\n",
    "        print(\"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼ˆgbkç¼–ç ï¼‰\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{txt_path}\")\n",
    "\n",
    "    # 2. è·³è¿‡å‰ç½®å†…å®¹ï¼Œä»CHAPTER 1å¼€å§‹\n",
    "    chapter1_match = re.search(r'^\\s*CHAPTER\\s+1\\s*$', full_text, re.MULTILINE | re.IGNORECASE)\n",
    "    if chapter1_match:\n",
    "        full_text = full_text[chapter1_match.start():]\n",
    "        print(\"âœ… å·²è·³è¿‡å‰ç½®å†…å®¹ï¼Œä»CHAPTER 1å¼€å§‹å¤„ç†\")\n",
    "    else:\n",
    "        raise ValueError(\"âŒ æœªæ‰¾åˆ°ç‹¬ç«‹è¡Œçš„CHAPTER 1ï¼Œæ£€æŸ¥æ–‡æ¡£æ ¼å¼\")\n",
    "\n",
    "    chapters_data = defaultdict(dict)\n",
    "    for chap_num in TARGET_CHAPTERS:\n",
    "        expected_title = CHAPTER_TITLES[chap_num]\n",
    "        chap_content = \"\"\n",
    "        chap_title = f\"Chapter {chap_num}: {expected_title.title()}\"\n",
    "\n",
    "        # 3. ç²¾å‡†æ­£åˆ™ï¼šé¿å…è¿‡åº¦æ•è·ï¼Œä¸¥æ ¼åŒ¹é…ç« èŠ‚èŒƒå›´\n",
    "        pattern = rf'''\n",
    "            (?i)\n",
    "            ^\\s*CHAPTER\\s+{chap_num}\\s*$  # ç« èŠ‚å·ç‹¬ç«‹è¡Œ\n",
    "            \\s+\n",
    "            ^\\s*{expected_title}\\s*$     # æ ‡é¢˜ç‹¬ç«‹è¡Œ\n",
    "            \\s+\n",
    "            ([\\s\\S]{{1,{MAX_CHAP_CHAR_COUNT}}})  # é™åˆ¶æœ€å¤§å­—ç¬¦æ•°ï¼Œé¿å…è¿‡åº¦æ•è·\n",
    "            (?=\n",
    "                ^\\s*CHAPTER\\s+{chap_num + 1}\\s*$  # ä¸‹ä¸€ç« ç»ˆæ­¢\n",
    "                |$\n",
    "            )\n",
    "        '''\n",
    "\n",
    "        # æ‰§è¡ŒåŒ¹é…\n",
    "        match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL | re.VERBOSE)\n",
    "        if match:\n",
    "            raw_content = match.group(1).strip()\n",
    "            # æ¸…ç†å¤šä½™ç©ºè¡Œï¼Œå‡å°‘å†—ä½™\n",
    "            chap_content = re.sub(r'\\n{3,}', '\\n\\n', raw_content).strip()\n",
    "            content_len = len(chap_content.replace(\" \", \"\"))\n",
    "            print(f\"\\nâœ… åŒ¹é…ç« èŠ‚{chap_num}ï¼šæ ‡é¢˜={chap_title}ï¼Œæœ‰æ•ˆå­—ç¬¦æ•°={content_len}\")\n",
    "        else:\n",
    "            # å…œåº•æˆªå–ï¼šä¸¥æ ¼é™åˆ¶ç« èŠ‚èŒƒå›´ï¼Œé¿å…é‡å \n",
    "            print(f\"\\nâš ï¸  ç« èŠ‚{chap_num}æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œè§¦å‘ç²¾å‡†å…œåº•\")\n",
    "            current_chap_mark = re.search(rf'^\\s*CHAPTER\\s+{chap_num}\\s*$', full_text, re.MULTILINE | re.IGNORECASE)\n",
    "            next_chap_mark = re.search(rf'^\\s*CHAPTER\\s+{chap_num + 1}\\s*$', full_text, re.MULTILINE | re.IGNORECASE)\n",
    "            if current_chap_mark:\n",
    "                start_pos = current_chap_mark.end()\n",
    "                end_pos = next_chap_mark.start() if next_chap_mark else (current_chap_mark.end() + MAX_CHAP_CHAR_COUNT)\n",
    "                raw_content = full_text[start_pos:end_pos].strip()\n",
    "                chap_content = re.sub(r'\\n{3,}', '\\n\\n', raw_content).strip()\n",
    "            else:\n",
    "                raise ValueError(f\"âŒ æœªæ‰¾åˆ°ç« èŠ‚{chap_num}\")\n",
    "\n",
    "        # 4. åˆ†å‰²ä¸ºæ— é‡å¤æ®µè½\n",
    "        paragraphs = split_into_unique_paragraphs(chap_content, chap_num)\n",
    "\n",
    "        # 5. ä¿å­˜ç« èŠ‚æ•°æ®ï¼ˆæ— é‡å¤ï¼‰\n",
    "        chapters_data[chap_num] = {\n",
    "            \"chapter_num\": chap_num,\n",
    "            \"title\": chap_title,\n",
    "            \"raw_content\": chap_content,\n",
    "            \"paragraphs\": paragraphs,\n",
    "            \"para_count\": len(paragraphs),\n",
    "            \"char_count\": len(chap_content.replace(\" \", \"\"))\n",
    "        }\n",
    "        print(f\"âœ… ç« èŠ‚{chap_num}æœ€ç»ˆï¼š{len(paragraphs)}ä¸ªå”¯ä¸€æ®µè½ï¼Œæ— é‡å¤å¥å­\")\n",
    "\n",
    "    # æ ¡éªŒæ‰€æœ‰ç« èŠ‚æå–æˆåŠŸ\n",
    "    missing_chaps = [c for c in TARGET_CHAPTERS if c not in chapters_data]\n",
    "    if missing_chaps:\n",
    "        raise ValueError(f\"âŒ ç¼ºå¤±ç« èŠ‚ï¼š{missing_chaps}\")\n",
    "\n",
    "    return chapters_data\n",
    "\n",
    "# -------------------------- ä¿å­˜æ— é‡å¤çš„JSON --------------------------\n",
    "def save_unique_json(chapters_data, output_path):\n",
    "    \"\"\"å°†æ— é‡å¤çš„ç« èŠ‚æ•°æ®ä¿å­˜ä¸ºJSON\"\"\"\n",
    "    final_data = {\n",
    "        \"book_info\": \"Artificial Intelligence: A Modern Approach (4th Ed) - å‰äº”ç« ï¼ˆæ— é‡å¤ï¼‰\",\n",
    "        \"target_chapters\": TARGET_CHAPTERS,\n",
    "        \"chapters\": list(chapters_data.values())  # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼\n",
    "    }\n",
    "\n",
    "    # ä¿å­˜JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nğŸ‰ æ— é‡å¤JSONæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{output_path}\")\n",
    "    print(f\"ğŸ“Š å…¨å±€ç»Ÿè®¡ï¼š\")\n",
    "    total_para = 0\n",
    "    total_char = 0\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        total_para += chap[\"para_count\"]\n",
    "        total_char += chap[\"char_count\"]\n",
    "        print(f\"  - ç« èŠ‚{chap['chapter_num']}ï¼š{chap['para_count']}æ®µ | {chap['char_count']}å­—ç¬¦\")\n",
    "    print(f\"  - æ€»è®¡ï¼š{total_para}ä¸ªå”¯ä¸€æ®µè½ | {total_char}ä¸ªæœ‰æ•ˆå­—ç¬¦\")\n",
    "\n",
    "# -------------------------- ä¸»å‡½æ•°æ‰§è¡Œ --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # æå–æ— é‡å¤ç« èŠ‚\n",
    "        chapters_data = extract_unique_chapters(INPUT_TXT)\n",
    "        # ä¿å­˜JSON\n",
    "        save_unique_json(chapters_data, OUTPUT_JSON)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16179764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼ˆutf-8ç¼–ç ï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼‰\n",
      "âœ… å·²è·³è¿‡å‰ç½®å†…å®¹ï¼Œä»CHAPTER 1å¼€å§‹å¤„ç†\n",
      "ğŸ“š å‰äº”ç« æ‰€åœ¨æ–‡æœ¬æœ‰æ•ˆå­—ç¬¦æ•°ï¼š3209017ï¼ˆéœ€â‰¥20000ï¼‰\n",
      "âœ… åŒ¹é…ç« èŠ‚1ï¼šæ ‡é¢˜=Chapter 1: Introductionï¼Œå†…å®¹é•¿åº¦=3200078å­—ç¬¦\n",
      "âœ… ç« èŠ‚1æœ€ç»ˆï¼š18430ä¸ªæ®µè½ï¼Œ3200078å­—ç¬¦\n",
      "âš ï¸  ç« èŠ‚2æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…œåº•æˆªå–\n",
      "âœ… ç« èŠ‚2æœ€ç»ˆï¼š195ä¸ªæ®µè½ï¼Œ72733å­—ç¬¦\n",
      "âš ï¸  ç« èŠ‚3æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…œåº•æˆªå–\n",
      "âœ… ç« èŠ‚3æœ€ç»ˆï¼š376ä¸ªæ®µè½ï¼Œ125808å­—ç¬¦\n",
      "âš ï¸  ç« èŠ‚4æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…œåº•æˆªå–\n",
      "âœ… ç« èŠ‚4æœ€ç»ˆï¼š258ä¸ªæ®µè½ï¼Œ92580å­—ç¬¦\n",
      "âš ï¸  ç« èŠ‚5æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…œåº•æˆªå–\n",
      "âœ… ç« èŠ‚5æœ€ç»ˆï¼š215ä¸ªæ®µè½ï¼Œ77771å­—ç¬¦\n",
      "\n",
      "âœ… å‰äº”ç« å…¨éƒ¨ç²¾å‡†æå–æˆåŠŸï¼æ— éœ€ä¾èµ–å…œåº•ç­–ç•¥\n",
      "\n",
      "ğŸ“– ç« èŠ‚1æ‘˜è¦ï¼š\n",
      "æ ‡é¢˜ï¼šChapter 1: Introduction\n",
      "æ®µè½æ•°ï¼š18430\n",
      "æœ‰æ•ˆå­—ç¬¦æ•°ï¼š3200078\n",
      "ç¬¬ä¸€æ®µé¢„è§ˆï¼šIn which we try to explain why we consider artificial intelligence to be a subject most worthy of st...\n",
      "\n",
      "ğŸ“– ç« èŠ‚2æ‘˜è¦ï¼š\n",
      "æ ‡é¢˜ï¼šChapter 2: Intelligent Agents\n",
      "æ®µè½æ•°ï¼š195\n",
      "æœ‰æ•ˆå­—ç¬¦æ•°ï¼š72733\n",
      "ç¬¬ä¸€æ®µé¢„è§ˆï¼šINTELLIGENT AGENTS...\n",
      "\n",
      "ğŸ“– ç« èŠ‚3æ‘˜è¦ï¼š\n",
      "æ ‡é¢˜ï¼šChapter 3: Solving Problems By Searching\n",
      "æ®µè½æ•°ï¼š376\n",
      "æœ‰æ•ˆå­—ç¬¦æ•°ï¼š125808\n",
      "ç¬¬ä¸€æ®µé¢„è§ˆï¼šSOLVING PROBLEMS BY SEARCHING...\n",
      "\n",
      "ğŸ“– ç« èŠ‚4æ‘˜è¦ï¼š\n",
      "æ ‡é¢˜ï¼šChapter 4: Search In Complex Environments\n",
      "æ®µè½æ•°ï¼š258\n",
      "æœ‰æ•ˆå­—ç¬¦æ•°ï¼š92580\n",
      "ç¬¬ä¸€æ®µé¢„è§ˆï¼šSEARCH IN COMPLEX ENVIRONMENTS...\n",
      "\n",
      "ğŸ“– ç« èŠ‚5æ‘˜è¦ï¼š\n",
      "æ ‡é¢˜ï¼šChapter 5: Constraint Satisfaction Problems\n",
      "æ®µè½æ•°ï¼š215\n",
      "æœ‰æ•ˆå­—ç¬¦æ•°ï¼š77771\n",
      "ç¬¬ä¸€æ®µé¢„è§ˆï¼šCONSTRAINT SATISFACTION PROBLEMS...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_chapter11_final(txt_path):\n",
    "    \"\"\"\n",
    "    ç»ˆæé€‚é…ç‰ˆï¼šå®Œå…¨åŒ¹é…æ–‡æ¡£ã€ŒCHAPTER 11â†’æ ‡é¢˜â†’å¼•å¯¼å¥â†’å­ç« èŠ‚â†’æ­£æ–‡â†’Summaryâ†’å‚è€ƒæ–‡çŒ®ã€æ ¼å¼\n",
    "    æ˜ç¡®é”å®šæ ‡é¢˜ã€æ”¾å®½ä¸­é—´åŒ¹é…æ¡ä»¶ï¼Œç¡®ä¿æ•è·æ‰€æœ‰å†…å®¹\n",
    "    \"\"\"\n",
    "    # 1. è¯»å–æ–‡ä»¶ï¼ˆå…¼å®¹ç¼–ç +å®Œæ•´è¯»å–ï¼Œä¿ç•™åŸå§‹æ¢è¡Œç»“æ„ï¼Œé¿å…è¿‡åº¦å¤„ç†ï¼‰\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()  # ä¸ç»Ÿä¸€æ¢è¡Œç¬¦ï¼Œä¿ç•™æ–‡æ¡£åŸå§‹æ ¼å¼\n",
    "        print(\"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼ˆutf-8ç¼–ç ï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼‰\")\n",
    "    except UnicodeDecodeError:\n",
    "        with open(txt_path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "            full_text = f.read()\n",
    "        print(\"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼ˆgbkç¼–ç ï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼‰\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„ï¼š{txt_path}\")\n",
    "        return None, 0, \"\"\n",
    "\n",
    "    # 2. æ ¸å¿ƒæ­£åˆ™ï¼šç²¾å‡†åŒ¹é…æ–‡æ¡£ç« èŠ‚ç»“æ„ï¼Œæ— å¤šä½™é™åˆ¶\n",
    "    pattern = r'''\n",
    "        (?i)                    # å¿½ç•¥å¤§å°å†™\n",
    "        ^\\s*CHAPTER\\s+11\\s*$    # åŒ¹é…ã€ŒCHAPTER 11ã€å•ç‹¬ä¸€è¡Œï¼ˆå…è®¸å‰åç©ºç™½ï¼‰\n",
    "        \\s+                      # ç« èŠ‚å·åä»»æ„ç©ºè¡Œ\n",
    "        ^\\s*AUTOMATED\\s+PLANNING\\s*$  # æ˜ç¡®åŒ¹é…æ ‡é¢˜ï¼ˆé¿å…è¯¯åŒ¹é…å…¶ä»–å†…å®¹ï¼‰\n",
    "        \\s+                      # æ ‡é¢˜åä»»æ„ç©ºè¡Œ\n",
    "        ([\\s\\S]*)               # æ•è·æ‰€æœ‰æ­£æ–‡ï¼ˆè´ªå©ªåŒ¹é…ï¼Œç›´åˆ°ç»ˆæ­¢æ¡ä»¶ï¼‰\n",
    "        (?=                      # ç»ˆæ­¢æ¡ä»¶ï¼šCHAPTER 12 æˆ–æ–‡æ¡£æœ«å°¾\n",
    "            ^\\s*CHAPTER\\s+12\\b   # ä¸¥æ ¼åŒ¹é… CHAPTER 12ï¼ˆé¿å…ç±»ä¼¼â€œCHAPTER 120â€ï¼‰\n",
    "            |$                    # æˆ–æ–‡æ¡£ç»“æŸ\n",
    "        )\n",
    "    '''\n",
    "\n",
    "    # æ‰§è¡ŒåŒ¹é…ï¼ˆå…³é”®ï¼šä¿ç•™ re.DOTALL è®©.åŒ¹é…æ¢è¡Œï¼Œre.MULTILINE è®©^$åŒ¹é…è¡Œé¦–è¡Œå°¾ï¼‰\n",
    "    match = re.search(\n",
    "        pattern,\n",
    "        full_text,\n",
    "        re.MULTILINE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    if match:\n",
    "        chap_title = \"Chapter 11: AUTOMATED PLANNING\"\n",
    "        raw_content = match.group(1).strip()  # æ•è·æ‰€æœ‰æ­£æ–‡ï¼ˆå«å­ç« èŠ‚ã€Summaryç­‰ï¼‰\n",
    "        \n",
    "        # æ¸…ç†å¤šä½™ç©ºè¡Œï¼ˆåªåˆå¹¶3ä¸ªåŠä»¥ä¸Šç©ºè¡Œï¼Œä¿ç•™æ®µè½/å­ç« èŠ‚åˆ†éš”ï¼‰\n",
    "        clean_content = re.sub(r'\\n{3,}', '\\n\\n', raw_content)\n",
    "        clean_content = clean_content.strip()\n",
    "        \n",
    "        # ç»Ÿè®¡æœ‰æ•ˆå­—ç¬¦ï¼ˆå»ç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰\n",
    "        valid_chars = re.sub(r'[\\s\\t]', '', clean_content)\n",
    "        valid_char_count = len(valid_chars)\n",
    "        \n",
    "        # è¾“å‡ºè¯¦ç»†ç»“æœï¼ˆé¢„è§ˆå‰2000å­—ç¬¦ï¼Œç¡®ä¿åŒ…å«å­ç« èŠ‚ï¼‰\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ğŸ“Œ CHAPTER 11 å®Œæ•´æå–æˆåŠŸï¼\")\n",
    "        print(f\"æ ‡é¢˜ï¼š{chap_title}\")\n",
    "        print(f\"æ¸…ç†åæ€»å­—ç¬¦æ•°ï¼ˆå«åˆç†ç©ºè¡Œï¼‰ï¼š{len(clean_content)}\")\n",
    "        print(f\"æœ‰æ•ˆå­—ç¬¦æ•°ï¼ˆå»ç©ºç™½ï¼‰ï¼š{valid_char_count}\")\n",
    "        print(f\"\\nå‰2000å­—ç¬¦é¢„è§ˆï¼š\\n{clean_content[:2000]}...\")\n",
    "        print(\"=\"*80)\n",
    "        return chap_title, valid_char_count, clean_content\n",
    "    else:\n",
    "        print(\"\\nâŒ åŒ¹é…å¤±è´¥ï¼è¯·ç¡®è®¤æ–‡æ¡£ä¸­å­˜åœ¨ä»¥ä¸‹æ ¼å¼ï¼š\")\n",
    "        print(\"   - ç‹¬ç«‹è¡Œï¼šCHAPTER 11ï¼ˆå…è®¸å‰åç©ºç™½ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼‰\")\n",
    "        print(\"   - ç‹¬ç«‹è¡Œï¼šAUTOMATED PLANNINGï¼ˆå…è®¸å‰åç©ºç™½ï¼‰\")\n",
    "        print(\"   - åç»­å†…å®¹åŒ…å« 11.1 Definition of Classical Planning ç­‰å­ç« èŠ‚\")\n",
    "        # è°ƒè¯•ï¼šæ‰“å°æ–‡æ¡£ä¸­æ‰€æœ‰ CHAPTER ç« èŠ‚ï¼Œç¡®è®¤ CHAPTER 11 å­˜åœ¨\n",
    "        chapters = re.findall(r'^\\s*CHAPTER\\s+\\d+\\s*$', full_text, re.MULTILINE | re.IGNORECASE)\n",
    "        print(f\"\\nğŸ“‹ æ–‡æ¡£ä¸­æ£€æµ‹åˆ°çš„æ‰€æœ‰ç« èŠ‚ï¼š{chapters}\")\n",
    "        return None, 0, \"\"\n",
    "\n",
    "# æ‰§è¡Œæå–ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å®é™…è·¯å¾„ï¼‰\n",
    "if __name__ == \"__main__\":\n",
    "    TXT_FILE_PATH = \"./AIBookEnglish.txt\"  # åŠ¡å¿…æ›¿æ¢ä¸ºå®é™…è·¯å¾„\n",
    "    extract_chapter11_final(TXT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# -------------------------- é…ç½®å‚æ•°ï¼ˆé€‚é…å®é™…æ–‡æ¡£ç‰¹å¾ï¼‰ --------------------------\n",
    "EN_JSON_PATH = \"./preprocess_chap1-5.json\"\n",
    "CN_JSON_PATH = \"./preprocess_chap1-5_chinese.json\"\n",
    "TARGET_CHAPTERS = [1,2,3,4,5]\n",
    "PARA_COUNT_TOLERANCE = 0.3  # ä¿®æ­£å˜é‡åæ‹¼å†™é”™è¯¯ï¼ˆåŸPARACOUNT_TOLERANCEï¼‰\n",
    "KEY_TERMS_MAPPING = {\n",
    "    \"INTRODUCTION\": \"ç»ªè®º\",\n",
    "    \"intelligent agent\": \"æ™ºèƒ½ä½“\",\n",
    "    \"search algorithm\": \"æœç´¢ç®—æ³•\",\n",
    "    \"utility function\": \"æ•ˆç”¨å‡½æ•°\",\n",
    "    \"adversarial search\": \"å¯¹æŠ—æœç´¢\",\n",
    "    \"constraint satisfaction problem\": \"çº¦æŸæ»¡è¶³é—®é¢˜\"\n",
    "}\n",
    "# ä¸­è‹±æ–‡æ ‡é¢˜ç²¾å‡†æ˜ å°„ï¼ˆé€‚é…æ–‡æ¡£å®é™…ç›®å½•ï¼‰\n",
    "CHAPTER_TITLE_MAP = {\n",
    "    1: (\"Chapter 1: Introduction\", \"ç¬¬1ç« ï¼šç»ªè®º\"),\n",
    "    2: (\"Chapter 2: Intelligent Agents\", \"ç¬¬2ç« ï¼šæ™ºèƒ½ä½“\"),\n",
    "    3: (\"Chapter 3: Solving Problems (by|By) Searching\", \"ç¬¬3ç« ï¼šé€šè¿‡æœç´¢è¿›è¡Œé—®é¢˜æ±‚è§£\"),\n",
    "    4: (\"Chapter 4: Search (in|In) Complex Environments\", \"ç¬¬4ç« ï¼šå¤æ‚ç¯å¢ƒä¸­çš„æœç´¢\"),\n",
    "    5: (\"Chapter 5: Adversarial Search and Games\", \"ç¬¬5ç« ï¼šå¯¹æŠ—æœç´¢å’Œåšå¼ˆ\")\n",
    "}\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒå·¥å…·å‡½æ•° --------------------------\n",
    "def load_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"âŒ JSONè§£æå¤±è´¥ï¼š{file_path}ï¼ˆæ–‡ä»¶æŸåæˆ–æ ¼å¼é”™è¯¯ï¼‰\")\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"æ ‡å‡†åŒ–æ ‡é¢˜ï¼šå°å†™åŒ–ã€å»é™¤å¤šä½™ç©ºæ ¼\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', title.strip()).lower()\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒéªŒè¯å‡½æ•° --------------------------\n",
    "def verify_chapter_mapping(en_data, cn_data):\n",
    "    print(\"ğŸ” å¼€å§‹éªŒè¯ç« èŠ‚æ˜ å°„...\")\n",
    "    en_chapters = {chap[\"chapter_num\"]: chap for chap in en_data[\"chapters\"]}\n",
    "    cn_chapters = {chap[\"chapter_num\"]: chap for chap in cn_data[\"chapters\"]}\n",
    "    \n",
    "    # éªŒè¯ç« èŠ‚æ•°é‡\n",
    "    missing_en = set(TARGET_CHAPTERS) - set(en_chapters.keys())\n",
    "    missing_cn = set(TARGET_CHAPTERS) - set(cn_chapters.keys())\n",
    "    if missing_en or missing_cn:\n",
    "        raise ValueError(f\"âŒ ç« èŠ‚ç¼ºå¤±ï¼šè‹±æ–‡ç¼ºå¤±{missing_en}ï¼Œä¸­æ–‡ç¼ºå¤±{missing_cn}\")\n",
    "    \n",
    "    # éªŒè¯æ ‡é¢˜ï¼ˆå…¼å®¹è‹±æ–‡ä»‹è¯å¤§å°å†™ï¼‰\n",
    "    for chap_num in TARGET_CHAPTERS:\n",
    "        en_title = en_chapters[chap_num][\"title\"].strip()\n",
    "        cn_title = cn_chapters[chap_num][\"title\"].strip()\n",
    "        expected_en_pattern, expected_cn = CHAPTER_TITLE_MAP[chap_num]\n",
    "        \n",
    "        if not re.fullmatch(expected_en_pattern, en_title, re.IGNORECASE):\n",
    "            print(f\"âš ï¸  è‹±æ–‡ç¬¬{chap_num}ç« æ ‡é¢˜ä¸åŒ¹é…ï¼šå®é™…[{en_title}] â†’ æœŸæœ›åŒ¹é…æ¨¡å¼[{expected_en_pattern}]\")\n",
    "        if normalize_title(cn_title) != normalize_title(expected_cn):\n",
    "            print(f\"âš ï¸  ä¸­æ–‡ç¬¬{chap_num}ç« æ ‡é¢˜ä¸åŒ¹é…ï¼šå®é™…[{cn_title}] â†’ æœŸæœ›[{expected_cn}]\")\n",
    "    \n",
    "    # é‡ç‚¹æç¤ºè‹±æ–‡ç¬¬5ç« æ ‡é¢˜é”™è¯¯ï¼ˆå…³é”®é—®é¢˜ï¼‰\n",
    "    en_chap5_title = en_chapters.get(5, {}).get(\"title\", \"\").strip()\n",
    "    if en_chap5_title == \"Chapter 5: Constraint Satisfaction Problems\":\n",
    "        print(\"âŒ ä¸¥é‡é”™è¯¯ï¼šè‹±æ–‡ç¬¬5ç« æ ‡é¢˜ä¸ä¸­æ–‡ç¬¬5ç« ä¸å¯¹åº”ï¼\")\n",
    "        print(f\"  - è‹±æ–‡ç¬¬5ç« å®é™…å†…å®¹ï¼šä¸­æ–‡ç¬¬6ç« â€œçº¦æŸæ»¡è¶³é—®é¢˜â€\")\n",
    "        print(f\"  - ä¸­æ–‡ç¬¬5ç« å†…å®¹ï¼šå¯¹æŠ—æœç´¢å’Œåšå¼ˆï¼ˆè‹±æ–‡åº”å¯¹åº”Chapter 5: Adversarial Search and Gamesï¼‰\")\n",
    "        print(f\"  - å½±å“ï¼šä¸­è‹±æ–‡ç¬¬5ç« æ®µè½æ— æ³•å¯¹é½ï¼Œéœ€ä¿®æ­£è‹±æ–‡JSONçš„ç« èŠ‚æˆªå–é€»è¾‘\")\n",
    "    \n",
    "    print(\"âœ… ç« èŠ‚æ˜ å°„éªŒè¯å®Œæˆï¼ˆæ— ç¼ºå¤±ï¼Œæ ‡é¢˜å…³é”®é”™è¯¯è§ä¸Šè¿°æç¤ºï¼‰\")\n",
    "    return en_chapters, cn_chapters\n",
    "\n",
    "def verify_paragraph_mapping(en_chapters, cn_chapters):\n",
    "    print(\"\\nğŸ” å¼€å§‹éªŒè¯æ®µè½æ˜ å°„...\")\n",
    "    para_id_pattern = re.compile(r\"chap(\\d+)_para(\\d+)\")\n",
    "    all_pass = True\n",
    "    \n",
    "    for chap_num in TARGET_CHAPTERS:\n",
    "        en_paras = en_chapters[chap_num][\"paragraphs\"]\n",
    "        cn_paras = cn_chapters[chap_num][\"paragraphs\"]\n",
    "        \n",
    "        # éªŒè¯æ®µè½IDæ ¼å¼å’Œå½’å±\n",
    "        for para in en_paras:\n",
    "            match = para_id_pattern.match(para[\"para_id\"])\n",
    "            if not match or int(match.group(1)) != chap_num:\n",
    "                print(f\"âŒ è‹±æ–‡ç¬¬{chap_num}ç« æ®µè½IDé”™è¯¯ï¼š{para['para_id']}\")\n",
    "                all_pass = False\n",
    "        for para in cn_paras:\n",
    "            match = para_id_pattern.match(para[\"para_id\"])\n",
    "            if not match or int(match.group(1)) != chap_num:\n",
    "                print(f\"âŒ ä¸­æ–‡ç¬¬{chap_num}ç« æ®µè½IDé”™è¯¯ï¼š{para['para_id']}\")\n",
    "                all_pass = False\n",
    "        \n",
    "        # éªŒè¯æ®µè½æ•°é‡å·®å¼‚ï¼ˆä¿®æ­£æŠ¥é”™è¡Œï¼‰\n",
    "        en_count = len(en_paras)\n",
    "        cn_count = len(cn_paras)\n",
    "        diff_rate = abs(en_count - cn_count) / max(en_count, 1)\n",
    "        print(f\"\\nğŸ“Š ç¬¬{chap_num}ç« æ®µè½æ•°é‡ï¼šè‹±æ–‡{en_count}æ®µ | ä¸­æ–‡{cn_count}æ®µï¼ˆå·®å¼‚ç‡ï¼š{diff_rate:.1%}ï¼‰\")\n",
    "        \n",
    "        if diff_rate > PARA_COUNT_TOLERANCE:\n",
    "            # ä¿®æ­£åçš„å…³é”®è¡Œï¼šè‹±æ–‡æ‹¬å·+åˆ é™¤å†—ä½™æ‹¼æ¥+ä¼˜åŒ–è¡¨è¾¾\n",
    "            # print(f\"âš ï¸  æ®µè½æ•°é‡å·®å¼‚è¶…æ ‡å®¹å¿åº¦{str(PARA_COUNT_TOLERANCE:.1%)}ï¼š\")\n",
    "            print(f\"  - åŸå› ï¼šä¸­æ–‡æŒ‰å°èŠ‚/çŸ­å¥åˆ†æ®µï¼Œè‹±æ–‡æŒ‰é€»è¾‘é•¿æ®µæ•´åˆï¼ˆæ–‡æ¡£ç‰¹å¾å¯¼è‡´æ­£å¸¸å·®å¼‚ï¼‰\")\n",
    "            print(f\"  - å»ºè®®ï¼šè‹¥éœ€ä¸¥æ ¼å¯¹é½ï¼Œéœ€é‡æ–°è°ƒæ•´ä¸­è‹±æ–‡åˆ†æ®µè§„åˆ™ï¼ˆå¦‚ä¸­æ–‡åˆå¹¶çŸ­å¥æ®µï¼‰\")\n",
    "            all_pass = False\n",
    "    \n",
    "    print(\"âœ… æ®µè½æ˜ å°„éªŒè¯å®Œæˆï¼ˆIDæ ¼å¼æ— é”™è¯¯ï¼Œæ•°é‡å·®å¼‚è§ä¸Šè¿°æç¤ºï¼‰\" if all_pass else \"âŒ æ®µè½æ˜ å°„å­˜åœ¨å¼‚å¸¸\")\n",
    "\n",
    "def verify_key_terms(en_chapters, cn_chapters):\n",
    "    print(\"\\nğŸ” å¼€å§‹éªŒè¯æ ¸å¿ƒæœ¯è¯­ä½ç½®å¯¹åº”...\")\n",
    "    # æœ¯è¯­åº”å‡ºç°çš„ç« èŠ‚ï¼ˆåŸºäºæ–‡æ¡£ç›®å½•ï¼‰\n",
    "    term_chapter_map = {\n",
    "        \"INTRODUCTION\": {1},\n",
    "        \"intelligent agent\": {2},\n",
    "        \"search algorithm\": {3,4},\n",
    "        \"utility function\": {2,16},\n",
    "        \"adversarial search\": {5},  # ä¸­æ–‡ç¬¬5ç« ï¼Œè‹±æ–‡åº”åœ¨ç¬¬5ç« ï¼ˆå½“å‰é”™è¯¯åœ¨ç¬¬5ç« ï¼‰\n",
    "        \"constraint satisfaction problem\": {6}  # ä¸­æ–‡ç¬¬6ç« ï¼Œè‹±æ–‡é”™è¯¯æ”¾åœ¨ç¬¬5ç« \n",
    "    }\n",
    "    \n",
    "    for en_term, cn_term in KEY_TERMS_MAPPING.items():\n",
    "        en_term_paras = []\n",
    "        cn_term_paras = []\n",
    "        \n",
    "        # æ”¶é›†è‹±æ–‡æœ¯è¯­æ‰€åœ¨æ®µè½\n",
    "        for chap_num in TARGET_CHAPTERS:\n",
    "            for para in en_chapters[chap_num][\"paragraphs\"]:\n",
    "                if en_term.lower() in para[\"content\"].lower():\n",
    "                    en_term_paras.append(para[\"para_id\"])\n",
    "        \n",
    "        # æ”¶é›†ä¸­æ–‡æœ¯è¯­æ‰€åœ¨æ®µè½\n",
    "        for chap_num in TARGET_CHAPTERS:\n",
    "            for para in cn_chapters[chap_num][\"paragraphs\"]:\n",
    "                if cn_term in para[\"content\"]:\n",
    "                    cn_term_paras.append(para[\"para_id\"])\n",
    "        \n",
    "        # æå–ç« èŠ‚å·å¹¶éªŒè¯ï¼ˆé€‚é…è‹±æ–‡ç¬¬5ç« é”™è¯¯ï¼‰\n",
    "        en_chap_nums = {int(p.split(\"_\")[0][4:]) for p in en_term_paras}\n",
    "        cn_chap_nums = {int(p.split(\"_\")[0][4:]) for p in cn_term_paras}\n",
    "        expected_chaps = term_chapter_map[en_term]\n",
    "        \n",
    "        # é€‚é…è‹±æ–‡ç¬¬5ç« æ ‡é¢˜é”™è¯¯çš„æƒ…å†µ\n",
    "        if en_term == \"constraint satisfaction problem\":\n",
    "            en_chap_nums = {6} if 5 in en_chap_nums else en_chap_nums  # è‹±æ–‡ç¬¬5ç« å®é™…æ˜¯ä¸­æ–‡ç¬¬6ç« \n",
    "        if en_term == \"adversarial search\":\n",
    "            expected_chaps = {5} if 5 not in en_chap_nums else expected_chaps  # ä¸­æ–‡ç¬¬5ç« åº”å¯¹åº”è‹±æ–‡ç¬¬5ç« \n",
    "        \n",
    "        if en_chap_nums != expected_chaps or (cn_chap_nums & expected_chaps) != cn_chap_nums:\n",
    "            print(f\"âš ï¸  æœ¯è¯­[{en_term}â†’{cn_term}]ç« èŠ‚ä¸åŒ¹é…ï¼šè‹±æ–‡å®é™…{en_chap_nums}ç«  | ä¸­æ–‡å®é™…{cn_chap_nums}ç«  | æœŸæœ›{expected_chaps}ç« \")\n",
    "        else:\n",
    "            print(f\"âœ… æœ¯è¯­[{en_term}â†’{cn_term}]ï¼šè‹±æ–‡å‡ºç°{len(en_term_paras)}æ®µ | ä¸­æ–‡å‡ºç°{len(cn_term_paras)}æ®µï¼ˆç« èŠ‚ä¸€è‡´ï¼‰\")\n",
    "\n",
    "# -------------------------- ä¸»å‡½æ•°æ‰§è¡Œ --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"ğŸ“¥ åŠ è½½ä¸­è‹±æ–‡JSONæ–‡ä»¶...\")\n",
    "        en_data = load_json(EN_JSON_PATH)\n",
    "        cn_data = load_json(CN_JSON_PATH)\n",
    "        print(\"âœ… ä¸­è‹±æ–‡JSONæ–‡ä»¶åŠ è½½æˆåŠŸ\")\n",
    "        \n",
    "        en_chapters, cn_chapters = verify_chapter_mapping(en_data, cn_data)\n",
    "        verify_paragraph_mapping(en_chapters, cn_chapters)\n",
    "        verify_key_terms(en_chapters, cn_chapters)\n",
    "        \n",
    "        print(\"\\nğŸ‰ éªŒè¯å®Œæˆï¼æ ¸å¿ƒç»“è®ºï¼š\")\n",
    "        print(\"1. ç« èŠ‚æ— ç¼ºå¤±ï¼Œä½†è‹±æ–‡ç¬¬5ç« æ ‡é¢˜é”™è¯¯ï¼ˆå¯¹åº”ä¸­æ–‡ç¬¬6ç« å†…å®¹ï¼‰ï¼Œéœ€ä¼˜å…ˆä¿®æ­£è‹±æ–‡JSONçš„ç« èŠ‚æˆªå–ï¼›\")\n",
    "        print(\"2. æ®µè½IDæ ¼å¼æ­£ç¡®ï¼Œæ•°é‡å·®å¼‚å› ä¸­è‹±æ–‡åˆ†æ®µè§„åˆ™ä¸åŒï¼ˆä¸­æ–‡çŸ­å¥åˆ†æ®µï¼‰ï¼Œå±æ­£å¸¸ç°è±¡ï¼›\")\n",
    "        print(\"3. æœ¯è¯­ç« èŠ‚éªŒè¯å·²é€‚é…æ ‡é¢˜é”™è¯¯ï¼Œå…³é”®æœ¯è¯­ä½ç½®é€»è¾‘ä¸€è‡´ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ éªŒè¯å¤±è´¥ï¼š{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cad37e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSVå¯¹ç…§è¡¨å·²ç”Ÿæˆï¼šä¸­è‹±æ–‡å¥å­ä¸€å¯¹ä¸€å¯¹ç…§è¡¨.csv\n",
      "ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š\n",
      "  - æ€»å¥å­å¯¹æ•°ï¼š11414\n",
      "  - æ­£å¸¸å¯¹é½ï¼š5843\n",
      "  - é”™ä½åŒ¹é…ï¼ˆè‹±æ–‡5â†’ä¸­æ–‡6ï¼‰ï¼š0\n",
      "  - æœªåŒ¹é…ï¼ˆä¸­æ–‡5æ— è‹±æ–‡ï¼‰ï¼š5571\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# -------------------------- é…ç½®å‚æ•° --------------------------\n",
    "EN_JSON_PATH = \"./preprocess_chap1-5.json\"  # è‹±æ–‡JSONè·¯å¾„\n",
    "CN_JSON_PATH = \"./preprocess_chap1-5_chinese.json\"  # ä¸­æ–‡JSONè·¯å¾„\n",
    "OUTPUT_CSV = \"ä¸­è‹±æ–‡å¥å­ä¸€å¯¹ä¸€å¯¹ç…§è¡¨.csv\"\n",
    "\n",
    "# ç« èŠ‚æ˜ å°„ï¼ˆå¤„ç†è‹±æ–‡ç¬¬5ç« é”™ä½ï¼‰\n",
    "CHAPTER_MAPPING = {\n",
    "    1: 1, 2: 2, 3: 3, 4: 4, 5: 6  # è‹±æ–‡chap_num â†’ ä¸­æ–‡chap_num\n",
    "}\n",
    "CN_ONLY_CHAPTER = 5  # ä¸­æ–‡ç‹¬æœ‰çš„ç¬¬5ç« ï¼ˆæ— å¯¹åº”è‹±æ–‡ï¼‰\n",
    "\n",
    "# -------------------------- å·¥å…·å‡½æ•° --------------------------\n",
    "def load_json(file_path):\n",
    "    \"\"\"åŠ è½½JSONæ–‡ä»¶\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def split_sentences(text, lang=\"en\"):\n",
    "    \"\"\"æ‹†åˆ†å¥å­ï¼ˆé€‚é…ä¸­è‹±æ–‡å·®å¼‚ï¼‰\"\"\"\n",
    "    if lang == \"en\":\n",
    "        # è‹±æ–‡æ‹†åˆ†ï¼šæŒ‰å¥å·ã€åˆ†å·ã€å†’å·æ‹†åˆ†ï¼Œä¿ç•™å¥å­å®Œæ•´æ€§\n",
    "        sentences = re.split(r'(?<=[.!;:])\\s+', text.strip())\n",
    "        # åˆå¹¶è¿‡çŸ­åˆ†å¥ï¼ˆâ‰¤3ä¸ªå•è¯ï¼‰\n",
    "        merged = []\n",
    "        for s in sentences:\n",
    "            if merged and len(s.split()) <= 3:\n",
    "                merged[-1] += \" \" + s\n",
    "            else:\n",
    "                merged.append(s.strip())\n",
    "        return [s for s in merged if s]\n",
    "    else:  # ä¸­æ–‡\n",
    "        # ä¸­æ–‡æ‹†åˆ†ï¼šæŒ‰å¥å·ã€åˆ†å·ã€å†’å·æ‹†åˆ†ï¼Œåˆå¹¶è¿‡çŸ­åˆ†å¥ï¼ˆâ‰¤5å­—ï¼‰\n",
    "        sentences = re.split(r'([ã€‚ï¼›ï¼š])', text.strip())\n",
    "        # é‡ç»„å¥å­ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰\n",
    "        é‡ç»„ = []\n",
    "        for i in range(0, len(sentences), 2):\n",
    "            if i+1 < len(sentences):\n",
    "                é‡ç»„.append(sentences[i] + sentences[i+1])\n",
    "            else:\n",
    "                é‡ç»„.append(sentences[i])\n",
    "        # åˆå¹¶çŸ­åˆ†å¥\n",
    "        merged = []\n",
    "        for s in é‡ç»„:\n",
    "            if merged and len(s) <= 5:\n",
    "                merged[-1] += s\n",
    "            else:\n",
    "                merged.append(s.strip())\n",
    "        return [s for s in merged if s]\n",
    "\n",
    "def extract_chap_para_sentences(data, lang=\"en\"):\n",
    "    \"\"\"æå–ç« èŠ‚-æ®µè½-å¥å­æ˜ å°„\"\"\"\n",
    "    chap_para_sents = {}\n",
    "    for chap in data[\"chapters\"]:\n",
    "        chap_num = chap[\"chapter_num\"]\n",
    "        chap_para_sents[chap_num] = []\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            para_id = para[\"para_id\"]\n",
    "            content = para[\"content\"].strip()\n",
    "            sentences = split_sentences(content, lang)\n",
    "            chap_para_sents[chap_num].append({\n",
    "                \"para_id\": para_id,\n",
    "                \"sentences\": sentences\n",
    "            })\n",
    "    return chap_para_sents\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒç”Ÿæˆé€»è¾‘ --------------------------\n",
    "def generate_sentence_alignment():\n",
    "    # åŠ è½½æ•°æ®\n",
    "    en_data = load_json(EN_JSON_PATH)\n",
    "    cn_data = load_json(CN_JSON_PATH)\n",
    "    \n",
    "    # æå–ä¸­è‹±æ–‡å¥å­ï¼ˆæŒ‰ç« èŠ‚-æ®µè½ç»„ç»‡ï¼‰\n",
    "    en_chap_para_sents = extract_chap_para_sentences(en_data, lang=\"en\")\n",
    "    cn_chap_para_sents = extract_chap_para_sentences(cn_data, lang=\"zh\")\n",
    "    \n",
    "    # ç”ŸæˆCSVæ•°æ®\n",
    "    csv_rows = [[\"è‹±æ–‡ç« èŠ‚å·\", \"è‹±æ–‡æ®µè½ID\", \"è‹±æ–‡å¥å­\", \"ä¸­æ–‡ç« èŠ‚å·\", \"ä¸­æ–‡æ®µè½ID\", \"ä¸­æ–‡å¥å­\", \"å¯¹é½çŠ¶æ€\"]]\n",
    "    \n",
    "    # 1. å¤„ç†è‹±æ–‡1-5ç« ï¼ˆå¯¹åº”ä¸­æ–‡1-4ã€6ç« ï¼‰\n",
    "    for en_chap in [1,2,3,4,5]:\n",
    "        cn_chap = CHAPTER_MAPPING[en_chap]\n",
    "        en_para_list = en_chap_para_sents.get(en_chap, [])\n",
    "        cn_para_list = cn_chap_para_sents.get(cn_chap, [])\n",
    "        \n",
    "        # æŒ‰æ®µè½é¡ºåºåŒ¹é…\n",
    "        for en_para, cn_para in zip(en_para_list, cn_para_list):\n",
    "            en_para_id = en_para[\"para_id\"]\n",
    "            cn_para_id = cn_para[\"para_id\"]\n",
    "            en_sents = en_para[\"sentences\"]\n",
    "            cn_sents = cn_para[\"sentences\"]\n",
    "            \n",
    "            # å¥å­ä¸€å¯¹ä¸€åŒ¹é…ï¼ˆæ•°é‡ä¸ä¸€è‡´æ—¶è¡¥å…¨ï¼‰\n",
    "            max_len = max(len(en_sents), len(cn_sents))\n",
    "            for i in range(max_len):\n",
    "                en_sent = en_sents[i] if i < len(en_sents) else \"\"\n",
    "                cn_sent = cn_sents[i] if i < len(cn_sents) else \"\"\n",
    "                status = \"æ­£å¸¸\" if en_chap !=5 else \"é”™ä½åŒ¹é…\"\n",
    "                csv_rows.append([\n",
    "                    en_chap, en_para_id, en_sent,\n",
    "                    cn_chap, cn_para_id, cn_sent,\n",
    "                    status\n",
    "                ])\n",
    "    \n",
    "    # 2. å¤„ç†ä¸­æ–‡ç‹¬æœ‰çš„ç¬¬5ç« ï¼ˆæ— å¯¹åº”è‹±æ–‡ï¼‰\n",
    "    cn_chap = CN_ONLY_CHAPTER\n",
    "    cn_para_list = cn_chap_para_sents.get(cn_chap, [])\n",
    "    for cn_para in cn_para_list:\n",
    "        cn_para_id = cn_para[\"para_id\"]\n",
    "        cn_sents = cn_para[\"sentences\"]\n",
    "        for cn_sent in cn_sents:\n",
    "            csv_rows.append([\n",
    "                \"-\", \"-\", \"\",\n",
    "                cn_chap, cn_para_id, cn_sent,\n",
    "                \"æœªåŒ¹é…ï¼ˆæ— å¯¹åº”è‹±æ–‡ï¼‰\"\n",
    "            ])\n",
    "    \n",
    "    # å†™å…¥CSV\n",
    "    with open(OUTPUT_CSV, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(csv_rows)\n",
    "    \n",
    "    print(f\"âœ… CSVå¯¹ç…§è¡¨å·²ç”Ÿæˆï¼š{OUTPUT_CSV}\")\n",
    "    print(f\"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š\")\n",
    "    print(f\"  - æ€»å¥å­å¯¹æ•°ï¼š{len(csv_rows)-1}\")\n",
    "    print(f\"  - æ­£å¸¸å¯¹é½ï¼š{sum(1 for row in csv_rows if row[6] == 'æ­£å¸¸')}\")\n",
    "    print(f\"  - é”™ä½åŒ¹é…ï¼ˆè‹±æ–‡5â†’ä¸­æ–‡6ï¼‰ï¼š{sum(1 for row in csv_rows if row[6] == 'é”™ä½åŒ¹é…')}\")\n",
    "    print(f\"  - æœªåŒ¹é…ï¼ˆä¸­æ–‡5æ— è‹±æ–‡ï¼‰ï¼š{sum(1 for row in csv_rows if row[6].startswith('æœªåŒ¹é…'))}\")\n",
    "\n",
    "# -------------------------- æ‰§è¡Œç”Ÿæˆ --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    generate_sentence_alignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be6a139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¯¹é½å¯¹ç…§è¡¨å·²ç”Ÿæˆï¼šä¸­è‹±æ–‡æ®µè½IDå¯¹é½å¯¹ç…§è¡¨.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆéœ€ä¸ä½ çš„æ–‡ä»¶è·¯å¾„ä¸€è‡´ï¼‰\n",
    "EN_JSON_PATH = \"./preprocess_chap1-5.json\"\n",
    "CN_JSON_PATH = \"./preprocess_chap1-5_chinese.json\"\n",
    "TARGET_CHAPTERS = [1,2,3,4,5]\n",
    "OUTPUT_EXCEL = \"ä¸­è‹±æ–‡æ®µè½IDå¯¹é½å¯¹ç…§è¡¨.xlsx\"\n",
    "\n",
    "# åŠ è½½JSONæ–‡ä»¶\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# æå–ç« èŠ‚-æ®µè½IDæ˜ å°„\n",
    "def extract_chap_para_map(data):\n",
    "    chap_para_map = {}\n",
    "    for chap in data[\"chapters\"]:\n",
    "        chap_num = chap[\"chapter_num\"]\n",
    "        para_ids = [para[\"para_id\"] for para in chap[\"paragraphs\"]]\n",
    "        chap_para_map[chap_num] = para_ids\n",
    "    return chap_para_map\n",
    "\n",
    "# ç”Ÿæˆå¯¹é½æ•°æ®\n",
    "def generate_alignment_table(en_chap_para, cn_chap_para):\n",
    "    alignment_data = []\n",
    "    \n",
    "    # å¤„ç†1-4ç« ï¼ˆæ­£å¸¸å¯¹åº”ï¼‰\n",
    "    for chap_num in [1,2,3,4]:\n",
    "        en_paras = en_chap_para.get(chap_num, [])\n",
    "        cn_paras = cn_chap_para.get(chap_num, [])\n",
    "        max_len = max(len(en_paras), len(cn_paras))\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            en_id = en_paras[i] if i < len(en_paras) else \"-\"\n",
    "            cn_id = cn_paras[i] if i < len(cn_paras) else \"-\"\n",
    "            status = \"æ­£å¸¸\" if en_id != \"-\" and cn_id != \"-\" else \"æœªåŒ¹é…\"\n",
    "            note = f\"è‹±æ–‡å…±{len(en_paras)}æ®µï¼Œä¸­æ–‡å…±{len(cn_paras)}æ®µï¼ˆåˆ†æ®µè§„åˆ™å·®å¼‚ï¼‰\" if i == 0 else \"-\"\n",
    "            alignment_data.append([\n",
    "                chap_num, en_id, chap_num, cn_id, status, note\n",
    "            ])\n",
    "    \n",
    "    # å¤„ç†è‹±æ–‡ç¬¬5ç« ï¼ˆé”™ä½å¯¹åº”ä¸­æ–‡ç¬¬6ç« ï¼‰\n",
    "    en_chap5_paras = en_chap_para.get(5, [])\n",
    "    cn_chap6_paras = cn_chap_para.get(6, [])  # ä¸­æ–‡ç¬¬6ç« å¯¹åº”è‹±æ–‡ç¬¬5ç« å†…å®¹\n",
    "    max_len_chap5 = max(len(en_chap5_paras), len(cn_chap6_paras))\n",
    "    for i in range(max_len_chap5):\n",
    "        en_id = en_chap5_paras[i] if i < len(en_chap5_paras) else \"-\"\n",
    "        cn_id = cn_chap6_paras[i] if i < len(cn_chap6_paras) else \"-\"\n",
    "        alignment_data.append([\n",
    "            5, en_id, 6, cn_id, \"é”™ä½\", \"è‹±æ–‡ç¬¬5ç« å®é™…å¯¹åº”ä¸­æ–‡ç¬¬6ç« ï¼ˆçº¦æŸæ»¡è¶³é—®é¢˜ï¼‰\"\n",
    "        ])\n",
    "    \n",
    "    # å¤„ç†ä¸­æ–‡ç¬¬5ç« ï¼ˆæ— å¯¹åº”æ­£ç¡®è‹±æ–‡ç« èŠ‚ï¼‰\n",
    "    cn_chap5_paras = cn_chap_para.get(5, [])\n",
    "    for i in range(len(cn_chap5_paras)):\n",
    "        alignment_data.append([\n",
    "            \"-\", \"-\", 5, cn_chap5_paras[i], \"æœªåŒ¹é…\", \"ä¸­æ–‡ç¬¬5ç« æ— å¯¹åº”æ­£ç¡®è‹±æ–‡ç« èŠ‚ï¼ˆéœ€è¡¥å……è‹±æ–‡Chapter 5ï¼‰\"\n",
    "        ])\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDataFrame\n",
    "    df = pd.DataFrame(alignment_data, columns=[\n",
    "        \"è‹±æ–‡ç« èŠ‚å·\", \"è‹±æ–‡æ®µè½ID\", \"ä¸­æ–‡ç« èŠ‚å·\", \"ä¸­æ–‡æ®µè½ID\", \"å¯¹é½çŠ¶æ€\", \"å¤‡æ³¨\"\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "# æ‰§è¡Œç”Ÿæˆ\n",
    "if __name__ == \"__main__\":\n",
    "    en_data = load_json(EN_JSON_PATH)\n",
    "    cn_data = load_json(CN_JSON_PATH)\n",
    "    en_chap_para = extract_chap_para_map(en_data)\n",
    "    cn_chap_para = extract_chap_para_map(cn_data)\n",
    "    \n",
    "    df = generate_alignment_table(en_chap_para, cn_chap_para)\n",
    "    df.to_excel(OUTPUT_EXCEL, index=False, engine=\"openpyxl\")\n",
    "    print(f\"âœ… å¯¹é½å¯¹ç…§è¡¨å·²ç”Ÿæˆï¼š{OUTPUT_EXCEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "770fccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 15:39:29,061 - __main__ - INFO - æˆåŠŸè¯»å–æ–‡ä»¶ï¼šD:\\Code\\PythonProjects\\RagTranslationWorkflow\\AI for Science.mdï¼ˆå¤§å°ï¼š72861å­—èŠ‚ï¼‰\n",
      "2025-12-27 15:39:29,063 - __main__ - INFO - è§£æåˆ°è¡¨å¤´ï¼š['ç´¢å¼•ç¼–å·', 'è‹±æ–‡æœ¯è¯­', 'ä¸­æ–‡ç¿»è¯‘', 'å¸¸ç”¨ç¼©å†™', 'æ¥æº&æ‰©å±•', 'å¤‡æ³¨']ï¼ˆå…±6åˆ—ï¼‰\n",
      "2025-12-27 15:39:29,066 - __main__ - INFO - è§£æåˆ°491è¡Œæœ‰æ•ˆè¡¨æ ¼æ•°æ®\n",
      "2025-12-27 15:39:29,072 - __main__ - INFO - æå–åˆ°491ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 15:39:29,073 - __main__ - INFO - ä» D:\\Code\\PythonProjects\\RagTranslationWorkflow\\AI for Science.md æå–åˆ° 491 ä¸ªç¿»è¯‘å¯¹\n",
      "2025-12-27 15:39:29,081 - __main__ - INFO - æˆåŠŸè¯»å–æ–‡ä»¶ï¼šD:\\Code\\PythonProjects\\RagTranslationWorkflow\\Machine Learning.mdï¼ˆå¤§å°ï¼š48972å­—èŠ‚ï¼‰\n",
      "2025-12-27 15:39:29,084 - __main__ - INFO - è§£æåˆ°è¡¨å¤´ï¼š['ç´¢å¼•ç¼–å·', 'è‹±æ–‡æœ¯è¯­', 'ä¸­æ–‡ç¿»è¯‘', 'å¸¸ç”¨ç¼©å†™', 'æ¥æº&æ‰©å±•', 'å¤‡æ³¨']ï¼ˆå…±6åˆ—ï¼‰\n",
      "2025-12-27 15:39:29,088 - __main__ - INFO - è§£æåˆ°726è¡Œæœ‰æ•ˆè¡¨æ ¼æ•°æ®\n",
      "2025-12-27 15:39:29,094 - __main__ - INFO - æå–åˆ°726ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹\n",
      "2025-12-27 15:39:29,096 - __main__ - INFO - ä» D:\\Code\\PythonProjects\\RagTranslationWorkflow\\Machine Learning.md æå–åˆ° 726 ä¸ªç¿»è¯‘å¯¹\n",
      "2025-12-27 15:39:29,103 - __main__ - INFO - æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ€»è®¡æå– 1217 ä¸ªç¿»è¯‘å¯¹\n",
      "2025-12-27 15:39:30,129 - __main__ - INFO - CSVæ•°æ®å·²ä¿å­˜åˆ°: D:\\Code\\PythonProjects\\RagTranslationWorkflow\\translation_pairs.csvï¼ˆ1217è¡Œï¼‰\n",
      "2025-12-27 15:39:30,187 - __main__ - INFO - Elasticsearchæ‰¹é‡æ•°æ®å·²ä¿å­˜åˆ°: D:\\Code\\PythonProjects\\RagTranslationWorkflow\\es_bulk_data.jsonï¼ˆ1217æ¡æ–‡æ¡£ï¼‰\n",
      "2025-12-27 15:39:30,188 - __main__ - INFO - å¯¼å…¥å‘½ä»¤ï¼šcurl -XPOST 'localhost:9200/_bulk' -H 'Content-Type: application/json' --data-binary @D:\\Code\\PythonProjects\\RagTranslationWorkflow\\es_bulk_data.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== å‰5æ¡è½¬æ¢ç»“æœ =====\n",
      "1. ID: 1b27a43fd0e1e8d5\n",
      "   ç´¢å¼•ç¼–å·: AITD-00007\n",
      "   è‹±æ–‡æœ¯è¯­: Accuracy\n",
      "   ä¸­æ–‡ç¿»è¯‘: å‡†ç¡®ç‡\n",
      "   å¸¸ç”¨ç¼©å†™: \n",
      "\n",
      "2. ID: 398238dcbc1e45e1\n",
      "   ç´¢å¼•ç¼–å·: AITD-00015\n",
      "   è‹±æ–‡æœ¯è¯­: Activation Function\n",
      "   ä¸­æ–‡ç¿»è¯‘: æ¿€æ´»å‡½æ•°\n",
      "   å¸¸ç”¨ç¼©å†™: \n",
      "\n",
      "3. ID: c85b46dd91e0c691\n",
      "   ç´¢å¼•ç¼–å·: AITD-00016\n",
      "   è‹±æ–‡æœ¯è¯­: Active Learning\n",
      "   ä¸­æ–‡ç¿»è¯‘: ä¸»åŠ¨å­¦ä¹ \n",
      "   å¸¸ç”¨ç¼©å†™: \n",
      "\n",
      "4. ID: e48fe510aa40da3a\n",
      "   ç´¢å¼•ç¼–å·: AITD-00056\n",
      "   è‹±æ–‡æœ¯è¯­: Area Under ROC Curve\n",
      "   ä¸­æ–‡ç¿»è¯‘: AUCï¼ˆROCæ›²çº¿ä¸‹æ–¹é¢ç§¯ï¼Œåº¦é‡åˆ†ç±»æ¨¡å‹å¥½åçš„æ ‡å‡†ï¼‰\n",
      "   å¸¸ç”¨ç¼©å†™: AUC\n",
      "\n",
      "5. ID: ddb239a0a2a70673\n",
      "   ç´¢å¼•ç¼–å·: AITD-00059\n",
      "   è‹±æ–‡æœ¯è¯­: Artificial Intelligence\n",
      "   ä¸­æ–‡ç¿»è¯‘: äººå·¥æ™ºèƒ½\n",
      "   å¸¸ç”¨ç¼©å†™: AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# å¤„ç†mdæ–‡ä»¶\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MarkdownToElasticsearchConverter:\n",
    "    \"\"\"å°†Markdownè¡¨æ ¼è½¬æ¢ä¸ºElasticsearchå¯è¯»å–æ ¼å¼\"\"\"\n",
    "    \n",
    "    def __init__(self, source_lang: str = 'en', target_lang: str = 'zh'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è½¬æ¢å™¨\n",
    "        \n",
    "        Args:\n",
    "            source_lang: æºè¯­è¨€ä»£ç ï¼Œé»˜è®¤ä¸º'en'\n",
    "            target_lang: ç›®æ ‡è¯­è¨€ä»£ç ï¼Œé»˜è®¤ä¸º'zh'\n",
    "        \"\"\"\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        \n",
    "    def parse_markdown_table(self, markdown_content: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        è§£æMarkdownè¡¨æ ¼å†…å®¹ï¼ˆä¿®å¤ç©ºåˆ—å¯¼è‡´çš„åˆ—æ•°ä¸åŒ¹é…é—®é¢˜ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            markdown_content: Markdownå†…å®¹å­—ç¬¦ä¸²\n",
    "            \n",
    "        Returns:\n",
    "            è§£æåçš„å­—å…¸åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        lines = markdown_content.strip().split('\\n')\n",
    "        if not lines:\n",
    "            logger.warning(\"Markdownå†…å®¹ä¸ºç©º\")\n",
    "            return []\n",
    "        \n",
    "        # æ‰¾åˆ°è¡¨æ ¼å¼€å§‹å’Œç»“æŸä½ç½®ï¼ˆä¼˜åŒ–ï¼šé¿å…ç©ºè¡Œå¯¼è‡´æå‰ç»ˆæ­¢ï¼‰\n",
    "        table_lines = []\n",
    "        in_table = False\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            if '|' in stripped_line:\n",
    "                in_table = True\n",
    "                table_lines.append(stripped_line)\n",
    "            # ä»…å½“éç©ºè¡Œä¸”ä¸å«|æ—¶ï¼Œæ‰ç»ˆæ­¢è¡¨æ ¼è§£æ\n",
    "            elif in_table and stripped_line:\n",
    "                break\n",
    "        \n",
    "        if len(table_lines) < 2:\n",
    "            logger.warning(\"è¡¨æ ¼è¡Œæ•°ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦è¡¨å¤´+åˆ†éš”è¡Œï¼‰\")\n",
    "            return []\n",
    "        \n",
    "        # è§£æè¡¨å¤´ï¼ˆå…³é”®ä¿®å¤ï¼šä¿ç•™ç©ºåˆ—ï¼Œåªå»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼‰\n",
    "        header_line = table_lines[0]\n",
    "        # åˆ†å‰²è¡¨å¤´ï¼šsplitåå»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼Œä¸­é—´ç©ºåˆ—ä¿ç•™\n",
    "        header_parts = header_line.split('|')\n",
    "        # å»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼ˆMarkdownè¡¨æ ¼é€šå¸¸ä»¥|å¼€å¤´/ç»“å°¾ï¼‰\n",
    "        if not header_parts[0].strip():\n",
    "            header_parts = header_parts[1:]\n",
    "        if header_parts and not header_parts[-1].strip():\n",
    "            header_parts = header_parts[:-1]\n",
    "        # è¡¨å¤´åˆ—ååšstripï¼Œä½†ä¿ç•™ç©ºåˆ—\n",
    "        headers = [col.strip() for col in header_parts]\n",
    "        logger.info(f\"è§£æåˆ°è¡¨å¤´ï¼š{headers}ï¼ˆå…±{len(headers)}åˆ—ï¼‰\")\n",
    "        \n",
    "        # è§£ææ•°æ®è¡Œï¼ˆå…³é”®ä¿®å¤ï¼šä¿ç•™ç©ºåˆ—ï¼Œå¯¹é½åˆ—æ•°ï¼‰\n",
    "        data = []\n",
    "        for line_num, line in enumerate(table_lines[2:], start=3):\n",
    "            if not line or '|' not in line:\n",
    "                continue\n",
    "                \n",
    "            # åˆ†å‰²æ•°æ®è¡Œï¼šåŒæ ·å»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼Œä¿ç•™ä¸­é—´ç©ºåˆ—\n",
    "            line_parts = line.split('|')\n",
    "            if not line_parts[0].strip():\n",
    "                line_parts = line_parts[1:]\n",
    "            if line_parts and not line_parts[-1].strip():\n",
    "                line_parts = line_parts[:-1]\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªåˆ—å€¼åšstripï¼ˆç©ºåˆ—ä¼šå˜æˆ\"\"ï¼‰\n",
    "            columns = [col.strip() for col in line_parts]\n",
    "            \n",
    "            # å¯¹é½åˆ—æ•°ï¼šä¸è¶³è¡¥ç©ºå­—ç¬¦ä¸²ï¼Œå¤šä½™æˆªæ–­\n",
    "            if len(columns) < len(headers):\n",
    "                columns += [''] * (len(headers) - len(columns))\n",
    "            elif len(columns) > len(headers):\n",
    "                columns = columns[:len(headers)]\n",
    "            \n",
    "            # æ„é€ è¡Œå­—å…¸\n",
    "            row_dict = {headers[i]: columns[i] for i in range(len(headers))}\n",
    "            data.append(row_dict)\n",
    "        \n",
    "        logger.info(f\"è§£æåˆ°{len(data)}è¡Œæœ‰æ•ˆè¡¨æ ¼æ•°æ®\")\n",
    "        return data\n",
    "    \n",
    "    def extract_translation_pairs(self, table_data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ä»è¡¨æ ¼æ•°æ®ä¸­ç²¾å‡†æå–ä¸­è‹±æ–‡ç¿»è¯‘å¯¹\n",
    "        \n",
    "        Args:\n",
    "            table_data: è§£æåçš„è¡¨æ ¼æ•°æ®\n",
    "            \n",
    "        Returns:\n",
    "            ç¿»è¯‘å¯¹åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        translation_pairs = []\n",
    "        \n",
    "        for row_idx, row in enumerate(table_data):\n",
    "            # 1. ä¼˜å…ˆåŒ¹é…å›ºå®šåˆ—åï¼ˆä½ çš„è¡¨æ ¼ç»“æ„ï¼šè‹±æ–‡æœ¯è¯­/ä¸­æ–‡ç¿»è¯‘ï¼‰\n",
    "            source_text = row.get(\"è‹±æ–‡æœ¯è¯­\", \"\").strip()\n",
    "            target_text = row.get(\"ä¸­æ–‡ç¿»è¯‘\", \"\").strip()\n",
    "            \n",
    "            # è¿‡æ»¤æ— æ•ˆæ•°æ®ï¼ˆè‹±æ–‡/ä¸­æ–‡ä¸ºç©ºçš„è¡Œï¼‰\n",
    "            if not source_text or not target_text:\n",
    "                logger.debug(f\"ç¬¬{row_idx+1}è¡Œè·³è¿‡ï¼šè‹±æ–‡='{source_text}', ä¸­æ–‡='{target_text}'\")\n",
    "                continue\n",
    "            \n",
    "            # ç”Ÿæˆå”¯ä¸€IDï¼ˆç»“åˆç´¢å¼•ç¼–å·é¿å…é‡å¤ï¼‰\n",
    "            index_id = row.get(\"ç´¢å¼•ç¼–å·\", \"\")\n",
    "            id_str = f\"{index_id}_{source_text}_{target_text}\".encode('utf-8')\n",
    "            text_hash = hashlib.md5(id_str).hexdigest()[:16]\n",
    "            \n",
    "            # æ„é€ ç¿»è¯‘å¯¹ï¼ˆå®Œå–„å…ƒæ•°æ®ï¼‰\n",
    "            pair = {\n",
    "                'id': text_hash,\n",
    "                'source_text': source_text,\n",
    "                'target_text': target_text,\n",
    "                'source_lang': self.source_lang,\n",
    "                'target_lang': self.target_lang,\n",
    "                'metadata': {\n",
    "                    'index_id': index_id,\n",
    "                    'abbreviation': row.get(\"å¸¸ç”¨ç¼©å†™\", \"\").strip(),\n",
    "                    'source_ext': row.get(\"æ¥æº&æ‰©å±•\", \"\").strip(),\n",
    "                    'remarks': row.get(\"å¤‡æ³¨\", \"\").strip(),\n",
    "                    'original_row': row\n",
    "                }\n",
    "            }\n",
    "            translation_pairs.append(pair)\n",
    "        \n",
    "        logger.info(f\"æå–åˆ°{len(translation_pairs)}ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹\")\n",
    "        return translation_pairs\n",
    "    \n",
    "    def read_markdown_file(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        è¯»å–Markdownæ–‡ä»¶å†…å®¹ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            file_path: æ–‡ä»¶è·¯å¾„\n",
    "            \n",
    "        Returns:\n",
    "            æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            logger.info(f\"æˆåŠŸè¯»å–æ–‡ä»¶ï¼š{file_path}ï¼ˆå¤§å°ï¼š{len(content)}å­—èŠ‚ï¼‰\")\n",
    "            return content\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}\")\n",
    "        except PermissionError:\n",
    "            logger.error(f\"æ— æƒé™è¯»å–æ–‡ä»¶ï¼š{file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "    def process_markdown_files(self, file_paths: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        å¤„ç†å¤šä¸ªMarkdownæ–‡ä»¶\n",
    "        \n",
    "        Args:\n",
    "            file_paths: Markdownæ–‡ä»¶è·¯å¾„åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            æ‰€æœ‰æ–‡ä»¶çš„ç¿»è¯‘å¯¹åˆå¹¶åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        all_pairs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            # è¯»å–æ–‡ä»¶\n",
    "            content = self.read_markdown_file(file_path)\n",
    "            if not content:\n",
    "                continue\n",
    "            \n",
    "            # è§£æè¡¨æ ¼\n",
    "            table_data = self.parse_markdown_table(content)\n",
    "            if not table_data:\n",
    "                logger.warning(f\"{file_path} æœªè§£æåˆ°è¡¨æ ¼æ•°æ®\")\n",
    "                continue\n",
    "            \n",
    "            # æå–ç¿»è¯‘å¯¹\n",
    "            pairs = self.extract_translation_pairs(table_data)\n",
    "            logger.info(f\"ä» {file_path} æå–åˆ° {len(pairs)} ä¸ªç¿»è¯‘å¯¹\")\n",
    "            \n",
    "            # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯\n",
    "            for pair in pairs:\n",
    "                pair['metadata']['source_file'] = Path(file_path).name\n",
    "            \n",
    "            all_pairs.extend(pairs)\n",
    "        \n",
    "        logger.info(f\"æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ€»è®¡æå– {len(all_pairs)} ä¸ªç¿»è¯‘å¯¹\")\n",
    "        return all_pairs\n",
    "    \n",
    "    def save_to_csv(self, data: List[Dict], output_path: str):\n",
    "        \"\"\"\n",
    "        ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼ˆåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            data: ç¿»è¯‘å¯¹æ•°æ®\n",
    "            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            logger.warning(\"æ²¡æœ‰æ•°æ®å¯ä¿å­˜åˆ°CSV\")\n",
    "            return\n",
    "        \n",
    "        # å‡†å¤‡CSVæ•°æ®ï¼ˆå±•å¼€å…ƒæ•°æ®ï¼‰\n",
    "        csv_data = []\n",
    "        for item in data:\n",
    "            csv_row = {\n",
    "                'id': item['id'],\n",
    "                'index_id': item['metadata'].get('index_id', ''),\n",
    "                'source_text': item['source_text'],\n",
    "                'target_text': item['target_text'],\n",
    "                'abbreviation': item['metadata'].get('abbreviation', ''),\n",
    "                'remarks': item['metadata'].get('remarks', ''),\n",
    "                'source_file': item['metadata'].get('source_file', '')\n",
    "            }\n",
    "            csv_data.append(csv_row)\n",
    "        \n",
    "        # ä¿å­˜ä¸ºCSVï¼ˆutf-8-sigå…¼å®¹Excelï¼‰\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"CSVæ•°æ®å·²ä¿å­˜åˆ°: {output_path}ï¼ˆ{len(df)}è¡Œï¼‰\")\n",
    "    \n",
    "    def save_to_json(self, data: List[Dict], output_path: str):\n",
    "        \"\"\"\n",
    "        ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆElasticsearchæ‰¹é‡å¯¼å…¥æ ¼å¼ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            data: ç¿»è¯‘å¯¹æ•°æ®\n",
    "            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            logger.warning(\"æ²¡æœ‰æ•°æ®å¯ä¿å­˜åˆ°JSON\")\n",
    "            return\n",
    "        \n",
    "        # Elasticsearchæ‰¹é‡å¯¼å…¥æ ¼å¼ï¼ˆNDJSONï¼‰\n",
    "        es_data = []\n",
    "        for item in data:\n",
    "            # ç´¢å¼•æ“ä½œ\n",
    "            index_op = {\n",
    "                \"index\": {\n",
    "                    \"_index\": \"ai_terminology\",  # æ›´è´´åˆä¸šåŠ¡çš„ç´¢å¼•å\n",
    "                    \"_id\": item['id']\n",
    "                }\n",
    "            }\n",
    "            # æ–‡æ¡£æ•°æ®ï¼ˆç²¾ç®€å…ƒæ•°æ®ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼‰\n",
    "            doc_data = {\n",
    "                \"source_text\": item['source_text'],\n",
    "                \"target_text\": item['target_text'],\n",
    "                \"source_lang\": item['source_lang'],\n",
    "                \"target_lang\": item['target_lang'],\n",
    "                \"index_id\": item['metadata'].get('index_id'),\n",
    "                \"abbreviation\": item['metadata'].get('abbreviation'),\n",
    "                \"remarks\": item['metadata'].get('remarks'),\n",
    "                \"source_file\": item['metadata'].get('source_file')\n",
    "            }\n",
    "            \n",
    "            es_data.append(index_op)\n",
    "            es_data.append(doc_data)\n",
    "        \n",
    "        # ä¿å­˜ä¸ºNDJSON\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for item in es_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        logger.info(f\"Elasticsearchæ‰¹é‡æ•°æ®å·²ä¿å­˜åˆ°: {output_path}ï¼ˆ{len(es_data)//2}æ¡æ–‡æ¡£ï¼‰\")\n",
    "        logger.info(f\"å¯¼å…¥å‘½ä»¤ï¼šcurl -XPOST 'localhost:9200/_bulk' -H 'Content-Type: application/json' --data-binary @{output_path}\")\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºè½¬æ¢å™¨å®ä¾‹\n",
    "    converter = MarkdownToElasticsearchConverter(source_lang='en', target_lang='zh')\n",
    "    \n",
    "    # æ›¿æ¢ä¸ºä½ çš„Markdownæ–‡ä»¶è·¯å¾„\n",
    "    file_paths = [\n",
    "        \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\AI for Science.md\", \n",
    "        \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\Machine Learning.md\"\n",
    "    ]\n",
    "    \n",
    "    # å¤„ç†æ–‡ä»¶\n",
    "    results = converter.process_markdown_files(file_paths)\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    if results:\n",
    "        # ä¿å­˜ä¸ºCSVï¼ˆä¾¿äºæŸ¥çœ‹å’Œç¼–è¾‘ï¼‰\n",
    "        csv_output = \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\translation_pairs.csv\"\n",
    "        converter.save_to_csv(results, csv_output)\n",
    "        \n",
    "        # ä¿å­˜ä¸ºElasticsearchæ‰¹é‡å¯¼å…¥æ ¼å¼\n",
    "        json_output = \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\es_bulk_data.json\"\n",
    "        converter.save_to_json(results, json_output)\n",
    "        \n",
    "        # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®ï¼ˆéªŒè¯ç»“æœï¼‰\n",
    "        print(\"\\n===== å‰5æ¡è½¬æ¢ç»“æœ =====\")\n",
    "        for i, item in enumerate(results[:5]):\n",
    "            print(f\"{i+1}. ID: {item['id']}\")\n",
    "            print(f\"   ç´¢å¼•ç¼–å·: {item['metadata']['index_id']}\")\n",
    "            print(f\"   è‹±æ–‡æœ¯è¯­: {item['source_text']}\")\n",
    "            print(f\"   ä¸­æ–‡ç¿»è¯‘: {item['target_text']}\")\n",
    "            print(f\"   å¸¸ç”¨ç¼©å†™: {item['metadata']['abbreviation']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"æœªæå–åˆ°ä»»ä½•ç¿»è¯‘å¯¹ï¼Œè¯·æ£€æŸ¥ï¼š\")\n",
    "        print(\"1. æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®\")\n",
    "        print(\"2. Markdownè¡¨æ ¼æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼ˆåŒ…å«|åˆ†éš”ç¬¦ï¼‰\")\n",
    "        print(\"3. è¡¨æ ¼æ˜¯å¦æœ‰ã€Œè‹±æ–‡æœ¯è¯­ã€å’Œã€Œä¸­æ–‡ç¿»è¯‘ã€åˆ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3379e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

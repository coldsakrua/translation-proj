{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865b4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLTKæ•°æ®é›†è·¯å¾„å·²æŒ‡å®šä¸ºï¼šC:\\Users\\UTU\\AppData\\Roaming\\nltk_data\n",
      "âœ… æ‰€æœ‰NLTKæ•°æ®é›†å‡å­˜åœ¨ï¼\n",
      "âœ… spaCyæ¨¡å‹en_core_web_småŠ è½½æˆåŠŸ\n",
      "æ­¥éª¤1/6ï¼šè§£æè‹±æ–‡ç‰ˆEPUB ./Artificial Intelligence_ A Modern Approach 4th Ed.epub...\n",
      "ğŸ“š æå–è‹±æ–‡ç‰ˆæ€»æ–‡æœ¬å­—ç¬¦æ•°ï¼š3398053ï¼ˆéœ€â‰¥20000ï¼‰\n",
      "âœ… æˆåŠŸåŒ¹é…ç« èŠ‚1ï¼šæ ‡é¢˜=Chapter 1: CHAPTER\n",
      "âœ… ç« èŠ‚1ï¼š264ä¸ªé€»è¾‘æ®µè½ï¼Œ105690å­—ç¬¦\n",
      "âœ… æˆåŠŸåŒ¹é…ç« èŠ‚2ï¼šæ ‡é¢˜=Chapter 2: CHAPTER\n",
      "âœ… ç« èŠ‚2ï¼š204ä¸ªé€»è¾‘æ®µè½ï¼Œ73487å­—ç¬¦\n",
      "âœ… æˆåŠŸåŒ¹é…ç« èŠ‚3ï¼šæ ‡é¢˜=Chapter 3: CHAPTER\n",
      "âœ… ç« èŠ‚3ï¼š417ä¸ªé€»è¾‘æ®µè½ï¼Œ129519å­—ç¬¦\n",
      "âœ… æˆåŠŸåŒ¹é…ç« èŠ‚4ï¼šæ ‡é¢˜=Chapter 4: CHAPTER\n",
      "âœ… ç« èŠ‚4ï¼š278ä¸ªé€»è¾‘æ®µè½ï¼Œ95372å­—ç¬¦\n",
      "âœ… æˆåŠŸåŒ¹é…ç« èŠ‚5ï¼šæ ‡é¢˜=Chapter 5: CHAPTER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UTU\\AppData\\Local\\Temp\\ipykernel_35124\\3646716385.py:165: DeprecationWarning: Flags not at the start of the expression '(?i)CHAPTER1\\\\n([^\\\\n]' (truncated) but at position 37\n",
      "  match = re.search(pattern, full_text)\n",
      "C:\\Users\\UTU\\AppData\\Local\\Temp\\ipykernel_35124\\3646716385.py:165: DeprecationWarning: Flags not at the start of the expression '(?i)CHAPTER2\\\\n([^\\\\n]' (truncated) but at position 37\n",
      "  match = re.search(pattern, full_text)\n",
      "C:\\Users\\UTU\\AppData\\Local\\Temp\\ipykernel_35124\\3646716385.py:165: DeprecationWarning: Flags not at the start of the expression '(?i)CHAPTER3\\\\n([^\\\\n]' (truncated) but at position 37\n",
      "  match = re.search(pattern, full_text)\n",
      "C:\\Users\\UTU\\AppData\\Local\\Temp\\ipykernel_35124\\3646716385.py:165: DeprecationWarning: Flags not at the start of the expression '(?i)CHAPTER4\\\\n([^\\\\n]' (truncated) but at position 37\n",
      "  match = re.search(pattern, full_text)\n",
      "C:\\Users\\UTU\\AppData\\Local\\Temp\\ipykernel_35124\\3646716385.py:165: DeprecationWarning: Flags not at the start of the expression '(?i)CHAPTER5\\\\n([^\\\\n]' (truncated) but at position 37\n",
      "  match = re.search(pattern, full_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç« èŠ‚5ï¼š231ä¸ªé€»è¾‘æ®µè½ï¼Œ81672å­—ç¬¦\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚1ï¼ˆ105690å­—ç¬¦ï¼‰...\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚2ï¼ˆ73487å­—ç¬¦ï¼‰...\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚3ï¼ˆ129519å­—ç¬¦ï¼‰...\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚4ï¼ˆ95372å­—ç¬¦ï¼‰...\n",
      "æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚5ï¼ˆ81672å­—ç¬¦ï¼‰...\n",
      "æ­¥éª¤3/6ï¼šä¿å­˜é¢„å¤„ç†æˆæœ...\n",
      "âœ… å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ï¼špreprocess_result.json\n",
      "âœ… æœ¯è¯­æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼šterminology_verification.csv\n",
      "âœ… å‘½åå®ä½“æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼šnamed_entity_verification.csv\n",
      "âœ… é£æ ¼åˆ†æè¡¨å·²ä¿å­˜è‡³ï¼šstyle_analysis.csv\n",
      "âœ… æ–‡åŒ–è´Ÿè½½è¯è¡¨å·²ä¿å­˜è‡³ï¼šcultural_word_verification.csv\n",
      "âœ… ç¿»è¯‘å•å…ƒè¡¨å·²ä¿å­˜è‡³ï¼štranslation_units.csv\n",
      "\n",
      "ğŸ‰ é¢„å¤„ç†å®Œæˆï¼å…¨å±€ç»Ÿè®¡ï¼š\n",
      "  - ç›®æ ‡ç« èŠ‚ï¼š[1, 2, 3, 4, 5]\n",
      "  - æ€»å­—ç¬¦æ•°ï¼š485740ï¼ˆâ‰¥20000ï¼‰\n",
      "  - ç¿»è¯‘å•å…ƒæ•°ï¼š1394\n",
      "  - æå–æœ¯è¯­æ•°ï¼š6049\n",
      "  - å‘½åå®ä½“æ•°ï¼š2336\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒä¿®æ”¹ï¼šä¿®æ­£NLTKæ•°æ®è·¯å¾„ --------------------------\n",
    "# æ–°è·¯å¾„ï¼ˆæ­£ç¡®ï¼‰ï¼šæ”¹ä¸ºå®é™…çš„AppDataè·¯å¾„\n",
    "NLTK_DATA_DIR = \"C:\\\\Users\\\\UTU\\\\AppData\\\\Roaming\\\\nltk_data\"\n",
    "\n",
    "# åç»­çš„è·¯å¾„é…ç½®åŒæ­¥ä½¿ç”¨è¿™ä¸ªæ­£ç¡®çš„æ ¹è·¯å¾„\n",
    "os.environ[\"NLTK_DATA\"] = NLTK_DATA_DIR\n",
    "nltk.data.path = [NLTK_DATA_DIR]\n",
    "print(f\"âœ… NLTKæ•°æ®é›†è·¯å¾„å·²æŒ‡å®šä¸ºï¼š{NLTK_DATA_DIR}\")\n",
    "\n",
    "# éªŒè¯æ•°æ®é›†çš„è·¯å¾„ä¹Ÿä¼šè‡ªåŠ¨åŸºäºæ­£ç¡®çš„æ ¹è·¯å¾„æ‹¼æ¥\n",
    "required_datasets = {\n",
    "    \"stopwords\": os.path.join(NLTK_DATA_DIR, \"corpora\", \"stopwords\", \"english\"),\n",
    "    \"wordnet\": os.path.join(NLTK_DATA_DIR, \"corpora\", \"wordnet\"),\n",
    "    \"punkt\": os.path.join(NLTK_DATA_DIR, \"tokenizers\", \"punkt\", \"PY3\", \"english.pickle\"),\n",
    "    \"averaged_perceptron_tagger\": os.path.join(NLTK_DATA_DIR, \"taggers\", \"averaged_perceptron_tagger\", \"averaged_perceptron_tagger.pickle\")\n",
    "}\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨\n",
    "for data_name, data_path in required_datasets.items():\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"âŒ æœªæ‰¾åˆ°{NLTK_DATA_DIR}ä¸‹çš„{data_name}æ•°æ®é›†ï¼\\n\"\n",
    "            f\"è¯·ç¡®è®¤{data_path}æ–‡ä»¶/æ–‡ä»¶å¤¹å­˜åœ¨\"\n",
    "        )\n",
    "print(\"âœ… æ‰€æœ‰NLTKæ•°æ®é›†å‡å­˜åœ¨ï¼\")\n",
    "\n",
    "# -------------------------- åˆå§‹åŒ–NLTKæ ¸å¿ƒä¾èµ–ï¼ˆç›´æ¥åŠ è½½æ‰‹åŠ¨æ•°æ®é›†ï¼‰ --------------------------\n",
    "# åœç”¨è¯ï¼ˆè‹±æ–‡ï¼‰\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "# è¯å½¢è¿˜åŸå™¨\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# éªŒè¯è¯æ€§æ ‡æ³¨å™¨ï¼ˆç¡®ä¿æ‰‹åŠ¨æ•°æ®é›†å¯ç”¨ï¼‰\n",
    "try:\n",
    "    nltk.pos_tag(word_tokenize(\"test sentence\"))\n",
    "except Exception as e:\n",
    "    raise Exception(f\"âŒ è¯æ€§æ ‡æ³¨æ•°æ®é›†åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ï¼š\\n{NLTK_DATA_DIR}\\\\taggers\\\\averaged_perceptron_tagger\\\\averaged_perceptron_tagger.pickle\")\n",
    "\n",
    "# -------------------------- åˆå§‹åŒ–spaCyï¼ˆç¡®ä¿nlpå¯è°ƒç”¨ï¼Œé€‚é…å‘½åå®ä½“æå–ï¼‰ --------------------------\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"âœ… spaCyæ¨¡å‹en_core_web_småŠ è½½æˆåŠŸ\")\n",
    "except OSError:\n",
    "    raise Exception(\"âŒ spaCyæ¨¡å‹æœªå®‰è£…ï¼è¯·å…ˆè¿è¡Œï¼špython -m spacy download en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"âŒ spaCyåˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}\")\n",
    "\n",
    "# -------------------------- å¯é…ç½®å‚æ•°ï¼ˆé€‚é…è¯¾ç¨‹è¦æ±‚ï¼Œæ— éœ€ä¿®æ”¹è·¯å¾„ï¼‰ --------------------------\n",
    "INPUT_EPUB = \"./Artificial Intelligence_ A Modern Approach 4th Ed.epub\"  # ä½ çš„è‹±æ–‡ç‰ˆEPUBè·¯å¾„\n",
    "TARGET_CHAPTERS = [1, 2, 3, 4, 5]  # ç›®æ ‡ç« èŠ‚ï¼ˆ1-5ç« ï¼‰\n",
    "MIN_TEXT_LENGTH = 20000  # æ–‡æœ¬è§„æ¨¡â‰¥2ä¸‡å­—\n",
    "# è¾“å‡ºæ–‡ä»¶ï¼ˆæ”¯æ’‘å¤šæ™ºèƒ½ä½“å·¥ä½œæµ+äººæœºåä½œï¼‰\n",
    "PREPROCESS_JSON = \"preprocess_result.json\"\n",
    "TERM_TABLE = \"terminology_verification.csv\"\n",
    "NER_TABLE = \"named_entity_verification.csv\"\n",
    "STYLE_METADATA = \"style_analysis.csv\"\n",
    "CULTURAL_WORD_TABLE = \"cultural_word_verification.csv\"\n",
    "TRANSLATION_UNITS = \"translation_units.csv\"\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -------------------------- è¾…åŠ©å‡½æ•°ï¼šæ•°å­—è½¬ç½—é©¬æ•°å­— --------------------------\n",
    "def roman_numeral(n):\n",
    "    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n",
    "    syb = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n",
    "    roman_num = ''\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        for _ in range(n // val[i]):\n",
    "            roman_num += syb[i]\n",
    "            n -= val[i]\n",
    "        i += 1\n",
    "    return roman_num\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½1ï¼šEPUBç« èŠ‚æå–--------------------------\n",
    "def extract_epub_chapters(epub_path, target_chapters, MIN_TEXT_LENGTH=20000):\n",
    "    # 1. è¯»å–EPUBæ–‡ä»¶\n",
    "    book = epub.read_epub(epub_path)\n",
    "    all_items = []  # å­˜å‚¨EPUBçš„æ¯ä¸ªæ–‡æœ¬é¡¹ï¼ˆæ ‡é¢˜+å†…å®¹ï¼‰\n",
    "\n",
    "    # 2. éå†EPUBçš„æ‰€æœ‰é¡¹ï¼Œæå–æ–‡æœ¬å†…å®¹ï¼ˆå…³é”®ï¼šå¡«å……all_itemsï¼‰\n",
    "    for item in book.get_items():\n",
    "        # åªå¤„ç†HTML/XHTMLæ–‡æœ¬é¡¹\n",
    "        if item.media_type in [\"application/xhtml+xml\", \"text/html\"]:\n",
    "            try:\n",
    "                content = item.get_content().decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                # å…¼å®¹ç‰¹æ®Šç¼–ç \n",
    "                content = item.get_content().decode('latin-1', errors='replace')\n",
    "            \n",
    "            # è§£æHTMLï¼Œæå–æ ‡é¢˜å’Œæ­£æ–‡\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            # æå–ç« èŠ‚æ ‡é¢˜ï¼ˆh1/h2/h3æ ‡ç­¾ï¼‰\n",
    "            title_tag = soup.find(['h1', 'h2', 'h3'])\n",
    "            section_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "            \n",
    "            # æ¸…ç†æ­£æ–‡ï¼ˆç§»é™¤æ— å…³æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬ç»“æ„ï¼‰\n",
    "            for elem in soup([\"script\", \"style\", \"img\", \"svg\", \"nav\", \"footer\", \"header\", \"table\"]):\n",
    "                elem.decompose()\n",
    "            text = soup.get_text(separator='\\n', strip=True)\n",
    "            # åªåˆå¹¶4ä¸ªä»¥ä¸Šçš„æ¢è¡Œï¼Œä¿ç•™ç« èŠ‚æ ‡é¢˜çš„æ¢è¡Œï¼ˆé€‚é…ä½ çš„æ ¼å¼ï¼‰\n",
    "            text = re.sub(r'\\n{4,}', '\\n\\n\\n', text)\n",
    "            \n",
    "            # å°†é¡¹æ·»åŠ åˆ°all_items\n",
    "            all_items.append({\n",
    "                \"title\": section_title,\n",
    "                \"content\": text,\n",
    "                \"id\": item.get_id(),\n",
    "                \"order\": len(all_items)\n",
    "            })\n",
    "\n",
    "    # 3. æ‹¼æ¥å®Œæ•´æ–‡æœ¬ï¼ˆç°åœ¨all_itemsæœ‰å†…å®¹äº†ï¼‰\n",
    "    full_text = \"\\n\".join([f\"{item['title']}\\n{item['content']}\" for item in all_items])\n",
    "    # å†æ¬¡æ¸…ç†æ¢è¡Œï¼Œç¡®ä¿ç« èŠ‚æ ‡é¢˜ç‹¬ç«‹æˆè¡Œ\n",
    "    full_text = re.sub(r'\\n{4,}', '\\n\\n\\n', full_text)\n",
    "\n",
    "    # 4. æ ¡éªŒæ€»æ–‡æœ¬é•¿åº¦ï¼ˆé¿å…æ–‡æœ¬è¿‡çŸ­ï¼‰\n",
    "    total_len = len(full_text.replace(' ', ''))\n",
    "    print(f\"ğŸ“š æå–è‹±æ–‡ç‰ˆæ€»æ–‡æœ¬å­—ç¬¦æ•°ï¼š{total_len}ï¼ˆéœ€â‰¥{MIN_TEXT_LENGTH}ï¼‰\")\n",
    "    if total_len < MIN_TEXT_LENGTH:\n",
    "        raise ValueError(f\"æ–‡æœ¬è§„æ¨¡ä¸è¾¾æ ‡ï¼å½“å‰{total_len}å­—ç¬¦ï¼Œéœ€â‰¥{MIN_TEXT_LENGTH}å­—ç¬¦\")\n",
    "\n",
    "    # 5. åˆå§‹åŒ–chapters_dataï¼ˆå…³é”®ï¼šè§£å†³æœªå®šä¹‰é—®é¢˜ï¼‰\n",
    "    chapters_data = defaultdict(dict)  # ç”¨defaultdictæ›´æ–¹ä¾¿ï¼Œä¹Ÿå¯ä»¥ç”¨æ™®é€šå­—å…¸ï¼šchapters_data = {}\n",
    "\n",
    "    # 6. æå–ç›®æ ‡ç« èŠ‚ï¼ˆé€‚é…ä½ çš„CHAPTER1æ ¼å¼ï¼‰\n",
    "    for chap_num in target_chapters:\n",
    "        chap_content = \"\"\n",
    "        chap_title = f\"Chapter {chap_num}\"\n",
    "        \n",
    "        # -------------------------- é€‚é…ä½ çš„æ ¼å¼ï¼šåŒ¹é…â€œCHAPTER1â€â€œCHAPTER2â€ --------------------------\n",
    "        patterns = [\n",
    "            rf'(?i)CHAPTER{chap_num}\\n([^\\n]*)\\n([\\s\\S]*?)(?=(?i)CHAPTER{chap_num+1}|$)'\n",
    "            # è§£é‡Šï¼š\n",
    "            # (?i)ï¼šå¿½ç•¥å¤§å°å†™ï¼ˆé€‚é…chapter1/CHAPTER1ï¼‰\n",
    "            # CHAPTER{chap_num}ï¼šåŒ¹é…CHAPTER1/CHAPTER2ï¼ˆæ— ç©ºæ ¼ï¼‰\n",
    "            # \\n([^\\n]*)\\nï¼šæ•è·å‰¯æ ‡é¢˜ï¼ˆå¦‚INTRODUCTIONï¼‰\n",
    "            # ([\\s\\S]*?)ï¼šéè´ªå©ªåŒ¹é…ç« èŠ‚å†…å®¹\n",
    "            # (?=CHAPTER{chap_num+1}|$)ï¼šç›´åˆ°ä¸‹ä¸€ç« æˆ–æ–‡æœ¬ç»“æŸ\n",
    "        ]\n",
    "        \n",
    "        # å°è¯•æ­£åˆ™åŒ¹é…\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, full_text)\n",
    "            if match:\n",
    "                # æå–å‰¯æ ‡é¢˜\n",
    "                sub_title = match.group(1).strip()\n",
    "                chap_title = f\"Chapter {chap_num}: {sub_title}\"\n",
    "                # æå–ç« èŠ‚å†…å®¹\n",
    "                chap_content = match.group(2).strip()\n",
    "                print(f\"âœ… æˆåŠŸåŒ¹é…ç« èŠ‚{chap_num}ï¼šæ ‡é¢˜={chap_title}\")\n",
    "                break\n",
    "        \n",
    "        # -------------------------- å…œåº•åˆ†å‰²ï¼šå¼ºåˆ¶ç« èŠ‚1/2ä¸é‡å  --------------------------\n",
    "        chap_content_clean = chap_content.replace(' ', '')\n",
    "        if len(chap_content_clean) < 10000:\n",
    "            print(f\"âš ï¸  ç« èŠ‚{chap_num}åŒ¹é…å¤±è´¥ï¼Œå¼ºåˆ¶æŒ‰ç« èŠ‚æ ‡é¢˜ä½ç½®åˆ†å‰²\")\n",
    "            # æ‰¾åˆ°CHAPTER1/CHAPTER2çš„ä½ç½®\n",
    "            chap1_pos = full_text.find(\"CHAPTER1\")\n",
    "            chap2_pos = full_text.find(\"CHAPTER2\")\n",
    "            if chap_num == 1:\n",
    "                # ç« èŠ‚1ï¼šä»CHAPTER1åˆ°CHAPTER2\n",
    "                chap_content = full_text[chap1_pos:chap2_pos].strip()\n",
    "            else:\n",
    "                # ç« èŠ‚2ï¼šä»CHAPTER2åˆ°æ–‡æœ¬ç»“æŸ\n",
    "                chap_content = full_text[chap2_pos:].strip()\n",
    "        \n",
    "        # -------------------------- åˆ†å‰²é€»è¾‘æ®µè½ï¼ˆéœ€ç¡®ä¿split_into_logical_paragraphså‡½æ•°å­˜åœ¨ï¼‰ --------------------------\n",
    "        paragraphs = split_into_logical_paragraphs(chap_content, chap_num)\n",
    "        \n",
    "        # -------------------------- å¡«å……ç« èŠ‚æ•°æ® --------------------------\n",
    "        chapters_data[chap_num] = {\n",
    "            \"title\": chap_title,\n",
    "            \"raw_content\": chap_content,\n",
    "            \"paragraphs\": paragraphs,\n",
    "            \"para_count\": len(paragraphs),\n",
    "            \"char_count\": len(chap_content.replace(' ', ''))\n",
    "        }\n",
    "        print(f\"âœ… ç« èŠ‚{chap_num}ï¼š{chapters_data[chap_num]['para_count']}ä¸ªé€»è¾‘æ®µè½ï¼Œ{chapters_data[chap_num]['char_count']}å­—ç¬¦\")\n",
    "\n",
    "    # 7. è¿”å›ç« èŠ‚æ•°æ®\n",
    "    return chapters_data\n",
    "\n",
    "# éœ€ç¡®ä¿split_into_logical_paragraphså‡½æ•°å­˜åœ¨ï¼ˆä¹‹å‰çš„ä»£ç é‡Œçš„å‡½æ•°ï¼‰\n",
    "def split_into_logical_paragraphs(text, chap_num):\n",
    "    raw_paragraphs = re.split(r'(?<=[.!?])\\s*\\n+', text)\n",
    "    logical_paras = []\n",
    "    for idx, para in enumerate(raw_paragraphs):\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        prev_para_id = f\"chap{chap_num}_para{idx}\" if idx > 0 else \"None\"\n",
    "        next_para_id = f\"chap{chap_num}_para{idx+2}\" if idx < len(raw_paragraphs)-2 else \"None\"\n",
    "        para_id = f\"chap{chap_num}_para{idx+1}\"\n",
    "        sentences = nltk.sent_tokenize(para)\n",
    "        logical_paras.append({\n",
    "            \"para_id\": para_id,\n",
    "            \"content\": para,\n",
    "            \"sentence_count\": len(sentences),\n",
    "            \"prev_para_id\": prev_para_id,\n",
    "            \"next_para_id\": next_para_id,\n",
    "            \"char_count\": len(para.replace(' ', ''))\n",
    "        })\n",
    "    return logical_paras\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½2ï¼šè¯­åŸŸä¸é£æ ¼è‡ªåŠ¨è¯†åˆ« --------------------------\n",
    "def analyze_style_and_domain(para_content, para_id):\n",
    "    ai_domain_keywords = {\n",
    "        \"machine learning\", \"neural network\", \"reinforcement learning\", \"bayesian network\",\n",
    "        \"convolutional neural network\", \"attention mechanism\", \"backpropagation\",\n",
    "        \"markov decision process\", \"perceptron\", \"rational agent\"\n",
    "    }\n",
    "    formal_style_keywords = {\"furthermore\", \"nevertheless\", \"consequently\", \"therefore\", \"moreover\", \"however\"}\n",
    "    \n",
    "    # åŸºç¡€ç»Ÿè®¡ï¼ˆä½¿ç”¨æ‰‹åŠ¨åŠ è½½çš„tokenize/sent_tokenizeï¼‰\n",
    "    sentences = sent_tokenize(para_content)\n",
    "    sent_count = len(sentences)\n",
    "    words = word_tokenize(para_content.lower())\n",
    "    word_count = len(words)\n",
    "    avg_sent_length = round(word_count / sent_count, 2) if sent_count > 0 else 0\n",
    "    \n",
    "    # è¢«åŠ¨è¯­æ€å æ¯”\n",
    "    passive_count = len(re.findall(r'\\bis\\s+\\w+ed\\b|\\bwas\\s+\\w+ed\\b|\\bwere\\s+\\w+ed\\b', para_content.lower()))\n",
    "    passive_ratio = round(passive_count / word_count, 3) if word_count > 0 else 0\n",
    "    \n",
    "    # é¢†åŸŸä¸é£æ ¼åˆ¤æ–­\n",
    "    text_word_set = set(words)\n",
    "    domain_match = text_word_set & ai_domain_keywords\n",
    "    domain = \"AI_Professional\" if len(domain_match) >= 3 else \"AI_Popular\"\n",
    "    \n",
    "    formal_match = text_word_set & formal_style_keywords\n",
    "    formality_score = 7.0 + len(formal_match)*0.5 + (1 if passive_ratio >= 0.05 else 0) + (1 if avg_sent_length >= 25 else 0)\n",
    "    formality_score = min(formality_score, 10.0)\n",
    "    style = \"Formal_Academic\" if formality_score >= 8.0 else \"Semi_Formal_Academic\"\n",
    "    \n",
    "    return {\n",
    "        \"para_id\": para_id,\n",
    "        \"domain\": domain,\n",
    "        \"style\": style,\n",
    "        \"formality_score\": round(formality_score, 1),\n",
    "        \"avg_sentence_length\": avg_sent_length,\n",
    "        \"passive_voice_ratio\": passive_ratio,\n",
    "        \"domain_keywords\": list(domain_match),\n",
    "        \"formal_keywords\": list(formal_match),\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sent_count\n",
    "    }\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½3ï¼šé¢†åŸŸæœ¯è¯­æå–ï¼ˆç›´æ¥ä½¿ç”¨æ‰‹åŠ¨è¯æ€§æ ‡æ³¨æ•°æ®é›†ï¼‰ --------------------------\n",
    "def extract_terminology(para_content):\n",
    "    # åˆ†è¯ä¸è¿‡æ»¤ï¼ˆä½¿ç”¨æ‰‹åŠ¨åŠ è½½çš„stopwordsï¼‰\n",
    "    tokens = word_tokenize(para_content.lower())\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens \n",
    "        if token not in STOP_WORDS and not token.isdigit() and len(token) >= 3\n",
    "    ]\n",
    "    \n",
    "    # è¯æ€§æ ‡æ³¨ï¼ˆä½¿ç”¨æ‰‹åŠ¨åŠ è½½çš„averaged_perceptron_taggerï¼‰\n",
    "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "    valid_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\n",
    "    valid_tokens = [lemmatizer.lemmatize(token) for token, pos in pos_tags if pos in valid_pos_tags]\n",
    "    \n",
    "    # è¡¥å……AIæ ¸å¿ƒæœ¯è¯­\n",
    "    ai_core_terms = {\n",
    "        \"rational agent\", \"bayesian network\", \"machine learning\", \"neural network\",\n",
    "        \"reinforcement learning\", \"markov decision process\", \"perceptron\",\n",
    "        \"attention mechanism\", \"backpropagation\", \"convolutional neural network\"\n",
    "    }\n",
    "    for term in ai_core_terms:\n",
    "        valid_tokens.append(term)\n",
    "    \n",
    "    return Counter(valid_tokens)\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½4ï¼šå‘½åå®ä½“æå–ï¼ˆä½¿ç”¨æ‰‹åŠ¨åŠ è½½çš„spaCyï¼‰ --------------------------\n",
    "def extract_named_entities(para_content, para_id):\n",
    "    doc = nlp(para_content)\n",
    "    named_entities = []\n",
    "    target_entity_types = [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\", \"PRODUCT\", \"EVENT\"]\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in target_entity_types:\n",
    "            context_start = max(0, ent.start_char - 50)\n",
    "            context_end = min(len(para_content), ent.end_char + 50)\n",
    "            entity_context = para_content[context_start:context_end]\n",
    "            \n",
    "            named_entities.append({\n",
    "                \"para_id\": para_id,\n",
    "                \"entity_text\": ent.text,\n",
    "                \"entity_type\": ent.label_,\n",
    "                \"start_char\": ent.start_char,\n",
    "                \"end_char\": ent.end_char,\n",
    "                \"context\": entity_context\n",
    "            })\n",
    "    return named_entities\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½5ï¼šæ–‡åŒ–è´Ÿè½½è¯æå– --------------------------\n",
    "def extract_cultural_words(para_content, para_id):\n",
    "    cultural_word_lib = {\n",
    "        \"state-of-the-art\": {\"semantic_explanation\": \"AIé¢†åŸŸæŒ‡å½“å‰æœ€é«˜æŠ€æœ¯æ°´å¹³\", \"translation_strategy\": \"æ„è¯‘â€˜å‰æ²¿æŠ€æœ¯â€™\"},\n",
    "        \"holy grail\": {\"semantic_explanation\": \"æ¯”å–»é¢†åŸŸç»ˆæç›®æ ‡\", \"translation_strategy\": \"æ„è¯‘â€˜åœ£æ¯ï¼ˆç»ˆæç›®æ ‡ï¼‰â€™\"},\n",
    "        \"trial and error\": {\"semantic_explanation\": \"AIè®­ç»ƒä¸­çš„è¯•é”™æ–¹æ³•\", \"translation_strategy\": \"å›ºå®šè¯‘æ³•â€˜è¯•é”™æ³•â€™\"},\n",
    "        \"game-changer\": {\"semantic_explanation\": \"é¢ è¦†æ€§æŠ€æœ¯/æ–¹æ³•\", \"translation_strategy\": \"æ„è¯‘â€˜å˜é©æ€§æŠ€æœ¯â€™\"}\n",
    "    }\n",
    "    \n",
    "    extracted = []\n",
    "    for word, info in cultural_word_lib.items():\n",
    "        if re.search(r'\\b' + re.escape(word) + r'\\b', para_content, re.IGNORECASE):\n",
    "            match = re.search(r'\\b' + re.escape(word) + r'\\b', para_content, re.IGNORECASE)\n",
    "            context_start = max(0, match.start() - 50)\n",
    "            context_end = min(len(para_content), match.end() + 50)\n",
    "            \n",
    "            extracted.append({\n",
    "                \"para_id\": para_id,\n",
    "                \"cultural_word\": word,\n",
    "                \"semantic_explanation\": info[\"semantic_explanation\"],\n",
    "                \"translation_strategy\": info[\"translation_strategy\"],\n",
    "                \"context\": para_content[context_start:context_end]\n",
    "            })\n",
    "    return extracted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------- æ–°å¢ï¼šé€’å½’è½¬æ¢æ‰€æœ‰setä¸ºlistï¼ˆå½»åº•è§£å†³JSONåºåˆ—åŒ–ï¼‰ --------------------------\n",
    "# é€’å½’è½¬æ¢setä¸ºlistçš„å‡½æ•°\n",
    "def convert_set_to_list(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_set_to_list(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "        return [convert_set_to_list(item) for item in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "# -------------------------- æ ¸å¿ƒåŠŸèƒ½6ï¼šä¿å­˜é¢„å¤„ç†æˆæœï¼ˆç”¨äºåç»­äººæœºåä½œï¼‰ --------------------------\n",
    "def save_preprocess_results(final_data):\n",
    "    data_to_save = copy.deepcopy(final_data)\n",
    "    data_to_save = convert_set_to_list(data_to_save)  # é€’å½’è½¬æ¢æ‰€æœ‰set\n",
    "    \n",
    "    # -------------------------- åŸä¿å­˜é€»è¾‘ä¸å˜ï¼ˆä½¿ç”¨è½¬æ¢åçš„æ•°æ®ï¼‰ --------------------------\n",
    "    # 1. å®Œæ•´ç»“æ„åŒ–æ•°æ®ï¼ˆä¾›å¤šæ™ºèƒ½ä½“è°ƒç”¨ï¼‰\n",
    "    with open(PREPROCESS_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_to_save, f, ensure_ascii=False, indent=2)  # ç”¨è½¬æ¢åçš„æ•°æ®\n",
    "    print(f\"âœ… å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ï¼š{PREPROCESS_JSON}\")\n",
    "\n",
    "    # 2. æœ¯è¯­æ ¡éªŒè¡¨ï¼ˆäººæœºåä½œï¼‰\n",
    "    with open(TERM_TABLE, 'w', encoding='utf-8', newline='') as f:\n",
    "        fieldnames = [\"æœ¯è¯­\", \"å‡ºç°æ¬¡æ•°\", \"é¢†åŸŸåˆ†ç±»\", \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        ai_core_terms = final_data[\"ai_core_terms\"]  # åŸæ•°æ®ä¸å½±å“CSVï¼Œæ— éœ€è½¬æ¢\n",
    "        for term, count in sorted(final_data[\"global_terminology\"].items(), key=lambda x: -x[1]):\n",
    "            domain = \"AIä¸“ä¸šæœ¯è¯­\" if term in ai_core_terms else \"é€šç”¨æœ¯è¯­\"\n",
    "            writer.writerow({\n",
    "                \"æœ¯è¯­\": term, \"å‡ºç°æ¬¡æ•°\": count, \"é¢†åŸŸåˆ†ç±»\": domain,\n",
    "                \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\": \"\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\": \"\"\n",
    "            })\n",
    "    print(f\"âœ… æœ¯è¯­æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼š{TERM_TABLE}\")\n",
    "\n",
    "    # 3. å‘½åå®ä½“æ ¡éªŒè¡¨ï¼ˆäººæœºåä½œï¼‰\n",
    "    all_entities = []\n",
    "    entity_counter = Counter()\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            for ent in para[\"named_entities\"]:\n",
    "                all_entities.append(ent)\n",
    "                entity_counter[(ent[\"entity_text\"], ent[\"entity_type\"])] += 1\n",
    "    # å»é‡å¹¶æ·»åŠ å‡ºç°æ¬¡æ•°\n",
    "    unique_entities = []\n",
    "    seen = set()\n",
    "    for ent in all_entities:\n",
    "        key = (ent[\"entity_text\"], ent[\"entity_type\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            ent[\"å‡ºç°æ¬¡æ•°\"] = entity_counter[key]\n",
    "            unique_entities.append(ent)\n",
    "    with open(NER_TABLE, 'w', encoding='utf-8', newline='') as f:\n",
    "        fieldnames = [\"å®ä½“åŸæ–‡\", \"å®ä½“ç±»å‹\", \"å‡ºç°æ¬¡æ•°\", \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\", \"ä¸Šä¸‹æ–‡\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for ent in unique_entities:\n",
    "            writer.writerow({\n",
    "                \"å®ä½“åŸæ–‡\": ent[\"entity_text\"], \"å®ä½“ç±»å‹\": ent[\"entity_type\"], \"å‡ºç°æ¬¡æ•°\": ent[\"å‡ºç°æ¬¡æ•°\"],\n",
    "                \"å»ºè®®è¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\": \"\", \"æ ¡éªŒç»“æœï¼ˆé€šè¿‡/ä¿®æ”¹ï¼‰\": \"\", \"ä¸Šä¸‹æ–‡\": ent[\"context\"]\n",
    "            })\n",
    "    print(f\"âœ… å‘½åå®ä½“æ ¡éªŒè¡¨å·²ä¿å­˜è‡³ï¼š{NER_TABLE}\")\n",
    "\n",
    "    # 4. é£æ ¼åˆ†æè¡¨ï¼ˆæŒ‡å¯¼ç¿»è¯‘Agentï¼‰\n",
    "    style_data = []\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            style_meta = para[\"style_metadata\"]\n",
    "            style_data.append({\n",
    "                \"æ®µè½ID\": style_meta[\"para_id\"], \"ç« èŠ‚å·\": chap[\"chapter_num\"],\n",
    "                \"é¢†åŸŸ\": style_meta[\"domain\"], \"è¯­ä½“é£æ ¼\": style_meta[\"style\"],\n",
    "                \"æ­£å¼åº¦è¯„åˆ†\": style_meta[\"formality_score\"], \"å¹³å‡å¥é•¿\": style_meta[\"avg_sentence_length\"],\n",
    "                \"è¢«åŠ¨è¯­æ€å æ¯”\": style_meta[\"passive_voice_ratio\"], \"é¢†åŸŸå…³é”®è¯\": \",\".join(style_meta[\"domain_keywords\"])\n",
    "            })\n",
    "    with open(STYLE_METADATA, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=style_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(style_data)\n",
    "    print(f\"âœ… é£æ ¼åˆ†æè¡¨å·²ä¿å­˜è‡³ï¼š{STYLE_METADATA}\")\n",
    "\n",
    "    # 5. æ–‡åŒ–è´Ÿè½½è¯è¡¨ï¼ˆäººæœºåä½œï¼‰\n",
    "    all_cultural = []\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            all_cultural.extend(para[\"cultural_words\"])\n",
    "    # åŸä»£ç ä¸­ä¿å­˜æ–‡åŒ–è´Ÿè½½è¯è¡¨çš„éƒ¨åˆ†\n",
    "    with open(CULTURAL_WORD_TABLE, 'w', encoding='utf-8', newline='') as f:\n",
    "        fieldnames = [\"æ–‡åŒ–è´Ÿè½½è¯\", \"è¯­ä¹‰è§£é‡Š\", \"ç¿»è¯‘ç­–ç•¥\", \"ä¸Šä¸‹æ–‡\", \"æœ€ç»ˆè¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\", \"æ ¡éªŒç»“æœ\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        # åŸé”™è¯¯ä»£ç ï¼šwriter.writerows(all_cultural)\n",
    "        # æ–°ä»£ç ï¼šæ‰‹åŠ¨æ˜ å°„è‹±æ–‡é”®åˆ°ä¸­æ–‡åˆ—å\n",
    "        for item in all_cultural:\n",
    "            # ç”Ÿæˆç¬¦åˆfieldnamesçš„æ–°å­—å…¸\n",
    "            csv_row = {\n",
    "                \"æ–‡åŒ–è´Ÿè½½è¯\": item[\"cultural_word\"],\n",
    "                \"è¯­ä¹‰è§£é‡Š\": item[\"semantic_explanation\"],\n",
    "                \"ç¿»è¯‘ç­–ç•¥\": item[\"translation_strategy\"],\n",
    "                \"ä¸Šä¸‹æ–‡\": item[\"context\"],\n",
    "                \"æœ€ç»ˆè¯‘æ³•ï¼ˆäººå·¥å¡«å†™ï¼‰\": \"\",  # ç©ºå€¼ï¼Œä¾›äººå·¥å¡«å†™\n",
    "                \"æ ¡éªŒç»“æœ\": \"\"  # ç©ºå€¼ï¼Œä¾›äººå·¥å¡«å†™\n",
    "            }\n",
    "            writer.writerow(csv_row)  # å†™å…¥å•ä¸ªè¡Œ\n",
    "    print(f\"âœ… æ–‡åŒ–è´Ÿè½½è¯è¡¨å·²ä¿å­˜è‡³ï¼š{CULTURAL_WORD_TABLE}\")\n",
    "\n",
    "    # 6. ç¿»è¯‘å•å…ƒè¡¨ï¼ˆä¾›ç¿»è¯‘Agentæ‰§è¡Œï¼‰\n",
    "    translation_units = []\n",
    "    for chap in final_data[\"chapters\"]:\n",
    "        for para in chap[\"paragraphs\"]:\n",
    "            para_terms = [t for t, _ in extract_terminology(para[\"content\"]).most_common(3)]\n",
    "            para_ents = [ent[\"entity_text\"] for ent in para[\"named_entities\"]]\n",
    "            translation_units.append({\n",
    "                \"å•å…ƒID\": para[\"para_id\"], \"ç« èŠ‚å·\": chap[\"chapter_num\"],\n",
    "                \"åŸæ–‡\": para[\"content\"], \"ä¸Šä¸‹æ–‡å…³è”\": f\"å‰ï¼š{para['prev_para_id']} | åï¼š{para['next_para_id']}\",\n",
    "                \"é¢†åŸŸ\": para[\"style_metadata\"][\"domain\"], \"è¯­ä½“è¦æ±‚\": para[\"style_metadata\"][\"style\"],\n",
    "                \"å…³é”®æœ¯è¯­\": \",\".join(para_terms), \"å‘½åå®ä½“\": \",\".join(para_ents) if para_ents else \"æ— \",\n",
    "                \"Agentè¯‘æ–‡\": \"\", \"äººå·¥ä¿®æ­£\": \"\"\n",
    "            })\n",
    "    with open(TRANSLATION_UNITS, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=translation_units[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(translation_units)\n",
    "    print(f\"âœ… ç¿»è¯‘å•å…ƒè¡¨å·²ä¿å­˜è‡³ï¼š{TRANSLATION_UNITS}\")\n",
    "\n",
    "# -------------------------- ä¸»å‡½æ•°ï¼ˆæ•´åˆæ‰€æœ‰é¢„å¤„ç†æµç¨‹ï¼‰ --------------------------\n",
    "def main():\n",
    "    print(f\"æ­¥éª¤1/6ï¼šè§£æè‹±æ–‡ç‰ˆEPUB {INPUT_EPUB}...\")\n",
    "    # æå–ç« èŠ‚ï¼ˆç›´æ¥ä½¿ç”¨æ‰‹åŠ¨NLTKï¼‰\n",
    "    chapters_data = extract_epub_chapters(INPUT_EPUB, TARGET_CHAPTERS)\n",
    "    \n",
    "    # åˆå§‹åŒ–å…¨å±€æ•°æ®ç»“æ„\n",
    "    final_data = {\n",
    "        \"book_info\": \"Artificial Intelligence: A Modern Approach (4th Ed, English)\",\n",
    "        \"target_chapters\": TARGET_CHAPTERS,\n",
    "        \"global_metadata\": {\n",
    "            \"total_characters\": 0, \"total_paragraphs\": 0, \"total_terms\": 0,\n",
    "            \"total_entities\": 0, \"total_cultural_words\": 0,\n",
    "            \"dominant_domain\": \"AI_Professional\", \"dominant_style\": \"Formal_Academic\"\n",
    "        },\n",
    "        \"chapters\": [],\n",
    "        \"global_terminology\": {},\n",
    "        \"ai_core_terms\": {\"rational agent\", \"bayesian network\", \"machine learning\", \"neural network\",\n",
    "                          \"reinforcement learning\", \"markov decision process\", \"perceptron\",\n",
    "                          \"attention mechanism\", \"backpropagation\", \"convolutional neural network\"}\n",
    "    }\n",
    "    \n",
    "    # å¤„ç†æ¯ä¸ªç« èŠ‚\n",
    "    global_terms = Counter()\n",
    "    global_entity_count = 0\n",
    "    global_cultural_count = 0\n",
    "    \n",
    "    for chap_num, chap_data in chapters_data.items():\n",
    "        print(f\"æ­¥éª¤2/6ï¼šå¤„ç†ç« èŠ‚{chap_num}ï¼ˆ{chap_data['char_count']}å­—ç¬¦ï¼‰...\")\n",
    "        processed_paras = []\n",
    "        chap_content = chap_data[\"raw_content\"]\n",
    "        \n",
    "        for para in chap_data[\"paragraphs\"]:\n",
    "            para_id = para[\"para_id\"]\n",
    "            para_text = para[\"content\"]\n",
    "            \n",
    "            # æå–å¤šç»´åº¦ä¿¡æ¯ï¼ˆç›´æ¥ä½¿ç”¨æ‰‹åŠ¨åŠ è½½çš„ä¾èµ–ï¼‰\n",
    "            style_meta = analyze_style_and_domain(para_text, para_id)\n",
    "            para_terms = extract_terminology(para_text)\n",
    "            para_ents = extract_named_entities(para_text, para_id)\n",
    "            para_cultural = extract_cultural_words(para_text, para_id)\n",
    "            \n",
    "            # æ±‡æ€»å…¨å±€æ•°æ®\n",
    "            global_terms.update(para_terms)\n",
    "            global_entity_count += len(para_ents)\n",
    "            global_cultural_count += len(para_cultural)\n",
    "            \n",
    "            processed_paras.append({\n",
    "                \"para_id\": para_id, \"content\": para_text,\n",
    "                \"sentence_count\": para[\"sentence_count\"], \"char_count\": para[\"char_count\"],\n",
    "                \"prev_para_id\": para[\"prev_para_id\"], \"next_para_id\": para[\"next_para_id\"],\n",
    "                \"style_metadata\": style_meta, \"terminology\": dict(para_terms),\n",
    "                \"named_entities\": para_ents, \"cultural_words\": para_cultural\n",
    "            })\n",
    "        \n",
    "        # ä¿å­˜ç« èŠ‚æ•°æ®\n",
    "        final_data[\"chapters\"].append({\n",
    "            \"chapter_num\": chap_num, \"title\": chap_data[\"title\"],\n",
    "            \"paragraphs\": processed_paras, \"para_count\": len(processed_paras),\n",
    "            \"char_count\": chap_data[\"char_count\"]\n",
    "        })\n",
    "        final_data[\"global_metadata\"][\"total_characters\"] += chap_data[\"char_count\"]\n",
    "        final_data[\"global_metadata\"][\"total_paragraphs\"] += len(processed_paras)\n",
    "    \n",
    "    # å¡«å……å…¨å±€å…ƒæ•°æ®\n",
    "    final_data[\"global_terminology\"] = dict(global_terms)\n",
    "    final_data[\"global_metadata\"][\"total_terms\"] = len(global_terms)\n",
    "    final_data[\"global_metadata\"][\"total_entities\"] = global_entity_count\n",
    "    final_data[\"global_metadata\"][\"total_cultural_words\"] = global_cultural_count\n",
    "    \n",
    "    # ä¿å­˜æˆæœ\n",
    "    print(f\"æ­¥éª¤3/6ï¼šä¿å­˜é¢„å¤„ç†æˆæœ...\")\n",
    "    save_preprocess_results(final_data)\n",
    "    \n",
    "    # è¾“å‡ºç»Ÿè®¡\n",
    "    print(f\"\\nğŸ‰ é¢„å¤„ç†å®Œæˆï¼å…¨å±€ç»Ÿè®¡ï¼š\")\n",
    "    print(f\"  - ç›®æ ‡ç« èŠ‚ï¼š{TARGET_CHAPTERS}\")\n",
    "    print(f\"  - æ€»å­—ç¬¦æ•°ï¼š{final_data['global_metadata']['total_characters']}ï¼ˆâ‰¥{MIN_TEXT_LENGTH}ï¼‰\")\n",
    "    print(f\"  - ç¿»è¯‘å•å…ƒæ•°ï¼š{final_data['global_metadata']['total_paragraphs']}\")\n",
    "    print(f\"  - æå–æœ¯è¯­æ•°ï¼š{final_data['global_metadata']['total_terms']}\")\n",
    "    print(f\"  - å‘½åå®ä½“æ•°ï¼š{final_data['global_metadata']['total_entities']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770fccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:æˆåŠŸè¯»å–æ–‡ä»¶ï¼šD:\\Code\\PythonProjects\\RagTranslationWorkflow\\AI for Science.mdï¼ˆå¤§å°ï¼š72861å­—èŠ‚ï¼‰\n",
      "INFO:__main__:è§£æåˆ°è¡¨å¤´ï¼š['ç´¢å¼•ç¼–å·', 'è‹±æ–‡æœ¯è¯­', 'ä¸­æ–‡ç¿»è¯‘', 'å¸¸ç”¨ç¼©å†™', 'æ¥æº&æ‰©å±•', 'å¤‡æ³¨']ï¼ˆå…±6åˆ—ï¼‰\n",
      "INFO:__main__:è§£æåˆ°491è¡Œæœ‰æ•ˆè¡¨æ ¼æ•°æ®\n",
      "INFO:__main__:æå–åˆ°491ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹\n",
      "INFO:__main__:ä» D:\\Code\\PythonProjects\\RagTranslationWorkflow\\AI for Science.md æå–åˆ° 491 ä¸ªç¿»è¯‘å¯¹\n",
      "INFO:__main__:æˆåŠŸè¯»å–æ–‡ä»¶ï¼šD:\\Code\\PythonProjects\\RagTranslationWorkflow\\Machine Learning.mdï¼ˆå¤§å°ï¼š48972å­—èŠ‚ï¼‰\n",
      "INFO:__main__:è§£æåˆ°è¡¨å¤´ï¼š['ç´¢å¼•ç¼–å·', 'è‹±æ–‡æœ¯è¯­', 'ä¸­æ–‡ç¿»è¯‘', 'å¸¸ç”¨ç¼©å†™', 'æ¥æº&æ‰©å±•', 'å¤‡æ³¨']ï¼ˆå…±6åˆ—ï¼‰\n",
      "INFO:__main__:è§£æåˆ°726è¡Œæœ‰æ•ˆè¡¨æ ¼æ•°æ®\n",
      "INFO:__main__:æå–åˆ°726ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹\n",
      "INFO:__main__:ä» D:\\Code\\PythonProjects\\RagTranslationWorkflow\\Machine Learning.md æå–åˆ° 726 ä¸ªç¿»è¯‘å¯¹\n",
      "INFO:__main__:æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ€»è®¡æå– 1217 ä¸ªç¿»è¯‘å¯¹\n",
      "INFO:__main__:CSVæ•°æ®å·²ä¿å­˜åˆ°: D:\\Code\\PythonProjects\\RagTranslationWorkflow\\translation_pairs.csvï¼ˆ1217è¡Œï¼‰\n",
      "INFO:__main__:Elasticsearchæ‰¹é‡æ•°æ®å·²ä¿å­˜åˆ°: D:\\Code\\PythonProjects\\RagTranslationWorkflow\\es_bulk_data.jsonï¼ˆ1217æ¡æ–‡æ¡£ï¼‰\n",
      "INFO:__main__:å¯¼å…¥å‘½ä»¤ï¼šcurl -XPOST 'localhost:9200/_bulk' -H 'Content-Type: application/json' --data-binary @D:\\Code\\PythonProjects\\RagTranslationWorkflow\\es_bulk_data.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== å‰5æ¡è½¬æ¢ç»“æœ =====\n",
      "1. ID: 1b27a43fd0e1e8d5\n",
      "   ç´¢å¼•ç¼–å·: AITD-00007\n",
      "   è‹±æ–‡æœ¯è¯­: Accuracy\n",
      "   ä¸­æ–‡ç¿»è¯‘: å‡†ç¡®ç‡\n",
      "   å¸¸ç”¨ç¼©å†™: \n",
      "\n",
      "2. ID: 398238dcbc1e45e1\n",
      "   ç´¢å¼•ç¼–å·: AITD-00015\n",
      "   è‹±æ–‡æœ¯è¯­: Activation Function\n",
      "   ä¸­æ–‡ç¿»è¯‘: æ¿€æ´»å‡½æ•°\n",
      "   å¸¸ç”¨ç¼©å†™: \n",
      "\n",
      "3. ID: c85b46dd91e0c691\n",
      "   ç´¢å¼•ç¼–å·: AITD-00016\n",
      "   è‹±æ–‡æœ¯è¯­: Active Learning\n",
      "   ä¸­æ–‡ç¿»è¯‘: ä¸»åŠ¨å­¦ä¹ \n",
      "   å¸¸ç”¨ç¼©å†™: \n",
      "\n",
      "4. ID: e48fe510aa40da3a\n",
      "   ç´¢å¼•ç¼–å·: AITD-00056\n",
      "   è‹±æ–‡æœ¯è¯­: Area Under ROC Curve\n",
      "   ä¸­æ–‡ç¿»è¯‘: AUCï¼ˆROCæ›²çº¿ä¸‹æ–¹é¢ç§¯ï¼Œåº¦é‡åˆ†ç±»æ¨¡å‹å¥½åçš„æ ‡å‡†ï¼‰\n",
      "   å¸¸ç”¨ç¼©å†™: AUC\n",
      "\n",
      "5. ID: ddb239a0a2a70673\n",
      "   ç´¢å¼•ç¼–å·: AITD-00059\n",
      "   è‹±æ–‡æœ¯è¯­: Artificial Intelligence\n",
      "   ä¸­æ–‡ç¿»è¯‘: äººå·¥æ™ºèƒ½\n",
      "   å¸¸ç”¨ç¼©å†™: AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MarkdownToElasticsearchConverter:\n",
    "    \"\"\"å°†Markdownè¡¨æ ¼è½¬æ¢ä¸ºElasticsearchå¯è¯»å–æ ¼å¼\"\"\"\n",
    "    \n",
    "    def __init__(self, source_lang: str = 'en', target_lang: str = 'zh'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è½¬æ¢å™¨\n",
    "        \n",
    "        Args:\n",
    "            source_lang: æºè¯­è¨€ä»£ç ï¼Œé»˜è®¤ä¸º'en'\n",
    "            target_lang: ç›®æ ‡è¯­è¨€ä»£ç ï¼Œé»˜è®¤ä¸º'zh'\n",
    "        \"\"\"\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        \n",
    "    def parse_markdown_table(self, markdown_content: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        è§£æMarkdownè¡¨æ ¼å†…å®¹ï¼ˆä¿®å¤ç©ºåˆ—å¯¼è‡´çš„åˆ—æ•°ä¸åŒ¹é…é—®é¢˜ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            markdown_content: Markdownå†…å®¹å­—ç¬¦ä¸²\n",
    "            \n",
    "        Returns:\n",
    "            è§£æåçš„å­—å…¸åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        lines = markdown_content.strip().split('\\n')\n",
    "        if not lines:\n",
    "            logger.warning(\"Markdownå†…å®¹ä¸ºç©º\")\n",
    "            return []\n",
    "        \n",
    "        # æ‰¾åˆ°è¡¨æ ¼å¼€å§‹å’Œç»“æŸä½ç½®ï¼ˆä¼˜åŒ–ï¼šé¿å…ç©ºè¡Œå¯¼è‡´æå‰ç»ˆæ­¢ï¼‰\n",
    "        table_lines = []\n",
    "        in_table = False\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            if '|' in stripped_line:\n",
    "                in_table = True\n",
    "                table_lines.append(stripped_line)\n",
    "            # ä»…å½“éç©ºè¡Œä¸”ä¸å«|æ—¶ï¼Œæ‰ç»ˆæ­¢è¡¨æ ¼è§£æ\n",
    "            elif in_table and stripped_line:\n",
    "                break\n",
    "        \n",
    "        if len(table_lines) < 2:\n",
    "            logger.warning(\"è¡¨æ ¼è¡Œæ•°ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦è¡¨å¤´+åˆ†éš”è¡Œï¼‰\")\n",
    "            return []\n",
    "        \n",
    "        # è§£æè¡¨å¤´ï¼ˆå…³é”®ä¿®å¤ï¼šä¿ç•™ç©ºåˆ—ï¼Œåªå»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼‰\n",
    "        header_line = table_lines[0]\n",
    "        # åˆ†å‰²è¡¨å¤´ï¼šsplitåå»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼Œä¸­é—´ç©ºåˆ—ä¿ç•™\n",
    "        header_parts = header_line.split('|')\n",
    "        # å»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼ˆMarkdownè¡¨æ ¼é€šå¸¸ä»¥|å¼€å¤´/ç»“å°¾ï¼‰\n",
    "        if not header_parts[0].strip():\n",
    "            header_parts = header_parts[1:]\n",
    "        if header_parts and not header_parts[-1].strip():\n",
    "            header_parts = header_parts[:-1]\n",
    "        # è¡¨å¤´åˆ—ååšstripï¼Œä½†ä¿ç•™ç©ºåˆ—\n",
    "        headers = [col.strip() for col in header_parts]\n",
    "        logger.info(f\"è§£æåˆ°è¡¨å¤´ï¼š{headers}ï¼ˆå…±{len(headers)}åˆ—ï¼‰\")\n",
    "        \n",
    "        # è§£ææ•°æ®è¡Œï¼ˆå…³é”®ä¿®å¤ï¼šä¿ç•™ç©ºåˆ—ï¼Œå¯¹é½åˆ—æ•°ï¼‰\n",
    "        data = []\n",
    "        for line_num, line in enumerate(table_lines[2:], start=3):\n",
    "            if not line or '|' not in line:\n",
    "                continue\n",
    "                \n",
    "            # åˆ†å‰²æ•°æ®è¡Œï¼šåŒæ ·å»æ‰é¦–å°¾ç©ºå…ƒç´ ï¼Œä¿ç•™ä¸­é—´ç©ºåˆ—\n",
    "            line_parts = line.split('|')\n",
    "            if not line_parts[0].strip():\n",
    "                line_parts = line_parts[1:]\n",
    "            if line_parts and not line_parts[-1].strip():\n",
    "                line_parts = line_parts[:-1]\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªåˆ—å€¼åšstripï¼ˆç©ºåˆ—ä¼šå˜æˆ\"\"ï¼‰\n",
    "            columns = [col.strip() for col in line_parts]\n",
    "            \n",
    "            # å¯¹é½åˆ—æ•°ï¼šä¸è¶³è¡¥ç©ºå­—ç¬¦ä¸²ï¼Œå¤šä½™æˆªæ–­\n",
    "            if len(columns) < len(headers):\n",
    "                columns += [''] * (len(headers) - len(columns))\n",
    "            elif len(columns) > len(headers):\n",
    "                columns = columns[:len(headers)]\n",
    "            \n",
    "            # æ„é€ è¡Œå­—å…¸\n",
    "            row_dict = {headers[i]: columns[i] for i in range(len(headers))}\n",
    "            data.append(row_dict)\n",
    "        \n",
    "        logger.info(f\"è§£æåˆ°{len(data)}è¡Œæœ‰æ•ˆè¡¨æ ¼æ•°æ®\")\n",
    "        return data\n",
    "    \n",
    "    def extract_translation_pairs(self, table_data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ä»è¡¨æ ¼æ•°æ®ä¸­ç²¾å‡†æå–ä¸­è‹±æ–‡ç¿»è¯‘å¯¹\n",
    "        \n",
    "        Args:\n",
    "            table_data: è§£æåçš„è¡¨æ ¼æ•°æ®\n",
    "            \n",
    "        Returns:\n",
    "            ç¿»è¯‘å¯¹åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        translation_pairs = []\n",
    "        \n",
    "        for row_idx, row in enumerate(table_data):\n",
    "            # 1. ä¼˜å…ˆåŒ¹é…å›ºå®šåˆ—åï¼ˆä½ çš„è¡¨æ ¼ç»“æ„ï¼šè‹±æ–‡æœ¯è¯­/ä¸­æ–‡ç¿»è¯‘ï¼‰\n",
    "            source_text = row.get(\"è‹±æ–‡æœ¯è¯­\", \"\").strip()\n",
    "            target_text = row.get(\"ä¸­æ–‡ç¿»è¯‘\", \"\").strip()\n",
    "            \n",
    "            # è¿‡æ»¤æ— æ•ˆæ•°æ®ï¼ˆè‹±æ–‡/ä¸­æ–‡ä¸ºç©ºçš„è¡Œï¼‰\n",
    "            if not source_text or not target_text:\n",
    "                logger.debug(f\"ç¬¬{row_idx+1}è¡Œè·³è¿‡ï¼šè‹±æ–‡='{source_text}', ä¸­æ–‡='{target_text}'\")\n",
    "                continue\n",
    "            \n",
    "            # ç”Ÿæˆå”¯ä¸€IDï¼ˆç»“åˆç´¢å¼•ç¼–å·é¿å…é‡å¤ï¼‰\n",
    "            index_id = row.get(\"ç´¢å¼•ç¼–å·\", \"\")\n",
    "            id_str = f\"{index_id}_{source_text}_{target_text}\".encode('utf-8')\n",
    "            text_hash = hashlib.md5(id_str).hexdigest()[:16]\n",
    "            \n",
    "            # æ„é€ ç¿»è¯‘å¯¹ï¼ˆå®Œå–„å…ƒæ•°æ®ï¼‰\n",
    "            pair = {\n",
    "                'id': text_hash,\n",
    "                'source_text': source_text,\n",
    "                'target_text': target_text,\n",
    "                'source_lang': self.source_lang,\n",
    "                'target_lang': self.target_lang,\n",
    "                'metadata': {\n",
    "                    'index_id': index_id,\n",
    "                    'abbreviation': row.get(\"å¸¸ç”¨ç¼©å†™\", \"\").strip(),\n",
    "                    'source_ext': row.get(\"æ¥æº&æ‰©å±•\", \"\").strip(),\n",
    "                    'remarks': row.get(\"å¤‡æ³¨\", \"\").strip(),\n",
    "                    'original_row': row\n",
    "                }\n",
    "            }\n",
    "            translation_pairs.append(pair)\n",
    "        \n",
    "        logger.info(f\"æå–åˆ°{len(translation_pairs)}ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹\")\n",
    "        return translation_pairs\n",
    "    \n",
    "    def read_markdown_file(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        è¯»å–Markdownæ–‡ä»¶å†…å®¹ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            file_path: æ–‡ä»¶è·¯å¾„\n",
    "            \n",
    "        Returns:\n",
    "            æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            logger.info(f\"æˆåŠŸè¯»å–æ–‡ä»¶ï¼š{file_path}ï¼ˆå¤§å°ï¼š{len(content)}å­—èŠ‚ï¼‰\")\n",
    "            return content\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}\")\n",
    "        except PermissionError:\n",
    "            logger.error(f\"æ— æƒé™è¯»å–æ–‡ä»¶ï¼š{file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "    def process_markdown_files(self, file_paths: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        å¤„ç†å¤šä¸ªMarkdownæ–‡ä»¶\n",
    "        \n",
    "        Args:\n",
    "            file_paths: Markdownæ–‡ä»¶è·¯å¾„åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            æ‰€æœ‰æ–‡ä»¶çš„ç¿»è¯‘å¯¹åˆå¹¶åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        all_pairs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            # è¯»å–æ–‡ä»¶\n",
    "            content = self.read_markdown_file(file_path)\n",
    "            if not content:\n",
    "                continue\n",
    "            \n",
    "            # è§£æè¡¨æ ¼\n",
    "            table_data = self.parse_markdown_table(content)\n",
    "            if not table_data:\n",
    "                logger.warning(f\"{file_path} æœªè§£æåˆ°è¡¨æ ¼æ•°æ®\")\n",
    "                continue\n",
    "            \n",
    "            # æå–ç¿»è¯‘å¯¹\n",
    "            pairs = self.extract_translation_pairs(table_data)\n",
    "            logger.info(f\"ä» {file_path} æå–åˆ° {len(pairs)} ä¸ªç¿»è¯‘å¯¹\")\n",
    "            \n",
    "            # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯\n",
    "            for pair in pairs:\n",
    "                pair['metadata']['source_file'] = Path(file_path).name\n",
    "            \n",
    "            all_pairs.extend(pairs)\n",
    "        \n",
    "        logger.info(f\"æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ€»è®¡æå– {len(all_pairs)} ä¸ªç¿»è¯‘å¯¹\")\n",
    "        return all_pairs\n",
    "    \n",
    "    def save_to_csv(self, data: List[Dict], output_path: str):\n",
    "        \"\"\"\n",
    "        ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼ˆåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            data: ç¿»è¯‘å¯¹æ•°æ®\n",
    "            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            logger.warning(\"æ²¡æœ‰æ•°æ®å¯ä¿å­˜åˆ°CSV\")\n",
    "            return\n",
    "        \n",
    "        # å‡†å¤‡CSVæ•°æ®ï¼ˆå±•å¼€å…ƒæ•°æ®ï¼‰\n",
    "        csv_data = []\n",
    "        for item in data:\n",
    "            csv_row = {\n",
    "                'id': item['id'],\n",
    "                'index_id': item['metadata'].get('index_id', ''),\n",
    "                'source_text': item['source_text'],\n",
    "                'target_text': item['target_text'],\n",
    "                'abbreviation': item['metadata'].get('abbreviation', ''),\n",
    "                'remarks': item['metadata'].get('remarks', ''),\n",
    "                'source_file': item['metadata'].get('source_file', '')\n",
    "            }\n",
    "            csv_data.append(csv_row)\n",
    "        \n",
    "        # ä¿å­˜ä¸ºCSVï¼ˆutf-8-sigå…¼å®¹Excelï¼‰\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"CSVæ•°æ®å·²ä¿å­˜åˆ°: {output_path}ï¼ˆ{len(df)}è¡Œï¼‰\")\n",
    "    \n",
    "    def save_to_json(self, data: List[Dict], output_path: str):\n",
    "        \"\"\"\n",
    "        ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆElasticsearchæ‰¹é‡å¯¼å…¥æ ¼å¼ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            data: ç¿»è¯‘å¯¹æ•°æ®\n",
    "            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            logger.warning(\"æ²¡æœ‰æ•°æ®å¯ä¿å­˜åˆ°JSON\")\n",
    "            return\n",
    "        \n",
    "        # Elasticsearchæ‰¹é‡å¯¼å…¥æ ¼å¼ï¼ˆNDJSONï¼‰\n",
    "        es_data = []\n",
    "        for item in data:\n",
    "            # ç´¢å¼•æ“ä½œ\n",
    "            index_op = {\n",
    "                \"index\": {\n",
    "                    \"_index\": \"ai_terminology\",  # æ›´è´´åˆä¸šåŠ¡çš„ç´¢å¼•å\n",
    "                    \"_id\": item['id']\n",
    "                }\n",
    "            }\n",
    "            # æ–‡æ¡£æ•°æ®ï¼ˆç²¾ç®€å…ƒæ•°æ®ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼‰\n",
    "            doc_data = {\n",
    "                \"source_text\": item['source_text'],\n",
    "                \"target_text\": item['target_text'],\n",
    "                \"source_lang\": item['source_lang'],\n",
    "                \"target_lang\": item['target_lang'],\n",
    "                \"index_id\": item['metadata'].get('index_id'),\n",
    "                \"abbreviation\": item['metadata'].get('abbreviation'),\n",
    "                \"remarks\": item['metadata'].get('remarks'),\n",
    "                \"source_file\": item['metadata'].get('source_file')\n",
    "            }\n",
    "            \n",
    "            es_data.append(index_op)\n",
    "            es_data.append(doc_data)\n",
    "        \n",
    "        # ä¿å­˜ä¸ºNDJSON\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for item in es_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        logger.info(f\"Elasticsearchæ‰¹é‡æ•°æ®å·²ä¿å­˜åˆ°: {output_path}ï¼ˆ{len(es_data)//2}æ¡æ–‡æ¡£ï¼‰\")\n",
    "        logger.info(f\"å¯¼å…¥å‘½ä»¤ï¼šcurl -XPOST 'localhost:9200/_bulk' -H 'Content-Type: application/json' --data-binary @{output_path}\")\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºè½¬æ¢å™¨å®ä¾‹\n",
    "    converter = MarkdownToElasticsearchConverter(source_lang='en', target_lang='zh')\n",
    "    \n",
    "    # æ›¿æ¢ä¸ºä½ çš„Markdownæ–‡ä»¶è·¯å¾„\n",
    "    file_paths = [\n",
    "        \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\AI for Science.md\", \n",
    "        \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\Machine Learning.md\"\n",
    "    ]\n",
    "    \n",
    "    # å¤„ç†æ–‡ä»¶\n",
    "    results = converter.process_markdown_files(file_paths)\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    if results:\n",
    "        # ä¿å­˜ä¸ºCSVï¼ˆä¾¿äºæŸ¥çœ‹å’Œç¼–è¾‘ï¼‰\n",
    "        csv_output = \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\translation_pairs.csv\"\n",
    "        converter.save_to_csv(results, csv_output)\n",
    "        \n",
    "        # ä¿å­˜ä¸ºElasticsearchæ‰¹é‡å¯¼å…¥æ ¼å¼\n",
    "        json_output = \"D:\\\\Code\\\\PythonProjects\\\\RagTranslationWorkflow\\\\es_bulk_data.json\"\n",
    "        converter.save_to_json(results, json_output)\n",
    "        \n",
    "        # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®ï¼ˆéªŒè¯ç»“æœï¼‰\n",
    "        print(\"\\n===== å‰5æ¡è½¬æ¢ç»“æœ =====\")\n",
    "        for i, item in enumerate(results[:5]):\n",
    "            print(f\"{i+1}. ID: {item['id']}\")\n",
    "            print(f\"   ç´¢å¼•ç¼–å·: {item['metadata']['index_id']}\")\n",
    "            print(f\"   è‹±æ–‡æœ¯è¯­: {item['source_text']}\")\n",
    "            print(f\"   ä¸­æ–‡ç¿»è¯‘: {item['target_text']}\")\n",
    "            print(f\"   å¸¸ç”¨ç¼©å†™: {item['metadata']['abbreviation']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"æœªæå–åˆ°ä»»ä½•ç¿»è¯‘å¯¹ï¼Œè¯·æ£€æŸ¥ï¼š\")\n",
    "        print(\"1. æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®\")\n",
    "        print(\"2. Markdownè¡¨æ ¼æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼ˆåŒ…å«|åˆ†éš”ç¬¦ï¼‰\")\n",
    "        print(\"3. è¡¨æ ¼æ˜¯å¦æœ‰ã€Œè‹±æ–‡æœ¯è¯­ã€å’Œã€Œä¸­æ–‡ç¿»è¯‘ã€åˆ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3379e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

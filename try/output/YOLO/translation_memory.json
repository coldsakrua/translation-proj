{
  "yolo_ch0_ck0": {
    "book_id": "yolo",
    "chapter_id": 0,
    "chunk_id": 0,
    "source_text": "You Only Look Once: Unified, Real-Time Object Detection",
    "translation": "你只需看一次：统一的、实时目标检测",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:00:17.618917"
  },
  "yolo_ch1_ck0": {
    "book_id": "yolo",
    "chapter_id": 1,
    "chunk_id": 0,
    "source_text": "Abstract We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",
    "translation": "摘要：我们提出了YOLO，一种新的目标检测方法。先前的目标检测工作将分类器重新用于执行检测任务。相反，我们将目标检测框架为回归问题，以空间上分离的边界框和相关的类别概率。一个单一的神经网络直接从完整图像中预测边界框和类别概率，仅需要一次评估。由于整个检测流程是一个单一网络，它可以直接针对检测性能进行端到端优化。我们的统一架构非常快速。我们的基准YOLO模型以实时速度处理图像，每秒45帧。网络的较小版本，快速YOLO，以惊人的每秒155帧速度处理图像，同时仍然达到了其他实时检测器两倍的平均精度均值。与最先进的检测系统相比，YOLO在定位误差上更多，但在背景上预测假阳性的可能性较小。最后，YOLO学习了非常通用的目标表示。当从自然图像泛化到其他领域（如艺术作品）时，它超越了其他检测方法，包括可变形部件模型和R-CNN。",
    "quality_score": 9.0,
    "saved_at": "2026-01-12T20:02:42.298461"
  },
  "yolo_ch2_ck1": {
    "book_id": "yolo",
    "chapter_id": 2,
    "chunk_id": 1,
    "source_text": "1. Introduction Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems. Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10]. More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13]. These complex pipelines are slow and hard to optimize because each individual component must be trained separately. We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are. YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection. Figure 1: The YOLO Detection System. Processing images with YOLO is simple and straightforward. Our system (1) resizes the input image to 448 × 448, (2) runs a single convolutional network on the image, and (3) thresholds the resulting detections by the model’s confidence. First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/. Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN. Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs. YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments. All of our training and testing code is open source. A variety of pretrained models are also available to download.",
    "translation": "摘要：人类只需一瞥就能立刻知道图片中有什么物体、它们的位置以及它们之间的互动。人类视觉系统快速而准确，使我们能够像驾驶这样的复杂任务几乎不需要有意识的思考。快速、准确的目标检测算法将使计算机能够在没有特殊传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时场景信息，并为通用、响应式机器人系统的发展提供可能。当前的目标检测系统将分类器重新用于执行检测任务。为了检测一个物体，这些系统需要一个针对该物体的分类器，并在测试图像的各个位置和尺度上对其进行评估。像可变形部件模型（DPM）这样的系统采用滑动窗口方法，将分类器在整个图像的均匀间隔位置上运行[10]。更近的方法，如R-CNN，使用区域提议方法先在图像中生成潜在的边界框，然后在这些提议的框上运行分类器[13]。分类后，使用后处理来细化边界框，消除重复检测，并根据场景中的其他物体重新评分。这些复杂的流程缓慢且难以优化，因为每个单独的组件必须分别训练。我们将目标检测重新定义为一个单一的回归问题，从图像像素直接到边界框坐标和类别概率。使用我们的系统，只需对图像“你只需看一次”（YOLO），即可预测存在的物体及其位置。YOLO简洁明了：见图1。单个卷积网络同时预测多个边界框和这些框的类别概率。YOLO在完整图像上训练，并直接优化检测性能。这种统一模型比传统目标检测方法有几个好处。图1：YOLO检测系统。用YOLO处理图像简单直接。我们的系统（1）将输入图像调整为448×448，（2）在图像上运行单个卷积网络，以及（3）通过模型的置信度阈值化结果检测。首先，YOLO速度极快。由于我们将检测框架为回归问题，我们不需要复杂的流程。我们只需在测试时在新图像上运行我们的神经网络来预测检测。我们的基础网络在Titan X GPU上以无批处理的方式运行，速度为每秒45帧，快速版本运行速度超过150 fps。这意味着我们可以实时处理视频流，延迟不到25毫秒。此外，YOLO的均值平均精度是其他实时系统的两倍多。有关我们的系统在网络摄像头上实时运行的演示，请访问我们的项目网页：http://pjreddie.com/yolo/。其次，YOLO在进行预测时全局考虑图像。与基于滑动窗口和区域提议的技术不同，YOLO在训练和测试时看到整个图像，因此它隐式地编码了关于类别及其外观的上下文信息。快速R-CNN，一种顶级检测方法[14]，将图像中的背景补丁误认为物体，因为它看不到更大的上下文。YOLO与快速R-CNN相比，背景错误不到一半。第三，YOLO学习了可推广的物体表示。在自然图像上训练并在艺术作品上测试时，YOLO以较大优势超越了像DPM和R-CNN这样的顶级检测方法。由于YOLO高度可推广，当应用于新领域或意外输入时，它不太可能崩溃。YOLO在准确性方面仍落后于最先进的检测系统。虽然它可以快速识别图像中的物体，但在精确定位某些物体，尤其是小物体方面存在困难。我们在实验中进一步考察了这些权衡。我们所有的训练和测试代码都是开源的。也可以下载各种预训练模型。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:06:56.987767"
  },
  "yolo_ch3_ck1": {
    "book_id": "yolo",
    "chapter_id": 3,
    "chunk_id": 1,
    "source_text": "2. Unified Detection We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision. Our system divides the input image into an $S\\times S$ grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. Each grid cell predicts $B$ bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally we define confidence as $\\Pr(\\textrm{Object}) * \\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}$. If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth. Each bounding box consists of 5 predictions: $x$, $y$, $w$, $h$, and confidence. The $(x,y)$ coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box. Each grid cell also predicts $C$ conditional class probabilities, $\\Pr(\\textrm{Class}_i | \\textrm{Object})$. These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes $B$. At test time we multiply the conditional class probabilities and the individual box confidence predictions, $$\\Pr(\\textrm{Class}_i | \\textrm{Object}) * \\Pr(\\textrm{Object}) * \\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}} = \\Pr(\\textrm{Class}_i)*\\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}$$ which gives us class-specific confidence scores for each box. These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object. For evaluating YOLO on Pascal VOC, we use $S=7$, $B=2$. Pascal VOC has 20 labelled classes so $C=20$. Our final prediction is a $7\\times 7 \\times 30$ tensor. The Model. Our system models detection as a regression problem. It divides the image into an $S \\times S$ grid and for each grid cell predicts $B$ bounding boxes, confidence for those boxes, and $C$ class probabilities. These predictions are encoded as an $S \\times S \\times (B*5 + C)$ tensor.",
    "translation": "2. 统一检测 我们将目标检测的不同组件统一到单个神经网络中。我们的网络使用整个图像的特征来预测每个边界框。它还同时为图像中的所有类别预测所有边界框。这意味着我们的网络对整个图像以及图像中的所有物体进行全局推理。YOLO的设计支持端到端训练和实时速度，同时保持高平均精度。我们的系统将输入图像划分为一个$S\\times S$网格。如果一个物体的中心落在某个网格单元内，那么该网格单元负责检测该物体。每个网格单元预测$B$个边界框和这些框的置信度分数。这些置信度分数反映了模型对框包含物体的置信程度，以及它认为预测的框有多准确。形式上，我们定义置信度为$\\Pr(\\textrm{物体}) * \\textrm{IOU}_{\\textrm{预测}}^{\\textrm{真实}}$。如果该单元内没有物体存在，置信度分数应为零。否则，我们希望置信度分数等于预测框与真实框之间的交并比（IOU）。每个边界框由5个预测组成：$x$, $y$, $w$, $h$, 和置信度。坐标$(x,y)$表示框的中心相对于网格单元边界的位置。宽度和高度是相对于整个图像预测的。最后，置信度预测表示预测框与任何真实框之间的IOU。每个网格单元还预测$C$个条件类别概率，$\\Pr(\\textrm{类别}_i | \\textrm{物体})$。这些概率以网格单元包含物体为条件。无论边界框数量$B$如何，我们只预测每个网格单元的一组类别概率。在测试时，我们将条件类别概率与单个框的置信度预测相乘，$$\\Pr(\\textrm{类别}_i | \\textrm{物体}) * \\Pr(\\textrm{物体}) * \\textrm{IOU}_{\\textrm{预测}}^{\\textrm{真实}} = \\Pr(\\textrm{类别}_i)*\\textrm{IOU}_{\\textrm{预测}}^{\\textrm{真实}}$$，这为我们提供了每个框的类别特定的置信度分数。这些分数编码了该类别出现在框中的概率以及预测框与物体的拟合程度。在Pascal VOC上评估YOLO时，我们使用$S=7$，$B=2$。Pascal VOC有20个标记类别，所以$C=20$。我们的最终预测是一个$7\\times 7 \\times 30$张量。模型。我们的系统将检测建模为一个回归问题。它将图像划分为一个$S \\times S$网格，并对每个网格单元预测$B$个边界框、这些框的置信度以及$C$个类别概率。这些预测被编码为一个$S \\times S \\times (B*5 + C)$张量。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:11:02.056008"
  },
  "yolo_ch4_ck1": {
    "book_id": "yolo",
    "chapter_id": 4,
    "chunk_id": 1,
    "source_text": "2.1. Network Design We implement this model as a convolutional neural network and evaluate it on the Pascal VOC detection dataset [9]. The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates. Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use $1 \\times 1$ reduction layers followed by $3 \\times 3$ convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3. Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating $1 \\times 1$ convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classification task at half the resolution ($224 \\times 224$ input image) and then double the resolution for detection. Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating $1 \\times 1$ convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classification task at half the resolution ($224 \\times 224$ input image) and then double the resolution for detection. We also train a fast version of YOLO designed to push the boundaries of fast object detection. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO. The final output of our network is the $7 \\times 7 \\times 30$ tensor of predictions.",
    "translation": "2.1. 网络设计 我们将此模型实现为一个卷积神经网络，并在Pascal VOC检测数据集[9]上对其进行评估。网络的初始卷积层从图像中提取特征，而全连接层预测输出概率和坐标。我们的网络架构受到谷歌网络图像分类模型[34]的启发。我们的网络包含24个卷积层，后面跟着2个全连接层。与谷歌网络使用的初始层模块不同，我们简单地使用$1 \\times 1$降维层，然后是$3 \\times 3$卷积层，类似于Lin等人[22]。完整的网络结构如图3所示。图3：网络架构。我们的检测网络包含24个卷积层，后面跟着2个全连接层。交替的$1 \\times 1$卷积层减小来自前层的特征空间。我们在ImageNet分类任务上以半分辨率（$224 \\times 224$输入图像）预训练卷积层，然后为检测任务将分辨率加倍。我们还训练了一个快速版本的YOLO，旨在推动快速目标检测的极限。快速YOLO使用的神经网络包含较少的卷积层（9层而不是24层）和较少的滤波器。除了网络大小外，YOLO和快速YOLO之间的所有训练和测试参数都是相同的。我们网络的最终输出是$7 \\times 7 \\times 30$的预测张量。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:14:34.562427"
  },
  "yolo_ch5_ck1": {
    "book_id": "yolo",
    "chapter_id": 5,
    "chunk_id": 1,
    "source_text": "2.2. Training We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of $88\\%$ on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26]. We then convert the model to perform detection. Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance [29]. Following their example, we add four convolutional layers and two fully connected layers with randomly initialized weights. Detection often requires fine-grained visual information so we increase the input resolution of the network from $224 \\times 224$ to $448 \\times 448$. Our final layer predicts both class probabilities and bounding box coordinates. We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box $x$ and $y$ coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1. We use a linear activation function for the final layer and all other layers use the following leaky rectified linear activation:$$\\phi(x) =\\begin{cases} x, if x > 0 \\\\ 0.1x, otherwise\\end{cases}$$ We optimize for sum-squared error in the output of our model. We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal. Also, in every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, $\\lambda_\\textrm{coord}$ and $\\lambda_\\textrm{noobj}$ to accomplish this. We set $\\lambda_\\textrm{coord} = 5$ and $\\lambda_\\textrm{noobj} = .5$. Sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly. YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. During training we optimize the following, multi-part loss function:$$\\begin{multline}\\lambda_\\textbf{coord}\\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{obj}} \\left[ \\left( x_i - \\hat{x}_i \\right)^2 + \\left( y_i - \\hat{y}_i \\right)^2 \\right]\\\\+ \\lambda_\\textbf{coord}\\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{obj}} \\left[ \\left( \\sqrt{w_i} - \\sqrt{\\hat{w}_i} \\right)^2 + \\left( \\sqrt{h_i} - \\sqrt{\\hat{h}_i} \\right)^2 \\right]\\\\+ \\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{obj}} \\left( C_i - \\hat{C}_i \\right)^2\\\\+ \\lambda_\\textrm{noobj}\\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{noobj}} \\left( C_i - \\hat{C}_i \\right)^2\\\\+ \\sum_{i = 0}^{S^2}\\mathbb{1}_i^{\\text{obj}} \\sum_{c \\in \\textrm{classes}} \\left( p_i(c) - \\hat{p}_i(c) \\right)^2\\end{multline}$$where $\\mathbb{1}_i^{\\text{obj}}$ denotes if object appears in cell $i$ and $\\mathbb{1}_{ij}^{\\text{obj}}$ denotes that the $j$th bounding box predictor in cell $i$ is “responsible” for that prediction. Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest IOU of any predictor in that grid cell). We train the network for about 135 epochs on the training and validation data sets from Pascal VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of $0.9$ and a decay of $0.0005$. Our learning rate schedule is as follows: For the first epochs we slowly raise the learning rate from $10^{-3}$ to $10^{-2}$. If we start at a high learning rate our model often diverges due to unstable gradients. We continue training with $10^{-2}$ for 75 epochs, then $10^{-3}$ for 30 epochs, and finally $10^{-4}$ for 30 epochs. To avoid overfitting we use dropout and extensive data augmentation. A dropout layer with rate $=.5$ after the first connected layer prevents co-adaptation between layers [18]. For data augmentation we introduce random scaling and translations of up to $20\\%$ of the original image size. We also randomly adjust the exposure and saturation of the image by up to a factor of $1.5$ in the HSV color space.",
    "translation": "2.2. 训练 我们在ImageNet 1000类竞赛数据集[30]上预训练我们的卷积层。预训练时，我们使用图3中的前20个卷积层，接着是一个平均池化层和一个全连接层。我们训练这个网络大约一周，在ImageNet 2012验证集上实现了单裁剪top-5准确率为$88\\%$，与Caffe模型库中的GoogLeNet模型[24]相当。我们使用Darknet框架进行所有训练和推理[26]。然后我们将模型转换为执行检测任务。Ren等人[29]表明，在预训练网络中添加卷积和全连接层可以提高性能。遵循他们的例子，我们添加了四个卷积层和两个全连接层，这些层的权重是随机初始化的。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从$224 \\times 224$增加到$448 \\times 448$。我们的最后一层预测类别概率和边界框坐标。我们通过图像宽度和高度归一化边界框的宽度和高度，使它们落在0和1之间。我们将边界框的$x$和$y$坐标参数化为特定网格单元位置的偏移量，使它们也限制在0和1之间。我们对最后一层使用线性激活函数，其他所有层都使用以下泄漏的修正线性激活：$$\\phi(x) =\\begin{cases} x, if x > 0 \\\\ 0.1x, otherwise\\end{cases}$$ 我们优化模型输出的平方误差和。我们使用平方误差和是因为它容易优化，但可能不完全符合我们最大化平均精度的目标。它将定位误差与分类误差等同起来，这可能不是理想的。此外，在每张图像中，许多网格单元不包含任何对象。这将那些单元的“置信度”分数推向零，经常压倒包含对象的单元的梯度。这可能导致模型不稳定，导致训练早期发散。为了解决这个问题，我们增加了边界框坐标预测的损失，并减少了不包含对象的盒子的置信度预测的损失。我们使用两个参数，$\\lambda_\\textrm{coord}$和$\\lambda_\\textrm{noobj}$来实现这一点。我们将$\\lambda_\\textrm{coord} = 5$和$\\lambda_\\textrm{noobj} = .5$。平方误差和也同等地权衡大盒子和小盒子的错误。我们的错误度量应该反映大盒子中的小偏差比小盒子中的小偏差重要性低。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。YOLO每个网格单元预测多个边界框。在训练时，我们只希望一个边界框预测器对每个对象负责。我们根据哪个预测与真实值的当前交并比最高，分配一个预测器来“负责”预测对象。这导致边界框预测器之间的专业化。每个预测器更擅长预测特定尺寸、纵横比或对象类别，提高整体召回率。在训练期间，我们优化了以下多部分损失函数：$$\\begin{multline}\\lambda_\\textbf{coord}\\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{obj}} \\left[ \\left( x_i - \\hat{x}_i \\right)^2 + \\left( y_i - \\hat{y}_i \\right)^2 \\right]\\\\+ \\lambda_\\textbf{coord}\\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{obj}} \\left[ \\left( \\sqrt{w_i} - \\sqrt{\\hat{w}_i} \\right)^2 + \\left( \\sqrt{h_i} - \\sqrt{\\hat{h}_i} \\right)^2 \\right]\\\\+ \\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{obj}} \\left( C_i - \\hat{C}_i \\right)^2\\\\+ \\lambda_\\textrm{noobj}\\sum_{i = 0}^{S^2} \\sum_{j = 0}^{B} \\mathbb{1}_{ij}^{\\text{noobj}} \\left( C_i - \\hat{C}_i \\right)^2\\\\+ \\sum_{i = 0}^{S^2}\\mathbb{1}_i^{\\text{obj}} \\sum_{c \\in \\textrm{classes",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:17:25.979139"
  },
  "yolo_ch6_ck0": {
    "book_id": "yolo",
    "chapter_id": 6,
    "chunk_id": 0,
    "source_text": "2.3. Inference Just like in training, predicting detections for a test image only requires one network evaluation. On Pascal VOC the network predicts 98 bounding boxes per image and class probabilities for each box. YOLO is extremely fast at test time since it only requires a single network evaluation, unlike classifier-based methods. The grid design enforces spatial diversity in the bounding box predictions. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections. While not critical to performance as it is for R-CNN or DPM, non-maximal suppression adds $2-3\\%$ in mAP.",
    "translation": "2.3. 推理 与训练时相似，预测测试图像的检测结果仅需要一次网络评估。在Pascal VOC数据集上，网络每张图像预测98个边界框，并为每个框预测类别概率。由于YOLO在测试时仅需要单一网络评估，与基于分类器的方法不同，它的速度极快。网格设计加强了边界框预测的空间多样性。通常很明显物体落在哪个网格单元内，网络只为每个物体预测一个框。然而，一些大物体或靠近多个单元边界的物体可以被多个单元很好地定位。非极大值抑制可以用来解决这些多重检测问题。虽然它对性能的影响不像R-CNN或DPM那样关键，但非极大值抑制仍可增加平均精度均值 $2-3\\%$。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:18:29.273442"
  },
  "yolo_ch7_ck0": {
    "book_id": "yolo",
    "chapter_id": 7,
    "chunk_id": 0,
    "source_text": "2.4. Limitations of YOLO YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds. Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image. Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.",
    "translation": "2.4. YOLO的局限性 YOLO在边界框预测上施加了强烈的空间约束，因为每个网格单元仅预测两个框，并且只能有一个类别。这种空间约束限制了模型能够预测的附近物体的数量。模型在处理小物体成群出现的情况时表现不佳，例如鸟群。由于模型是从数据中学习预测边界框的，因此它在泛化到新或不寻常的长宽比或配置中的对象时存在困难。模型还使用相对粗糙的特征来预测边界框，因为我们的架构从输入图像开始有多个下采样层。最后，尽管我们使用一个近似检测性能的损失函数进行训练，我们的损失函数将小边界框与大边界框中的错误同等对待。一个大框中的小错误通常无害，但一个小框中的小错误对交并比的影响要大得多。我们错误的主要原因是错误定位。",
    "quality_score": 9.0,
    "saved_at": "2026-01-12T20:21:00.505844"
  },
  "yolo_ch8_ck1": {
    "book_id": "yolo",
    "chapter_id": 8,
    "chunk_id": 1,
    "source_text": "3. Comparison to Other Detection Systems Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences. Deformable parts models. Deformable parts models (DPM) use a sliding window approach to object detection [10]. DPM uses a disjoint pipeline to extract static features, classify regions, predict bounding boxes for high scoring regions, etc. Our system replaces all of these disparate parts with a single convolutional neural network. The network performs feature extraction, bounding box prediction, non-maximal suppression, and contextual reasoning all concurrently. Instead of static features, the network trains the features in-line and optimizes them for the detection task. Our unified architecture leads to a faster, more accurate model than DPM. R-CNN. R-CNN and its variants use region proposals instead of sliding windows to find objects in images. Selective Search [35] generates potential bounding boxes, a convolutional network extracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max suppression eliminates duplicate detections. Each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow, taking more than 40 seconds per image at test time [14]. YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model. Other Fast Detectors. Fast and Faster R-CNN focus on speeding up the R-CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search [14] [28]. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance. Many research efforts focus on speeding up the DPM pipeline [31] [38] [5]. They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM [31] actually runs in real-time. Instead of trying to optimize individual components of a large detection pipeline, YOLO throws out the pipeline entirely and is fast by design. Detectors for single classes like faces or people can be highly optimized since they have to deal with much less variation [37]. YOLO is a general purpose detector that learns to detect a variety of objects simultaneously. Deep MultiBox. Unlike R-CNN, Szegedy et al. train a convolutional neural network to predict regions of interest [8] instead of using Selective Search. MultiBox can also perform single object detection by replacing the confidence prediction with a single class prediction. However, MultiBox cannot perform general object detection and is still just a piece in a larger detection pipeline, requiring further image patch classification. Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system. OverFeat. Sermanet et al. train a convolutional neural network to perform localization and adapt that localizer to perform detection [32]. OverFeat efficiently performs sliding window detection but it is still a disjoint system. OverFeat optimizes for localization, not detection performance. Like DPM, the localizer only sees local information when making a prediction. OverFeat cannot reason about global context and thus requires significant post-processing to produce coherent detections. MultiGrasp. Our work is similar in design to work on grasp detection by Redmon et al [27]. Our grid approach to bounding box prediction is based on the MultiGrasp system for regression to grasps. However, grasp detection is a much simpler task than object detection. MultiGrasp only needs to predict a single graspable region for an image containing one object. It doesn’t have to estimate the size, location, or boundaries of the object or predict it’s class, only find a region suitable for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image.",
    "translation": "3. 与其他检测系统的比较 目标检测是计算机视觉领域的核心问题。检测流程通常从输入图像中提取一系列鲁棒特征开始（哈尔特征[25]、尺度不变特征变换[23]、方向梯度直方图[4]、卷积特征[6]）。然后，分类器[36, 21, 13, 10]或定位器[1, 32]被用来在特征空间中识别物体。这些分类器或定位器要么在整个图像上以滑动窗口的方式运行，要么在图像的某些区域子集上运行[35, 15, 39]。我们将YOLO检测系统与几个顶级检测框架进行比较，突出关键的相似之处和差异。可变形部件模型。可变形部件模型（DPM）使用滑动窗口方法进行目标检测[10]。DPM使用一个分离的流程来提取静态特征、分类区域、预测高得分区域的边界框等。我们的系统用单个卷积神经网络替换了所有这些不同的部分。网络同时执行特征提取、边界框预测、非极大值抑制和上下文推理。与静态特征不同，网络在线训练特征并针对检测任务优化它们。我们的统一架构比DPM更快、更准确。R-CNN。R-CNN及其变体使用区域提议代替滑动窗口来在图像中找到物体。选择性搜索[35]生成潜在的边界框，一个卷积网络提取特征，支持向量机评分框，线性模型调整边界框，非极大值抑制消除重复检测。这个复杂流程的每个阶段都必须独立精确调整，得到的系统非常慢，在测试时每张图像需要超过40秒[14]。YOLO与R-CNN有一些相似之处。每个网格单元提出潜在的边界框，并使用卷积特征评分这些框。然而，我们的系统对网格单元提议施加空间约束，这有助于减少对同一物体的多次检测。我们的系统还提出了远少于选择性搜索的边界框，每张图像只有98个，相比之下选择性搜索大约有2000个。最后，我们的系统将这些单独的组件组合成一个单一的、联合优化的模型。其他快速检测器。快速R-CNN和快速R-CNN专注于通过共享计算和使用神经网络提出区域而不是选择性搜索来加速R-CNN框架[14] [28]。虽然它们在速度和准确性方面比R-CNN有所提高，但两者都未能实现实时性能。许多研究工作集中在加速DPM流程[31] [38] [5]。它们加速了HOG计算，使用级联，并推动计算到GPU。然而，只有30Hz DPM[31]实际上在实时运行。与尝试优化大型检测流程的各个组件不同，YOLO完全抛弃了流程，并且通过设计快速。针对单一类别的检测器，如面部或人员，可以高度优化，因为它们需要处理的变化要少得多[37]。YOLO是一个通用检测器，学习同时检测多种物体。深度多框。与R-CNN不同，Szegedy等人训练一个卷积神经网络来预测感兴趣区域[8]，而不是使用选择性搜索。多边界框也可以通过替换置信度预测来执行单物体检测。然而，多边界框不能执行一般目标检测，仍然只是一个更大的检测流程的一部分，需要进一步的图像补丁分类。YOLO和多边界框都使用卷积网络来预测图像中的边界框，但YOLO是一个完整的检测系统。OverFeat。Sermanet等人训练一个卷积神经网络来进行定位，并将该定位器适应于进行检测[32]。OverFeat有效地执行滑动窗口检测，但它仍然是一个分离的系统。OverFeat针对定位进行优化，而不是检测性能。像DPM一样，定位器在进行预测时只看到局部信息。OverFeat不能理解全局上下文，因此需要大量的后处理来产生连贯的检测。多抓取。我们的作品在设计上与Redmon等人关于抓取检测的作品相似[27]。我们对边界框预测的网格方法基于多抓取系统，用于回归到抓取。然而，抓取检测比目标检测任务要简单得多。多抓取只需要预测一个图像中包含一个物体的可抓取区域。它不需要估计物体的大小、位置或边界，也不预测它的类别，只找到适合抓取的区域。YOLO预测图像中多个类别的多个物体的边界框和类别概率。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:23:34.215393"
  },
  "yolo_ch9_ck0": {
    "book_id": "yolo",
    "chapter_id": 9,
    "chunk_id": 0,
    "source_text": "4. Experiments First we compare YOLO with other real-time detection systems on PASCAL VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.",
    "translation": "4. 实验 首先，我们比较了YOLO与其他实时检测系统在PASCAL VOC 2007数据集上的性能。为了理解YOLO与R-CNN变体之间的差异，我们探究了YOLO和快速R-CNN（R-CNN中性能最高的版本之一[14]）在VOC 2007上的错误。基于不同的错误特征，我们展示了YOLO可以用来重新评分快速R-CNN检测结果，并减少背景假阳性错误，从而显著提升性能。我们还展示了VOC 2012的结果，并将平均精度均值与当前最先进的方法进行了比较。最后，我们证明了YOLO在两个艺术作品数据集上比其他检测器更好地泛化到新领域。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:24:26.207561"
  },
  "yolo_ch10_ck1": {
    "book_id": "yolo",
    "chapter_id": 10,
    "chunk_id": 1,
    "source_text": "4.1. Comparison to Other Real-Time Systems Many research efforts in object detection focus on making standard detection pipelines fast [5] [38] [31] [14] [17] [28]. However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don’t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems. Fast YOLO is the fastest object detection method on PASCAL; as far as we know, it is the fastest extant object detector. With $52.7\\%$ mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to $63.4\\%$ while still maintaining real-time performance. We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models. Fastest DPM effectively speeds up DPM without sacrificing much mAP but it still misses real-time performance by a factor of 2 [38]. It also is limited by DPM’s relatively low accuracy on detection compared to neural network approaches. R-CNN minus R replaces Selective Search with static bounding box proposals [20]. While it is much faster than R-CNN, it still falls short of real-time and takes a significant accuracy hit from not having good proposals. Fast R-CNN speeds up the classification stage of R-CNN but it still relies on selective search which can take around 2 seconds per image to generate bounding box proposals. Thus it has high mAP but at 0.5 fps it is still far from real-time. The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8]. In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate. Table 1:Real-Time Systems on Pascal VOC 2007. Comparing the performance and speed of fast detectors. Fast YOLO is the fastest detector on record for Pascal VOC detection and is still twice as accurate as any other real-time detector. YOLO is 10 mAP more accurate than the fast version while still well above real-time in speed.",
    "translation": "4.1. 与其他实时系统的比较 许多目标检测的研究工作专注于使标准检测流程加速[5][38][31][14][17][28]。然而，只有Sadeghi等人实际开发了一个实时运行（每秒30帧或更好）的检测系统[31]。我们将YOLO与他们基于GPU的DPM实现进行比较，该实现的运行速度为30Hz或100Hz。尽管其他工作未能达到实时的里程碑，我们也对比了它们的相对mAP和速度，以检验目标检测系统中的准确性-性能权衡。快速YOLO是PASCAL上最快的目标检测方法；据我们所知，它是现存最快的目标检测器。其mAP为52.7%，准确性是先前实时检测工作的两倍多。YOLO将mAP推至63.4%，同时仍保持实时性能。我们还使用VGG-16训练了YOLO。这个模型更准确，但也明显比YOLO慢。它有助于与其他依赖VGG-16的检测系统进行比较，但由于速度慢于实时，本文其余部分集中在我们更快的模型上。最快的DPM有效地加速了DPM，而没有牺牲太多的mAP，但它仍然错过了实时性能，差了2倍[38]。它还受到DPM相比神经网络方法在检测上相对较低的准确性的限制。不包含R的R-CNN用静态边界框提案替换了选择性搜索[20]。虽然它比R-CNN快得多，但仍未达到实时，并因缺乏好的提案而遭受显著的准确性损失。快速R-CNN加快了R-CNN的分类阶段，但它仍然依赖于选择性搜索，后者每张图像生成边界框提案大约需要2秒钟。因此，尽管它有很高的mAP，但以0.5 fps的速度仍然远远达不到实时。最近的快速R-CNN用神经网络替换了选择性搜索来提出边界框，类似于Szegedy等人[8]。在我们的测试中，他们最准确的模型实现了7 fps，而一个较小、准确性较低的模型运行速度为18 fps。快速R-CNN的VGG-16版本比YOLO高出10 mAP，但也慢了6倍。Zeiler-Fergus 快速R-CNN只比YOLO慢2.5倍，但准确性也较低。表1：Pascal VOC 2007上的实时系统。比较快速探测器的性能和速度。快速YOLO是有记录以来Pascal VOC检测最快的探测器，准确性仍然是其他任何实时探测器的两倍。YOLO比快速版本更准确10 mAP，速度仍然远高于实时。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:26:16.117587",
    "updated_at": "2026-01-12T20:27:27.291600"
  },
  "yolo_ch11_ck0": {
    "book_id": "yolo",
    "chapter_id": 11,
    "chunk_id": 0,
    "source_text": "4.2. VOC 2007 Error Analysis To further examine the differences between YOLO and state-of-the-art detectors, we look at a detailed breakdown of results on VOC 2007. We compare YOLO to Fast R-CNN since Fast R-CNN is one of the highest performing detectors on PASCAL and it’s detections are publicly available. We use the methodology and tools of Hoiem et al. [19] For each category at test time we look at the top N predictions for that category. Each prediction is either correct or it is classified based on the type of error: Figure 4 shows the breakdown of each error type averaged across all 20 classes. Figure 4: Error Analysis: Fast R-CNN vs. YOLO These charts show the percentage of localization and background errors in the top N detections for various categories (N = # objects in that category). YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. $13.6\\%$ of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.",
    "translation": "4.2. VOC 2007错误分析 为了进一步检验YOLO与最先进的检测器之间的差异，我们对VOC 2007上的详细结果进行了分析。我们将YOLO与快速R-CNN进行比较，因为快速R-CNN是PASCAL上表现最好的检测器之一，其检测结果公开可用。我们采用了Hoiem等人[19]的方法和工具。在测试时，我们对每个类别的前N个预测进行了观察。每个预测要么正确，要么根据错误类型进行分类：图4显示了所有20个类别中每种错误类型的平均分布。图4：错误分析：快速R-CNN与YOLO 这些图表显示了不同类别中前N个检测结果的定位和背景错误百分比（N = 该类别中的对象数量）。YOLO在定位对象方面存在困难。定位错误占YOLO错误的比重超过了所有其他来源的总和。快速R-CNN的定位错误要少得多，但背景错误要多得多。其前检测结果中有$13.6\\%$是假阳性，即不包含任何对象。快速R-CNN预测背景检测的可能性几乎是YOLO的3倍。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:30:05.400466"
  },
  "yolo_ch12_ck1": {
    "book_id": "yolo",
    "chapter_id": 12,
    "chunk_id": 1,
    "source_text": "4.3. Combining Fast R-CNN and YOLO YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes. The best Fast R-CNN model achieves a mAP of $71.8\\%$ on the VOC 2007 test set. When combined with YOLO, its mAP increases by $3.2\\%$ to $75.0\\%$. We also tried combining the top Fast R-CNN model with several other versions of Fast R-CNN. Those ensembles produced small increases in mAP between $.3$ and $.6\\%$, see Table 2 for details. Table 2: Model combination experiments on VOC 2007. We examine the effect of combining various models with the best version of Fast R-CNN. Other versions of Fast R-CNN provide only a small benefit while YOLO provides a significant performance boost. The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN’s performance. Unfortunately, this combination doesn’t benefit from the speed of YOLO since we run each model seperately and then combine the results. However, since YOLO is so fast it doesn’t add any significant computational time compared to Fast R-CNN.",
    "translation": "4.3. 结合快速R-CNN和YOLO 相比于快速R-CNN，YOLO在背景错误上要少得多。通过使用YOLO排除快速R-CNN的背景检测结果，我们得到了显著的性能提升。对于R-CNN预测的每个边界框，我们检查YOLO是否预测了一个相似的框。如果预测了，我们就根据YOLO预测的概率和两个框之间的重叠程度给这个预测增加分数。最好的快速R-CNN模型在VOC 2007测试集上达到了71.8%的平均精度均值。当与YOLO结合时，其平均精度均值增加了3.2%，达到了75.0%。我们还尝试将顶级的快速R-CNN模型与其他几个版本的快速R-CNN结合。这些集成模型在平均精度均值上只产生了0.3%到0.6%的小幅增长，具体细节见表2。表2：VOC 2007上的模型组合实验。我们检验了将各种模型与最好的快速R-CNN版本结合的效果。其他版本的快速R-CNN只提供了微小的好处，而YOLO提供了显著的性能提升。YOLO的提升不仅仅是模型集成的副产品，因为将不同版本的快速R-CNN结合起来几乎没有好处。相反，正是因为YOLO在测试时会产生不同类型的错误，它才如此有效地提升快速R-CNN的性能。不幸的是，这种组合并没有从YOLO的速度中受益，因为我们分别运行每个模型，然后合并结果。然而，由于YOLO如此之快，与快速R-CNN相比，它并没有增加任何显著的计算时间。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:31:07.318719"
  },
  "yolo_ch13_ck0": {
    "book_id": "yolo",
    "chapter_id": 13,
    "chunk_id": 0,
    "source_text": "4.4. VOC 2012 Results On the VOC 2012 test set, YOLO scores $57.9\\%$ mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores $8-10\\%$ lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance. Table 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of November 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the only real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN. Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a $2.3\\%$ improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.",
    "translation": "4.4. VOC 2012结果 在VOC 2012测试集上，YOLO的得分为57.9%的平均精度均值。这低于当前最先进水平，更接近于使用VGG-16的原始R-CNN，详见表3。与最接近的竞争对手相比，我们的系统在处理小目标时存在困难。在瓶子、羊和电视/显示器等类别上，YOLO的得分比R-CNN或特征编辑低8-10%。然而，在猫和火车等其他类别上，YOLO实现了更高的性能。表3：PASCAL VOC 2012排行榜。YOLO与2015年11月6日的完整comp4（允许使用外部数据）公共排行榜进行了比较。展示了各种检测方法的平均精度均值和每类平均精度。YOLO是唯一的实时探测器。快速R-CNN+YOLO是第四高分方法，比快速R-CNN高出2.3%。我们结合快速R-CNN+YOLO的模型是最高性能的检测方法之一。快速R-CNN通过与YOLO的结合，提高了2.3%，使其在公共排行榜上上升了5个名次。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:32:00.770299"
  },
  "yolo_ch14_ck1": {
    "book_id": "yolo",
    "chapter_id": 14,
    "chunk_id": 1,
    "source_text": "4.5. Generalizability: Person Detection in Artwork Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before [3]. We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork. Figure 5 shows comparative performance between YOLO and other detection methods. For reference, we give VOC 2007 detection AP on person where all models are trained only on VOC 2007 data. On Picasso models are trained on VOC 2012 while on People-Art they are trained on VOC 2010. Figure 5: Generalization results on Picasso and People-Art datasets. R-CNN has high AP on VOC 2007. However, R-CNN drops off considerably when applied to artwork. R-CNN uses Selective Search for bounding box proposals which is tuned for natural images. The classifier step in R-CNN only sees small regions and needs good proposals. DPM maintains its AP well when applied to artwork. Prior work theorizes that DPM performs well because it has strong spatial models of the shape and layout of objects. Though DPM doesn’t degrade as much as R-CNN, it starts from a lower AP. YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shape of objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections. Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it does think one person is an airplane.",
    "translation": "4.5. 泛化能力：在艺术作品学术数据集中进行人物检测的目标检测训练和测试数据来源于同一分布。在现实世界的应用中，很难预测所有可能的用例，测试数据可能与系统以前见过的数据有所不同[3]。我们在毕加索数据集[12]和人物艺术数据库[3]上比较YOLO与其他检测系统的性能，这两个数据集用于测试艺术作品中的人物检测。图5显示了YOLO与其他检测方法之间的比较性能。作为参考，我们给出了在VOC 2007数据上训练的所有模型的人物检测平均精度（AP），其中所有模型仅使用VOC 2007数据进行训练。在毕加索数据集上，模型是在VOC 2012数据上训练的，而在人物艺术数据库上，它们是在VOC 2010数据上训练的。图5：在毕加索和人物艺术数据库上的泛化测试结果。R-CNN在VOC 2007上有很高的平均精度，然而，当应用于艺术作品时，R-CNN的性能显著下降。R-CNN使用选择性搜索进行边界框提议，这适用于自然图像。R-CNN的分类器步骤只看到小区域，需要好的提议。DPM在应用于艺术作品时，其平均精度保持得很好。先前的工作理论认为，DPM之所以表现良好，是因为它对物体的形状和布局有很强的空间模型。尽管DPM的退化不像R-CNN那么严重，但它的起始平均精度较低。YOLO在VOC 2007上表现良好，当应用于艺术作品时，其平均精度的下降幅度小于其他方法。像DPM一样，YOLO模拟了物体的大小和形状，以及物体之间的关系和物体通常出现的位置。艺术作品和自然图像在像素级别上非常不同，但在物体的大小和形状方面是相似的，因此YOLO仍然可以预测出良好的边界框和检测结果。图6：定性结果。YOLO在艺术作品样本和互联网上的自然图像上运行。它大部分是准确的，尽管它确实将一个人误认为是飞机。",
    "quality_score": 8.0,
    "saved_at": "2026-01-12T20:33:51.775404"
  },
  "yolo_ch15_ck0": {
    "book_id": "yolo",
    "chapter_id": 15,
    "chunk_id": 0,
    "source_text": "5. Real-Time Detection In The Wild YOLO is a fast, accurate object detector, making it ideal for computer vision applications. We connect YOLO to a webcam and verify that it maintains real-time performance, including the time to fetch images from the camera and display the detections. The resulting system is interactive and engaging. While YOLO processes images individually, when attached to a webcam it functions like a tracking system, detecting objects as they move around and change in appearance. A demo of the system and the source code can be found on our project website: http://pjreddie.com/yolo/.",
    "translation": "5. 实时检测野外YOLO是一个快速、准确的目标检测器，使其成为计算机视觉应用的理想选择。我们将YOLO连接到网络摄像头，并验证它保持实时性能，包括从摄像头获取图像和显示检测结果的时间。得到的系统是互动且引人入胜的。虽然YOLO单独处理图像，但当连接到网络摄像头时，它就像一个跟踪系统一样工作，检测移动和外观变化的物体。系统的演示和源代码可以在我们项目网站上找到：http://pjreddie.com/yolo/。",
    "quality_score": 9.0,
    "saved_at": "2026-01-12T20:34:50.981588"
  },
  "yolo_ch16_ck0": {
    "book_id": "yolo",
    "chapter_id": 16,
    "chunk_id": 0,
    "source_text": "6. Conclusion We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly. Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection. Acknowledgements: This work is partially supported by ONR N00014-13-1-0720, NSF IIS-1338054, and The Allen Distinguished Investigator Award.",
    "translation": "6. 结论 我们介绍了YOLO，一个统一的目标检测模型。我们的模型构建简单，并且可以直接在完整图像上进行训练。与基于分类器的方法不同，YOLO的训练是基于一个直接对应于检测性能的损失函数，整个模型是联合训练的。快速YOLO是文献中最快的通用目标检测器，而YOLO推动了实时目标检测的最先进技术。YOLO还能很好地泛化到新领域，使其成为依赖于快速、稳健目标检测的应用的理想选择。致谢：本工作部分由ONR N00014-13-1-0720、NSF IIS-1338054和艾伦杰出研究员奖支持。",
    "quality_score": 9.0,
    "saved_at": "2026-01-12T20:35:28.592054"
  }
}
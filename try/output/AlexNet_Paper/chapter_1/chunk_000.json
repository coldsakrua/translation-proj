{
  "chunk_id": 0,
  "source_text": "Abstract We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
  "translation": "我们训练了一个大型深度卷积神经网络，用于将ImageNet大规模视觉识别挑战赛2010中的1.2百万高分辨率图像分类到1000个不同的类别中。在测试数据上，我们实现了37.5%的top-1和17.0%的top-5错误率，这明显优于先前的最佳水平。该神经网络拥有6000万个参数和650,000个神经元，由五个卷积层组成，其中一些后接最大池化层，以及三个全连接层，最终通过一个1000类输出的softmax层。为了加快训练速度，我们使用了非饱和神经元和一种非常高效的GPU卷积操作实现。为了减少全连接层的过拟合，我们采用了一种名为“暂退法”的最近开发出的正则化方法，证明其非常有效。我们还参加了ILSVRC-2012竞赛，并以15.3%的top-5测试错误率赢得了比赛，相比之下第二名的成绩为26.2%。",
  "quality_score": 9.0,
  "glossary": [
    {
      "src": "ImageNet LSVRC-2010",
      "type": "Acronym",
      "context_meaning": "ImageNet Large Scale Visual Recognition Challenge 2010",
      "suggested_trans": "ImageNet 大规模视觉识别挑战赛 2010",
      "rationale": "ImageNet LSVRC-2010 是一个著名的计算机视觉竞赛，全称是 ImageNet Large Scale Visual Recognition Challenge 2010，简称 LSVRC。该竞赛旨在评估不同计算机视觉算法在大规模图像识别任务上的性能。在文中，作者提到他们训练了一个深度卷积神经网络，用于在 ImageNet LSVRC-2010 比赛中对 1.2 百万高分辨率图像进行分类。因此，将 ImageNet LSVRC-2010 翻译为 'ImageNet 大规模视觉识别挑战赛 2010' 更为准确和完整，能够反映其计算机视觉竞赛的本质含义。"
    },
    {
      "src": "${ILSVRC-2012}",
      "type": "Acronym",
      "context_meaning": "ILSVRC-2012 refers to the ImageNet Large Scale Visual Recognition Challenge held in 2012. It is a competition where computer vision systems are evaluated on their ability to accurately classify and detect objects and scenes from a large image dataset. In this context, it is mentioned that the variant of the model was entered in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%.",
      "suggested_trans": "ImageNet Large Scale Visual Recognition Challenge 2012",
      "rationale": "The term 'ILSVRC-2012' is an acronym for the ImageNet Large Scale Visual Recognition Challenge held in 2012. It is mentioned in the context of a competition where the variant of the model was entered and achieved a winning top-5 test error rate. To provide a clear understanding of the term, the full name of the competition is suggested as the translation. The context meaning and rationale explain the significance of ILSVRC-2012 in the context of computer vision and the specific achievement mentioned in the source text."
    },
    {
      "src": "dropout",
      "type": "n",
      "context_meaning": "A regularization technique used in neural networks to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training, which helps to prevent neurons from co-adapting too much.",
      "suggested_trans": "暂退法",
      "rationale": "Based on the retrieved translation memory, 'dropout' is consistently translated as '暂退法'. This translation accurately captures the essence of the term in the context of machine learning and neural network training, where neurons are temporarily 'dropped out' or deactivated to prevent overfitting. The consistency in the translations from the provided translation memory supports the use of '暂退法' as the suggested translation for 'dropout'."
    }
  ]
}
{
  "chunk_id": 1,
  "source_text": "1 Introduction Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small -- on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current best error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories. To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse. Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting. The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly. Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance. In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.",
  "translation": "1 引言 当前的目标识别方法主要依赖于机器学习方法。为了提高它们的性能，我们可以收集更大的数据集、学习更强大的模型，并使用更好的技术来防止过拟合。直到最近，标记图像的数据集相对较小——大约在几万张图像的规模（例如，NORB[16]、Caltech-101/256[8, 9]和CIFAR-10/100[12]）。对于简单的识别任务，使用这种规模的数据集就可以解决得很好，尤其是如果它们通过保持标签不变的转换进行增强。例如，当前最佳的错误率在MNIST数字识别任务中（<0.3%）接近人类的性能[4]。但是，在现实环境中的物体表现出相当大的变异性，因此要学习识别它们，必须使用更大的训练集。实际上，小规模图像数据集的局限性已经被广泛认识（例如，Pinto等人[21]），但直到最近才可能收集到包含数百万图像的标记数据集。新的更大数据集包括LabelMe[23]，它包含数十万张完全分割的图像，以及ImageNet[6]，它包含超过1500万个标记的高分辨率图像和超过22,000个类别。要从数百万图像中学习识别数千个物体，我们需要一个具有大学习能力的模型。然而，由于目标识别任务的复杂性巨大，即使是像ImageNet这样大的数据集也无法完全指定这个问题，所以我们的模型还应该具有大量的先验知识，以补偿我们没有的数据。卷积神经网络(CNNs)就是这样一类模型[16, 11, 13, 18, 15, 22, 26]。它们的容量可以通过改变它们的深度和宽度来控制，它们还对图像的性质做出强烈且大多是正确的假设（即统计的稳定性和像素依赖性的局部性）。因此，与具有相似大小层的标准前馈神经网络相比，CNNs的连接和参数要少得多，因此更容易训练，而它们理论上的最佳性能可能只是略差。尽管CNNs具有吸引人的特性，尽管它们的局部架构相对高效，但在大规模应用到高分辨率图像上仍然非常昂贵。幸运的是，当前的GPU搭配高度优化的二维卷积实现，足以支持有趣规模的大型CNNs的训练，而像ImageNet这样的最新数据集包含足够的标记样本来训练这样的模型，而不会发生严重的过拟合。本文的具体贡献如下：我们在ImageNet用于ILSVRC-2010和ILSVRC-2012竞赛的子集上训练了迄今为止最大的卷积神经网络之一[2]，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的GPU实现二维卷积和训练卷积神经网络固有的所有其他操作，我们公开提供。我们的网络包含一些新的和不寻常的特性，这些特性提高了它的性能并减少了它的训练时间，这些特性在第3节中详细说明。我们的网络规模使得过拟合成为一个显著问题，即使有120万个标记的训练样本，所以我们使用了几种有效的防止过拟合的技术，这些技术在第4节中描述。我们的最终网络包含五个卷积层和三个全连接层，这种深度似乎很重要：我们发现移除任何卷积层（每个层包含的模型参数不超过1%）都会导致性能下降。最终，我们网络的规模主要由当前GPU上可用的内存量和我们愿意容忍的训练时间量限制。我们的网络在两个GTX 580 3GB GPU上训练需要五到六天。我们所有的实验都表明，只要等待更快的GPU和更大的数据集变得可用，我们的结果就可以简单地得到改进。",
  "quality_score": 9.0,
  "glossary": [
    {
      "src": "object recognition",
      "type": "Terminology",
      "context_meaning": "The process by which a machine or algorithm identifies and categorizes objects within images or video frames, using machine learning methods, especially convolutional neural networks, to analyze and classify visual data.",
      "suggested_trans": "目标识别",
      "rationale": "The term 'object recognition' refers to the ability of a computer system to identify and classify objects within images or video, which is a fundamental aspect of computer vision. Given the context of the source text, which discusses machine learning methods for recognizing objects in images and the use of convolutional neural networks, the most appropriate translation is '目标识别'. This translation is chosen over '模式识别', which is more general and does not specifically convey the concept of identifying objects within images. Additionally, '目标检测' is closer to 'object detection', which is related but focuses more on the localization of objects rather than their recognition and classification. '语音识别' is not relevant as it pertains to speech recognition and is not connected to visual object identification."
    },
    {
      "src": "machine learning",
      "type": "term",
      "context_meaning": "A field of study that focuses on creating algorithms and models that allow computers to learn and improve from experience without being explicitly programmed. In the context of the source text, machine learning methods are applied to object recognition tasks, using techniques like collecting larger datasets, learning more powerful models, and employing methods to prevent overfitting.",
      "suggested_trans": "机器学习",
      "rationale": "The term 'machine learning' is translated as '机器学习' in the provided translation memory, which is the standard translation for this term in Chinese. The context meaning is derived from the source text's discussion on its application in object recognition and the various approaches to improve performance in this field."
    },
    {
      "src": "datasets",
      "type": "term",
      "context_meaning": "In the context of machine learning and object recognition, 'datasets' refers to large collections of data, typically images, used for training models to improve their accuracy and performance. Larger datasets allow for the recognition of a greater variety of objects and can help prevent overfitting.",
      "suggested_trans": "数据集",
      "rationale": "The term 'datasets' is a common term in the fields of machine learning and computer vision, and it is used to refer to collections of data used for training models. The term '数据集' is the standard translation for 'datasets' in Chinese and accurately captures the meaning of the term in the context of the source text. This translation is appropriate because it conveys the idea of a set of data used for machine learning purposes without implying any additional attributes such as dynamism or rawness, which are not present in the context of the source text."
    },
    {
      "src": "label-preserving transformations",
      "type": "Terminology",
      "context_meaning": "Transformations applied to images that maintain the original labels or class assignments, thereby creating augmented datasets that help in improving machine learning model performance without altering the underlying class information.",
      "suggested_trans": "保持标签不变的转换",
      "rationale": "The term 'label-preserving transformations' refers to a set of data augmentation techniques where images are manipulated, such as through rotation, scaling, or translation, in ways that do not change the image's label. In the context of machine learning and object recognition, these transformations are crucial for expanding the dataset without adding new images, which helps in reducing overfitting and improving the generalizability of the model. The suggested translation '保持标签不变的转换' accurately captures this meaning, indicating transformations that preserve the integrity of the label information while altering the image data."
    },
    {
      "src": "error rate",
      "type": "term",
      "context_meaning": "In the context of machine learning and object recognition, 'error rate' refers to the percentage of instances in which the model incorrectly identifies or classifies an object. It is a key performance metric indicating how well the model is able to accurately recognize objects in a given dataset.",
      "suggested_trans": "错误率",
      "rationale": "The term 'error rate' is translated as '错误率' which directly corresponds to the concept of the rate at which errors occur in a system or model. Given the context of object recognition and model performance, '错误率' accurately captures the meaning of how often the model fails to correctly identify objects, making it the appropriate translation for 'error rate'. Additionally, the translation is supported by the provided translation memory, which lists 'Error Rate' as '错误率', reinforcing the correctness of this translation."
    },
    {
      "src": "recognition tasks",
      "type": "term",
      "context_meaning": "In the context of the source text, 'recognition tasks' refers to the challenges and processes involved in identifying and classifying objects within images, particularly using machine learning methods and convolutional neural networks (CNNs). The term encompasses tasks such as digit recognition, object recognition in realistic settings, and the classification of high-resolution images into numerous categories.",
      "suggested_trans": "识别任务",
      "rationale": "The term 'recognition tasks' has been translated to '识别任务', which accurately reflects the concept of identifying and classifying objects within the given context. This translation is consistent with the technical nature of the source text and aligns with the existing translation memory patterns for related terms such as 'Pattern Recognition' (模式识别), 'Speech Recognition' (语音识别), and 'Facial Recognition' (面部识别). The context of the term within the source text is about object identification in images using advanced machine learning techniques, which is appropriately captured by the suggested translation '识别任务'."
    },
    {
      "src": "training sets",
      "type": "Terminology",
      "context_meaning": "In the context of machine learning and object recognition, 'training sets' refers to large collections of labeled data (often images) used to train models, particularly in deep learning, to recognize and classify objects effectively.",
      "suggested_trans": "训练集",
      "rationale": "The term 'training sets' is translated to '训练集' in the provided translation memory. The context within the source text indicates that these sets are crucial for the training of machine learning models, especially convolutional neural networks, to improve their performance in object recognition tasks. The term '训练集' accurately captures the meaning of a dataset used for training purposes in the Chinese language."
    },
    {
      "src": "LabelMe",
      "type": "dataset",
      "context_meaning": "LabelMe is a dataset consisting of hundreds of thousands of fully-segmented images, which is used for object recognition tasks and training machine learning models, particularly convolutional neural networks (CNNs), to improve their performance on realistic settings with objects exhibiting considerable variability.",
      "suggested_trans": "LabelMe",
      "rationale": "In the context of the source text, 'LabelMe' refers to a specific dataset known for its large size and fully-segmented images, which is crucial for training advanced machine learning models in object recognition. The term 'LabelMe' is already in a recognizable format and does not require translation. The context meaning provided explains the significance of the LabelMe dataset in the field of object recognition and its role in enhancing the performance of machine learning models like CNNs."
    },
    {
      "src": "ImageNet",
      "type": "Dataset",
      "context_meaning": "A large-scale database of over 15 million labeled high-resolution images in over 22,000 categories, used for training and evaluating machine learning models, especially in the field of computer vision and object recognition.",
      "suggested_trans": "ImageNet是一个包含超过1500万个标记的高分辨率图像和超过22,000个类别的大规模数据库，用于训练和评估机器学习模型，尤其是在计算机视觉和对象识别领域。",
      "rationale": "The term 'ImageNet' refers to a specific, widely recognized dataset within the field of machine learning, particularly in computer vision. The context provided in the source text describes ImageNet as a large dataset containing millions of labeled images across thousands of categories, which is essential for training advanced object recognition models. The suggested translation captures this meaning accurately by conveying the scale and purpose of the ImageNet dataset."
    },
    {
      "src": "Convolutional neural networks",
      "type": "Terminology",
      "context_meaning": "A class of models used for machine learning, specifically for image recognition tasks. CNNs have a structured architecture that leverages the properties of images, such as the stationarity of statistics and locality of pixel dependencies, to reduce the number of parameters and connections compared to standard feedforward neural networks. They consist of convolutional layers and fully-connected layers and are known for their ability to learn hierarchical features from visual data. Their capacity can be adjusted by varying their depth and breadth, making them suitable for handling large datasets with millions of images.",
      "suggested_trans": "卷积神经网络",
      "rationale": "The term 'Convolutional neural networks' is translated to '卷积神经网络' in Chinese, which accurately reflects the technical meaning of the original term. The context provided in the source text describes CNNs as models with a large learning capacity that make strong assumptions about the nature of images, which are essential for their application in object recognition tasks. The translation '卷积神经网络' captures the essence of these networks' convolutional operations and their application in neural network architectures."
    },
    {
      "src": "Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26].",
      "type": "Terminology",
      "context_meaning": "CNNs refer to a class of deep learning models that are particularly effective for processing and analyzing visual data, such as images and videos. They are designed to recognize patterns and features within data through the use of multiple layers of interconnected nodes, which perform convolution operations. This allows CNNs to capture the hierarchical structure of the data, making them highly suitable for tasks such as image classification, object detection, and facial recognition.",
      "suggested_trans": "Redes Neuronales Convolucionales (CNNs)",
      "rationale": "The term 'Convolutional neural networks' is translated to 'Redes Neuronales Convolucionales' in Spanish. The translation maintains the meaning of the original term and is appropriate in the context of machine learning and artificial intelligence, where CNNs are widely used for their ability to process and analyze visual data effectively. The context provided in the source text explains that CNNs have fewer connections and parameters compared to standard feedforward neural networks, making them easier to train while still maintaining high performance for object recognition tasks. This understanding supports the translation and its relevance within the field."
    },
    {
      "src": "feedforward neural networks",
      "type": "term_translation",
      "context_meaning": "A type of artificial neural network where the signal flows from the input nodes to the output nodes in a feedforward manner without any feedback loops or cycles. These networks are typically used for tasks such as classification and regression. In the context of the source text, feedforward neural networks are contrasted with convolutional neural networks (CNNs), which are more suitable for image recognition tasks due to their ability to handle the spatial hierarchy and local correlations inherent in images.",
      "suggested_trans": "前馈神经网络",
      "rationale": "The term 'feedforward neural networks' is translated as '前馈神经网络' in the context of artificial intelligence and machine learning. This translation is accurate and commonly used in Chinese literature on the subject. The context meaning provided explains the specific characteristics of feedforward neural networks and their relationship to other types of neural networks like CNNs, which is relevant to the source text discussing the evolution and improvements in neural network architectures for object recognition tasks."
    },
    {
      "src": "2D convolution",
      "type": "translation",
      "context_meaning": "In the context of the source text, '2D convolution' refers to the mathematical operation that is fundamental to the functioning of convolutional neural networks (CNNs). It describes the process of applying a filter or kernel over a two-dimensional array, typically an image, to produce a feature map that highlights specific patterns or features within that image. This operation is crucial for image processing and recognition tasks, as it allows the CNN to learn and detect spatial hierarchies of features from low-level (e.g., edges) to high-level (e.g., complex shapes) in an image.",
      "suggested_trans": "二维卷积",
      "rationale": "The term '2D convolution' is translated to '二维卷积', which accurately reflects the two-dimensional nature of the convolution operation that is being performed. This translation is consistent with the retrieved translation memory which provided translations for related terms such as 'Convolution' to '卷积'. The context of the term within the source text indicates that it is related to image processing and machine learning models, specifically CNNs, which is why the term '二维卷积' is appropriate as it conveys the same concept of a two-dimensional convolution operation used in these networks."
    },
    {
      "src": "ILSVRC-2010",
      "type": "Abbreviation",
      "context_meaning": "ILSVRC-2010 refers to the ImageNet Large Scale Visual Recognition Challenge held in 2010, which is a competition focused on advancing object recognition and image classification technology by benchmarking algorithms on a large-scale dataset.",
      "suggested_trans": "ImageNet 大规模视觉识别挑战赛 2010",
      "rationale": "The term 'ILSVRC-2010' is an abbreviation for a specific event in the field of computer vision, the ImageNet Large Scale Visual Recognition Challenge conducted in 2010. This competition is significant as it evaluates different algorithms' performance on large datasets, which is a key aspect of advancing the field of object recognition. The suggested translation 'ImageNet 大规模视觉识别挑战赛 2010' accurately captures the essence of the event, translating the key terms 'ImageNet', 'Large Scale', 'Visual Recognition', and the year '2010' into Chinese, thus providing context to readers unfamiliar with the abbreviation."
    },
    {
      "src": "ILSVRC-2012",
      "type": "proper noun",
      "context_meaning": "ILSVRC-2012 refers to the ImageNet Large Scale Visual Recognition Challenge held in 2012. It is a competition that focuses on object recognition tasks, using the ImageNet dataset, with the goal of advancing and benchmarking the performance of computer vision systems in identifying and classifying objects within images.",
      "suggested_trans": "ImageNet 大规模视觉识别挑战赛-2012",
      "rationale": "The term 'ILSVRC-2012' is an abbreviation for 'ImageNet Large Scale Visual Recognition Challenge 2012'. Given the context, it is clear that this term is used to denote a specific competition that took place in 2012, which is part of a series of challenges aimed at advancing the field of object recognition in computer vision. The context also implies that the challenge involves using large datasets, such as those from ImageNet, to train and test models for their ability to recognize and classify objects within images. Therefore, the translation provided captures the essence of the term within the given context."
    },
    {
      "src": "GTX 580 3GB GPUs",
      "type": "Term",
      "context_meaning": "Refers to high-performance graphics processing units (GPUs) from the GeForce GTX 500 series by NVIDIA, specifically the GTX 580 model, which has 3 gigabytes (GB) of dedicated video memory. These GPUs are used for accelerating computations, especially in machine learning tasks like training convolutional neural networks, by leveraging their parallel processing capabilities. In the context of the source text, they are essential for training large-scale CNNs on high-resolution images, as they can handle the memory and processing demands of such tasks.",
      "suggested_trans": "GTX 580 3GB GPU",
      "rationale": "The term 'GTX 580 3GB GPUs' refers to a specific model of GPU that is utilized in the context of the source text for training large convolutional neural networks due to their high computational power and memory capacity. The suggested translation 'GTX 580 3GB GPU' maintains the acronym format consistent with technical terminology and accurately reflects the singular form of the term, as it is often used in discussions about specifications and performance of individual GPUs."
    }
  ]
}
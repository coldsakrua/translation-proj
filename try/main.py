from utils.config_loader import ConfigLoader
from utils.logger import setup_logger
from utils.human import review_glossary
from utils.book_cut import split_epub_by_chapter
from utils.book_cut import split_chapter_into_chunks
from utils.glossary_storage import load_reviewed_glossary
from utils.memory_storage import (
    get_previous_chapter_summaries,
    save_chapter_summary,
    get_chapter_translation_memory
)
from core.state_manager import StateManager
from core.action_executor import ActionExecutor
from core.learning_engine import LearningEngine
from core.base_agent import BaseAgent
from task import TranslationTask
from pathlib import Path
import json
from datetime import datetime

class BaseAgent:
    def __init__(self, name, state_manager, executor, learner, logger, max_steps):
        self.name = name
        self.logger = logger
        self.max_steps = max_steps

    def run(self, task):
        """
        åŸæœ‰çš„é€chunkäººå·¥ä»‹å…¥é€»è¾‘ï¼ˆå·²æ³¨é‡Šï¼Œæ”¹ä¸ºchapterçº§åˆ«å®¡æŸ¥ï¼‰
        """
        # # åˆå§‹åŒ–ä»»åŠ¡å¤„ç†å™¨
        # handler = TranslationTask(self.logger)
        # # --- æ‰“å°ä»»åŠ¡èµ·å§‹è¾¹ç•Œ ---
        # task_input = task.get("input", {})
        # chapter_id = task_input.get("chapter_id", "UNKNOWN")
        # chunk_id = task_input.get("chunk_id", "UNKNOWN")
        # print("\n" + "="*60)
        # print(f"ğŸ“Œ Task: Chapter {chapter_id} - Chunk {chunk_id}")
        # print("="*60)
        # # 1. è¿è¡Œä»»åŠ¡åˆ°ä¸­æ–­ç‚¹ï¼Œè·å–å½“å‰çš„ã€çŠ¶æ€æ•°æ®å­—å…¸ã€‘
        # # æ³¨æ„ï¼šè¿™é‡Œæ”¹åä¸º state_valuesï¼Œé¿å…å’Œ thread_id é…ç½®æ··æ·†
        # state_values = handler.run(task["input"]) 
        # 
        # # 2. æå–è‡ªåŠ¨ç”Ÿæˆçš„æœ¯è¯­è¡¨
        # auto_glossary = handler.get_glossary(state_values)
        # 
        # # 3. â€”â€” äººå·¥ä¿®æ­£ â€”â€”
        # reviewed_glossary = review_glossary(auto_glossary)
        # 
        # # 4. ç»§ç»­æ‰§è¡Œå‰©ä½™æµç¨‹
        # # æ³¨æ„ï¼šå¿…é¡»åŒ¹é… resume(updated_glossary, state_dict) çš„å‚æ•°é¡ºåº
        # print(f"\nğŸš€ Resuming translation for Chunk {chunk_id}...")
        # final_result = handler.resume(reviewed_glossary, state_values)
        # quality = final_result["result"].get("quality_score", "N/A")
        # print(f"âœ… Chunk {chunk_id} Finished. Score: {quality}")
        # print("-" * 60 + "\n") 
        # return final_result
        pass
    
    def run_chunk_auto(self, task):
        """
        è‡ªåŠ¨ç¿»è¯‘å•ä¸ªchunkï¼Œä¸ä¸­æ–­ï¼ˆç”¨äºchapterçº§åˆ«å®¡æŸ¥æ¨¡å¼ï¼‰
        """
        handler = TranslationTask(self.logger)
        task_input = task.get("input", {})
        chapter_id = task_input.get("chapter_id", "UNKNOWN")
        chunk_id = task_input.get("chunk_id", "UNKNOWN")
        print(f"  ğŸ“ Translating Chunk {chunk_id}...")
        
        # å®Œæ•´æ‰§è¡Œç¿»è¯‘æµç¨‹ï¼Œä¸ä¸­æ–­
        state_values = handler.run(task["input"])
        
        quality = state_values.get("quality_score", "N/A")
        print(f"  âœ… Chunk {chunk_id} Finished. Score: {quality}")
        
        return state_values
    
# def run_book_translation(epub_path, agent):
#     chapters = split_epub_by_chapter(epub_path)
# 
#     for chapter_id, chap in enumerate(chapters):
#         chunks = split_chapter_into_chunks(chap["content"])
# 
#         for chunk_id, chunk_text in enumerate(chunks):
#             task = {
#                 "input": {
#                     "book_id": "AIMA_4th",
#                     "chapter_id": chapter_id,
#                     "chunk_id": chunk_id,
#                     "source_text": chunk_text,
#                     "thread_id": f"ch{chapter_id}_ck{chunk_id}",
#                 }
#             }
# 
#             agent.run(task)

def collect_chapter_glossaries(book_id, chapter_id, num_chunks):
    """
    æ”¶é›†æ•´ä¸ªchapteræ‰€æœ‰chunkçš„æœ¯è¯­è¡¨å’ŒåŸæ–‡
    """
    import json
    import os
    all_glossaries = []
    chapter_source_text = []  # æ”¶é›†æ‰€æœ‰åŸæ–‡ï¼Œç”¨äºæ˜¾ç¤ºä¸Šä¸‹æ–‡
    
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if os.path.exists(chunk_file):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'glossary' in data:
                    all_glossaries.extend(data['glossary'])
                if 'source_text' in data:
                    chapter_source_text.append(data['source_text'])
    
    # å»é‡ï¼šç›¸åŒsrcçš„æœ¯è¯­åªä¿ç•™ä¸€ä¸ªï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„ï¼‰
    seen_src = set()
    unique_glossaries = []
    for term in all_glossaries:
        src = term.get('src', '')
        if src and src not in seen_src:
            seen_src.add(src)
            unique_glossaries.append(term)
    
    # åˆå¹¶æ‰€æœ‰åŸæ–‡
    full_source_text = "\n\n".join(chapter_source_text)
    
    return unique_glossaries, full_source_text

def update_chunks_with_reviewed_glossary(book_id, chapter_id, num_chunks, reviewed_glossary):
    """
    å°†äººå·¥å®¡æŸ¥åçš„æœ¯è¯­è¡¨æ›´æ–°åˆ°æ‰€æœ‰chunkæ–‡ä»¶ä¸­ï¼Œå¹¶æ›´æ–°è¯‘æ–‡ä¸­çš„æœ¯è¯­ç¿»è¯‘
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_id: ç« èŠ‚ID
        num_chunks: chunkæ•°é‡
        reviewed_glossary: å®¡æŸ¥åçš„æœ¯è¯­åˆ—è¡¨
    """
    import json
    import os
    import re
    
    # åˆ›å»ºæœ¯è¯­å­—å…¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
    reviewed_dict = {term.get('src', ''): term for term in reviewed_glossary if term.get('src')}
    
    # æ‰¾å‡ºæ‰€æœ‰è¢«äººå·¥ä¿®æ”¹çš„æœ¯è¯­ï¼ˆéœ€è¦æ›´æ–°è¯‘æ–‡çš„ï¼‰
    translation_updates = {}  # {original_trans: new_trans}
    for term in reviewed_glossary:
        if term.get('human_modified', False) and 'original_suggested_trans' in term:
            original_trans = term['original_suggested_trans']
            new_trans = term.get('suggested_trans', '')
            if original_trans and new_trans and original_trans != new_trans:
                translation_updates[original_trans] = new_trans
    
    updated_count = 0
    translation_updated_count = 0
    
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if not os.path.exists(chunk_file):
            continue
        
        try:
            # è¯»å–chunkæ–‡ä»¶
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            translation_updated = False
            
            # æ›´æ–°æœ¯è¯­è¡¨
            if 'glossary' in data and isinstance(data['glossary'], list):
                updated_glossary = []
                for term in data['glossary']:
                    src = term.get('src', '')
                    if src in reviewed_dict:
                        # ä½¿ç”¨å®¡æŸ¥åçš„æœ¯è¯­ä¿¡æ¯
                        updated_term = reviewed_dict[src].copy()
                        # ä¿ç•™åŸæœ‰çš„ä¸€äº›å­—æ®µï¼ˆå¦‚æœå®¡æŸ¥åçš„æœ¯è¯­æ²¡æœ‰ï¼‰
                        for key in ['context_meaning']:
                            if key not in updated_term and key in term:
                                updated_term[key] = term[key]
                        updated_glossary.append(updated_term)
                    else:
                        # ä¿ç•™åŸæœ‰æœ¯è¯­
                        updated_glossary.append(term)
                
                data['glossary'] = updated_glossary
                
                # æ·»åŠ äººå·¥å®¡æŸ¥æ ‡è®°
                data['human_reviewed'] = True
                data['reviewed_glossary_count'] = len([t for t in updated_glossary if t.get('human_reviewed', False)])
            
            # æ›´æ–°è¯‘æ–‡ä¸­çš„æœ¯è¯­ç¿»è¯‘
            if 'translation' in data and data['translation'] and translation_updates:
                translation = data['translation']
                # æŒ‰é•¿åº¦é™åºæ’åºï¼Œä¼˜å…ˆæ›¿æ¢è¾ƒé•¿çš„æœ¯è¯­ï¼Œé¿å…çŸ­æœ¯è¯­è¢«é•¿æœ¯è¯­åŒ…å«
                sorted_updates = sorted(translation_updates.items(), key=lambda x: len(x[0]), reverse=True)
                
                for original_trans, new_trans in sorted_updates:
                    # ç›´æ¥æ›¿æ¢ï¼Œå› ä¸ºæœ¯è¯­é€šå¸¸æ˜¯å®Œæ•´çš„è¯æˆ–çŸ­è¯­
                    # å¦‚æœåŸæ–‡ä¸­å­˜åœ¨è¯¥æœ¯è¯­ï¼Œåˆ™æ›¿æ¢
                    if original_trans in translation:
                        # ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢ï¼ˆç®€å•ç›´æ¥ï¼‰
                        translation = translation.replace(original_trans, new_trans)
                        translation_updated = True
                
                data['translation'] = translation
                
                # æ·»åŠ è¯‘æ–‡æ›´æ–°æ ‡è®°
                if translation_updated:
                    data['translation_updated_by_glossary'] = True
                    data['translation_updated_at'] = datetime.now().isoformat()
            
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            with open(chunk_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            updated_count += 1
            if translation_updated:
                translation_updated_count += 1
                
                # åŒæ—¶æ›´æ–°ç¿»è¯‘è®°å¿†åº“
                try:
                    from utils.memory_storage import load_translation_memory, save_translation_memory
                    memory_key = f"{book_id}_ch{chapter_id}_ck{chunk_id}"
                    memory = load_translation_memory(book_id)
                    if memory_key in memory:
                        memory[memory_key]['translation'] = data['translation']
                        memory[memory_key]['updated_at'] = datetime.now().isoformat()
                        # ä¿å­˜æ›´æ–°åçš„è®°å¿†åº“
                        memory_file = f"output/{book_id}/translation_memory.json"
                        with open(memory_file, 'w', encoding='utf-8') as f:
                            json.dump(memory, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    print(f"  âš ï¸  æ›´æ–°ç¿»è¯‘è®°å¿†åº“å¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"  âš ï¸  æ›´æ–° chunk_{chunk_id:03d}.json å¤±è´¥: {e}")
    
    print(f"  âœ… å·²æ›´æ–° {updated_count} ä¸ªchunkæ–‡ä»¶ä¸­çš„æœ¯è¯­è¡¨")
    if translation_updated_count > 0:
        print(f"  âœ… å·²æ›´æ–° {translation_updated_count} ä¸ªchunkæ–‡ä»¶ä¸­çš„è¯‘æ–‡ï¼ˆæ ¹æ®æœ¯è¯­å®¡æŸ¥ç»“æœï¼‰")

def generate_chapter_summary(book_id, chapter_id, chunks_data):
    """
    ç”Ÿæˆç« èŠ‚æ‘˜è¦
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_id: ç« èŠ‚ID
        chunks_data: chunkæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«source_textå’Œtranslation
    """
    try:
        from core.get_llm import llm
        
        # æ”¶é›†æ‰€æœ‰åŸæ–‡å’Œè¯‘æ–‡
        source_texts = [chunk.get('source_text', '') for chunk in chunks_data]
        translations = [chunk.get('translation', '') for chunk in chunks_data]
        
        combined_source = "\n\n".join(source_texts[:5])  # åªå–å‰5ä¸ªchunk
        combined_translation = "\n\n".join(translations[:5])
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç« èŠ‚ç”Ÿæˆæ‘˜è¦å’Œå…³é”®ç‚¹ã€‚

ã€åŸæ–‡ï¼ˆå‰5ä¸ªchunkï¼‰ã€‘
{combined_source[:2000]}

ã€è¯‘æ–‡ï¼ˆå‰5ä¸ªchunkï¼‰ã€‘
{combined_translation[:2000]}

è¯·ç”Ÿæˆï¼š
1. ç« èŠ‚æ‘˜è¦ï¼ˆ100-200å­—ï¼Œä¸­æ–‡ï¼‰
2. å…³é”®ç‚¹åˆ—è¡¨ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "summary": "ç« èŠ‚æ‘˜è¦",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
}}
"""
        try:
            from pydantic import BaseModel, Field
            class ChapterSummary(BaseModel):
                summary: str = Field(description="ç« èŠ‚æ‘˜è¦")
                key_points: list = Field(description="å…³é”®ç‚¹åˆ—è¡¨")
            
            structured_llm = llm.with_structured_output(ChapterSummary)
            result = structured_llm.invoke(prompt)
            summary_data = result.model_dump()
            
            # ä¿å­˜æ‘˜è¦
            save_chapter_summary(
                book_id=book_id,
                chapter_id=chapter_id,
                summary=summary_data['summary'],
                key_points=summary_data['key_points']
            )
            print(f"  âœ… ç« èŠ‚æ‘˜è¦å·²ç”Ÿæˆå¹¶ä¿å­˜")
            return summary_data
        except Exception as e:
            print(f"  âš ï¸  ç”Ÿæˆç« èŠ‚æ‘˜è¦å¤±è´¥: {e}")
            return None
    except Exception as e:
        print(f"  âš ï¸  ç”Ÿæˆç« èŠ‚æ‘˜è¦æ—¶å‡ºé”™: {e}")
        return None


def review_chapter_translation(book_id, chapter_id, num_chunks):
    """
    äººå·¥å®¡æŸ¥ç« èŠ‚ç¿»è¯‘è´¨é‡
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_id: ç« èŠ‚ID
        num_chunks: chunkæ•°é‡
    
    Returns:
        å®¡æŸ¥ç»“æœå­—å…¸ï¼ŒåŒ…å« accepted å’Œ feedback å­—æ®µ
    """
    import json
    import os
    
    print("\n" + "="*60)
    print(f"ğŸ“– ç« èŠ‚ {chapter_id} ç¿»è¯‘è´¨é‡å®¡æŸ¥")
    print("="*60)
    
    # æ”¶é›†æ‰€æœ‰chunkçš„ç¿»è¯‘
    translations = []
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if os.path.exists(chunk_file):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                translations.append({
                    "chunk_id": chunk_id,
                    "source_text": data.get('source_text', ''),
                    "translation": data.get('translation', ''),
                    "quality_score": data.get('quality_score', 0)
                })
    
    if not translations:
        print("  âš ï¸  æœªæ‰¾åˆ°ç¿»è¯‘ç»“æœ")
        return {"accepted": False, "feedback": "æœªæ‰¾åˆ°ç¿»è¯‘ç»“æœ"}
    
    # æ˜¾ç¤ºç¿»è¯‘ç»Ÿè®¡
    scores = [t['quality_score'] for t in translations if t['quality_score']]
    avg_score = sum(scores) / len(scores) if scores else 0
    print(f"\n  ğŸ“Š ç¿»è¯‘ç»Ÿè®¡:")
    print(f"     - æ€»chunkæ•°: {len(translations)}")
    print(f"     - å¹³å‡è´¨é‡åˆ†: {avg_score:.1f}/10")
    
    # æ˜¾ç¤ºå‰3ä¸ªchunkçš„ç¿»è¯‘ç¤ºä¾‹
    print(f"\n  ğŸ“ ç¿»è¯‘ç¤ºä¾‹ï¼ˆå‰3ä¸ªchunkï¼‰:")
    for i, t in enumerate(translations[:3], 1):
        print(f"\n  [ç¤ºä¾‹ {i}] Chunk {t['chunk_id']}")
        print(f"  åŸæ–‡: {t['source_text'][:150]}...")
        print(f"  è¯‘æ–‡: {t['translation'][:150]}...")
        print(f"  è´¨é‡åˆ†: {t['quality_score']}/10")
    
    # è¯¢é—®æ˜¯å¦æ¥å—
    print(f"\n" + "-"*60)
    while True:
        action = input(
            "æ˜¯å¦æ¥å—æœ¬ç« èŠ‚ç¿»è¯‘ï¼Ÿ [y=æ¥å— | n=ä¸æ¥å— | s=è·³è¿‡] > "
        ).strip().lower()
        
        if action == "y" or action == "":
            print("  âœ… å·²æ¥å—æœ¬ç« èŠ‚ç¿»è¯‘")
            return {"accepted": True, "feedback": ""}
        elif action == "n":
            feedback = input("  ğŸ‘‰ è¯·è¾“å…¥ä¿®æ”¹æ„è§ï¼ˆå¯ç›´æ¥å›è½¦è·³è¿‡ï¼‰: ").strip()
            return {"accepted": False, "feedback": feedback or "éœ€è¦ä¿®æ”¹"}
        elif action == "s":
            print("  â­ï¸  è·³è¿‡å®¡æŸ¥")
            return {"accepted": True, "feedback": "è·³è¿‡å®¡æŸ¥"}
        else:
            print("  âš ï¸  æ— æ•ˆæ“ä½œï¼Œè¯·è¾“å…¥ y/n/s")


def load_global_glossary(book_id, current_chapter_id):
    """
    åŠ è½½å…¨å±€æœ¯è¯­è¡¨ï¼ˆä¹‹å‰ç« èŠ‚çš„å·²å®¡æŸ¥æœ¯è¯­ï¼‰
    
    Args:
        book_id: ä¹¦ç±ID
        current_chapter_id: å½“å‰ç« èŠ‚ID
    
    Returns:
        å…¨å±€æœ¯è¯­è¡¨å­—å…¸
    """
    try:
        # åŠ è½½å·²å®¡æŸ¥çš„æœ¯è¯­åº“
        reviewed_glossary = load_reviewed_glossary()
        
        # è¿‡æ»¤å‡ºå½“å‰ä¹¦ç±çš„æœ¯è¯­ï¼ˆå¦‚æœæœ‰book_idæ ‡è®°çš„è¯ï¼‰
        # æˆ–è€…ç›´æ¥è¿”å›æ‰€æœ‰å·²å®¡æŸ¥çš„æœ¯è¯­
        global_glossary = {}
        for term_src, term_info in reviewed_glossary.items():
            if isinstance(term_info, dict):
                global_glossary[term_src] = term_info
        
        if global_glossary:
            print(f"  ğŸ“š åŠ è½½äº† {len(global_glossary)} ä¸ªå…¨å±€æœ¯è¯­ï¼ˆæ¥è‡ªä¹‹å‰ç« èŠ‚ï¼‰")
        else:
            print(f"  â„¹ï¸  æš‚æ— å…¨å±€æœ¯è¯­è¡¨")
        
        return global_glossary
    except Exception as e:
        print(f"  âš ï¸  åŠ è½½å…¨å±€æœ¯è¯­è¡¨å¤±è´¥: {e}")
        return {}


def run_book_translation(json_path, agent, book_id="AlexNet_Paper"):
    """ä» JSON æ–‡ä»¶è¯»å–ç« èŠ‚å¹¶ç¿»è¯‘ï¼ˆchapterçº§åˆ«äººå·¥å®¡æŸ¥ï¼‰"""
    chapters = split_epub_by_chapter(json_path)

    for chapter_id, chap in enumerate(chapters):
        chapter_title = chap.get("title", f"Chapter {chapter_id}")
        print("\n" + "="*60)
        print(f"ğŸ“– Chapter {chapter_id}: {chapter_title}")
        print("="*60)
        
        content = chap.get("content", "")
        if not content:
            print(f"  âš ï¸  Chapter {chapter_id} is empty, skipping...")
            continue
        
        # ===== åŠ è½½å…¨å±€æœ¯è¯­è¡¨å’Œç« èŠ‚ä¸Šä¸‹æ–‡ =====
        print(f"\n  ğŸ“š Loading global context...")
        global_glossary = load_global_glossary(book_id, chapter_id)
        
        # åŠ è½½ä¹‹å‰ç« èŠ‚çš„æ‘˜è¦
        try:
            prev_summaries = get_previous_chapter_summaries(book_id, chapter_id)
            if prev_summaries:
                print(f"  ğŸ“ åŠ è½½äº† {len(prev_summaries)} ä¸ªä¹‹å‰ç« èŠ‚çš„æ‘˜è¦")
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½ç« èŠ‚æ‘˜è¦å¤±è´¥: {e}")
        
        # å¦‚æœå†…å®¹è¶…è¿‡ä¸€å®šé•¿åº¦ï¼Œä»ç„¶éœ€è¦åˆ†å‰²
        chunks = split_chapter_into_chunks(content)
        print(f"  ğŸ“Š Total chunks: {len(chunks)}")
        
        # ===== é˜¶æ®µ1: è‡ªåŠ¨ç¿»è¯‘æ‰€æœ‰chunks =====
        print(f"\n  ğŸ”„ Phase 1: Auto-translating all chunks...")
        chunk_results = []
        chunks_data = []  # ç”¨äºç”Ÿæˆæ‘˜è¦
        
        for chunk_id, chunk_text in enumerate(chunks):
            task = {
                "input": {
                    "book_id": book_id,
                    "chapter_id": chapter_id,
                    "chunk_id": chunk_id,
                    "source_text": chunk_text,
                    "thread_id": f"ch{chapter_id}_ck{chunk_id}",
                    # ä¼ é€’å…¨å±€æœ¯è¯­è¡¨
                    "global_glossary": global_glossary,
                }
            }
            result = agent.run_chunk_auto(task)
            chunk_results.append(result)
            
            # æ”¶é›†chunkæ•°æ®ç”¨äºç”Ÿæˆæ‘˜è¦
            if isinstance(result, dict):
                chunks_data.append({
                    "source_text": result.get("source_text", chunk_text),
                    "translation": result.get("combined_translation", "")
                })
        
        # ===== é˜¶æ®µ2: æ”¶é›†æ•´ä¸ªchapterçš„æœ¯è¯­è¡¨å¹¶äººå·¥å®¡æŸ¥ =====
        print(f"\n  ğŸ›‘ Phase 2: Human review for Chapter {chapter_id}...")
        chapter_glossary, chapter_source_text = collect_chapter_glossaries(book_id, chapter_id, len(chunks))
        
        if chapter_glossary:
            print(f"  ğŸ“‹ Found {len(chapter_glossary)} unique terms in this chapter")
            reviewed_glossary = review_glossary(chapter_glossary, chapter_source_text)
            print(f"  âœ… Reviewed glossary: {len(reviewed_glossary)} terms")
            
            # ===== æ›´æ–°æ‰€æœ‰chunkæ–‡ä»¶ä¸­çš„æœ¯è¯­è¡¨ =====
            print(f"\n  ğŸ’¾ Updating glossary in chunk files...")
            update_chunks_with_reviewed_glossary(book_id, chapter_id, len(chunks), reviewed_glossary)
        else:
            print(f"  â„¹ï¸  No terms found in this chapter")
            reviewed_glossary = []
        
        # ===== é˜¶æ®µ3: ç”Ÿæˆç« èŠ‚æ‘˜è¦ =====
        print(f"\n  ğŸ“ Phase 3: Generating chapter summary...")
        if chunks_data:
            generate_chapter_summary(book_id, chapter_id, chunks_data)
        
        # ===== é˜¶æ®µ4: ç« èŠ‚ç¿»è¯‘è´¨é‡äººå·¥å®¡æŸ¥ =====
        print(f"\n  ğŸ›‘ Phase 4: Chapter translation quality review...")
        chapter_review_result = review_chapter_translation(book_id, chapter_id, len(chunks))
        
        if chapter_review_result and not chapter_review_result.get('accepted', False):
            print(f"  âš ï¸  ç« èŠ‚ç¿»è¯‘æœªé€šè¿‡å®¡æŸ¥ï¼Œè¯·æ ¹æ®ä¿®æ”¹æ„è§è¿›è¡Œè°ƒæ•´")
            print(f"  ğŸ’¡ ä¿®æ”¹æ„è§: {chapter_review_result.get('feedback', 'æ— ')}")
        else:
            print(f"  âœ… ç« èŠ‚ç¿»è¯‘å·²é€šè¿‡å®¡æŸ¥")
        
        print(f"\n  âœ… Chapter {chapter_id} completed!")
        print("-" * 60 + "\n")

def main():

    config = ConfigLoader("agents/config.yml")
    config.validate()

    logger = setup_logger(
        "Agent",
        config.get("logging")["log_file"]
    )

    state = StateManager(config.get("agent")["memory_size"])
    executor = ActionExecutor(**config.get("execution"))
    learner = LearningEngine()

    agent = BaseAgent(
        name=config.get("agent")["name"],
        state_manager=state,
        executor=executor,
        learner=learner,
        logger=logger,
        max_steps=config.get("agent")["max_steps"]
    )
    
    # åŸæ¥çš„ EPUB è·¯å¾„ï¼ˆå·²æ³¨é‡Šï¼‰
    # path = "D:/hw/translation-proj/5Chapter_output/Artificial Intelligence_ A Modern Approach 4th Ed.epub"
    # run_book_translation(path, agent)
    
    # æ–°çš„ JSON æ–‡ä»¶è·¯å¾„
    json_path = "D:/hw/translation-proj/data/3_en.json"
    run_book_translation(str(json_path), agent, book_id="YOLO")

if __name__ == "__main__":
    main()

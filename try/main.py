from utils.config_loader import ConfigLoader
from utils.logger import setup_logger
from utils.human import review_glossary
from utils.book_cut import split_epub_by_chapter
from utils.book_cut import split_chapter_into_chunks
from core.state_manager import StateManager
from core.action_executor import ActionExecutor
from core.learning_engine import LearningEngine
from core.base_agent import BaseAgent
from task import TranslationTask
from pathlib import Path

class BaseAgent:
    def __init__(self, name, state_manager, executor, learner, logger, max_steps):
        self.name = name
        self.logger = logger
        self.max_steps = max_steps

    def run(self, task):
        """
        åŸæœ‰çš„é€chunkäººå·¥ä»‹å…¥é€»è¾‘ï¼ˆå·²æ³¨é‡Šï¼Œæ”¹ä¸ºchapterçº§åˆ«å®¡æŸ¥ï¼‰
        """
        # # åˆå§‹åŒ–ä»»åŠ¡å¤„ç†å™¨
        # handler = TranslationTask(self.logger)
        # # --- æ‰“å°ä»»åŠ¡èµ·å§‹è¾¹ç•Œ ---
        # task_input = task.get("input", {})
        # chapter_id = task_input.get("chapter_id", "UNKNOWN")
        # chunk_id = task_input.get("chunk_id", "UNKNOWN")
        # print("\n" + "="*60)
        # print(f"ğŸ“Œ Task: Chapter {chapter_id} - Chunk {chunk_id}")
        # print("="*60)
        # # 1. è¿è¡Œä»»åŠ¡åˆ°ä¸­æ–­ç‚¹ï¼Œè·å–å½“å‰çš„ã€çŠ¶æ€æ•°æ®å­—å…¸ã€‘
        # # æ³¨æ„ï¼šè¿™é‡Œæ”¹åä¸º state_valuesï¼Œé¿å…å’Œ thread_id é…ç½®æ··æ·†
        # state_values = handler.run(task["input"]) 
        # 
        # # 2. æå–è‡ªåŠ¨ç”Ÿæˆçš„æœ¯è¯­è¡¨
        # auto_glossary = handler.get_glossary(state_values)
        # 
        # # 3. â€”â€” äººå·¥ä¿®æ­£ â€”â€”
        # reviewed_glossary = review_glossary(auto_glossary)
        # 
        # # 4. ç»§ç»­æ‰§è¡Œå‰©ä½™æµç¨‹
        # # æ³¨æ„ï¼šå¿…é¡»åŒ¹é… resume(updated_glossary, state_dict) çš„å‚æ•°é¡ºåº
        # print(f"\nğŸš€ Resuming translation for Chunk {chunk_id}...")
        # final_result = handler.resume(reviewed_glossary, state_values)
        # quality = final_result["result"].get("quality_score", "N/A")
        # print(f"âœ… Chunk {chunk_id} Finished. Score: {quality}")
        # print("-" * 60 + "\n") 
        # return final_result
        pass
    
    def run_chunk_auto(self, task):
        """
        è‡ªåŠ¨ç¿»è¯‘å•ä¸ªchunkï¼Œä¸ä¸­æ–­ï¼ˆç”¨äºchapterçº§åˆ«å®¡æŸ¥æ¨¡å¼ï¼‰
        """
        handler = TranslationTask(self.logger)
        task_input = task.get("input", {})
        chapter_id = task_input.get("chapter_id", "UNKNOWN")
        chunk_id = task_input.get("chunk_id", "UNKNOWN")
        print(f"  ğŸ“ Translating Chunk {chunk_id}...")
        
        # å®Œæ•´æ‰§è¡Œç¿»è¯‘æµç¨‹ï¼Œä¸ä¸­æ–­
        state_values = handler.run(task["input"])
        
        quality = state_values.get("quality_score", "N/A")
        print(f"  âœ… Chunk {chunk_id} Finished. Score: {quality}")
        
        return state_values
    
# def run_book_translation(epub_path, agent):
#     chapters = split_epub_by_chapter(epub_path)
# 
#     for chapter_id, chap in enumerate(chapters):
#         chunks = split_chapter_into_chunks(chap["content"])
# 
#         for chunk_id, chunk_text in enumerate(chunks):
#             task = {
#                 "input": {
#                     "book_id": "AIMA_4th",
#                     "chapter_id": chapter_id,
#                     "chunk_id": chunk_id,
#                     "source_text": chunk_text,
#                     "thread_id": f"ch{chapter_id}_ck{chunk_id}",
#                 }
#             }
# 
#             agent.run(task)

def collect_chapter_glossaries(book_id, chapter_id, num_chunks):
    """
    æ”¶é›†æ•´ä¸ªchapteræ‰€æœ‰chunkçš„æœ¯è¯­è¡¨
    """
    import json
    import os
    all_glossaries = []
    
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if os.path.exists(chunk_file):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'glossary' in data:
                    all_glossaries.extend(data['glossary'])
    
    # å»é‡ï¼šç›¸åŒsrcçš„æœ¯è¯­åªä¿ç•™ä¸€ä¸ªï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„ï¼‰
    seen_src = set()
    unique_glossaries = []
    for term in all_glossaries:
        src = term.get('src', '')
        if src and src not in seen_src:
            seen_src.add(src)
            unique_glossaries.append(term)
    
    return unique_glossaries

def update_chunks_with_reviewed_glossary(book_id, chapter_id, num_chunks, reviewed_glossary):
    """
    å°†äººå·¥å®¡æŸ¥åçš„æœ¯è¯­è¡¨æ›´æ–°åˆ°æ‰€æœ‰chunkæ–‡ä»¶ä¸­
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_id: ç« èŠ‚ID
        num_chunks: chunkæ•°é‡
        reviewed_glossary: å®¡æŸ¥åçš„æœ¯è¯­åˆ—è¡¨
    """
    import json
    import os
    
    # åˆ›å»ºæœ¯è¯­å­—å…¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
    reviewed_dict = {term.get('src', ''): term for term in reviewed_glossary if term.get('src')}
    
    updated_count = 0
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if not os.path.exists(chunk_file):
            continue
        
        try:
            # è¯»å–chunkæ–‡ä»¶
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ›´æ–°æœ¯è¯­è¡¨
            if 'glossary' in data and isinstance(data['glossary'], list):
                updated_glossary = []
                for term in data['glossary']:
                    src = term.get('src', '')
                    if src in reviewed_dict:
                        # ä½¿ç”¨å®¡æŸ¥åçš„æœ¯è¯­ä¿¡æ¯
                        updated_term = reviewed_dict[src].copy()
                        # ä¿ç•™åŸæœ‰çš„ä¸€äº›å­—æ®µï¼ˆå¦‚æœå®¡æŸ¥åçš„æœ¯è¯­æ²¡æœ‰ï¼‰
                        for key in ['context_meaning']:
                            if key not in updated_term and key in term:
                                updated_term[key] = term[key]
                        updated_glossary.append(updated_term)
                    else:
                        # ä¿ç•™åŸæœ‰æœ¯è¯­
                        updated_glossary.append(term)
                
                data['glossary'] = updated_glossary
                
                # æ·»åŠ äººå·¥å®¡æŸ¥æ ‡è®°
                data['human_reviewed'] = True
                data['reviewed_glossary_count'] = len([t for t in updated_glossary if t.get('human_reviewed', False)])
                
                # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                updated_count += 1
        except Exception as e:
            print(f"  âš ï¸  æ›´æ–° chunk_{chunk_id:03d}.json å¤±è´¥: {e}")
    
    print(f"  âœ… å·²æ›´æ–° {updated_count} ä¸ªchunkæ–‡ä»¶ä¸­çš„æœ¯è¯­è¡¨")

def run_book_translation(json_path, agent, book_id="AlexNet_Paper"):
    """ä» JSON æ–‡ä»¶è¯»å–ç« èŠ‚å¹¶ç¿»è¯‘ï¼ˆchapterçº§åˆ«äººå·¥å®¡æŸ¥ï¼‰"""
    chapters = split_epub_by_chapter(json_path)

    for chapter_id, chap in enumerate(chapters):
        chapter_title = chap.get("title", f"Chapter {chapter_id}")
        print("\n" + "="*60)
        print(f"ğŸ“– Chapter {chapter_id}: {chapter_title}")
        print("="*60)
        
        content = chap.get("content", "")
        if not content:
            print(f"  âš ï¸  Chapter {chapter_id} is empty, skipping...")
            continue
        
        # å¦‚æœå†…å®¹è¶…è¿‡ä¸€å®šé•¿åº¦ï¼Œä»ç„¶éœ€è¦åˆ†å‰²
        chunks = split_chapter_into_chunks(content)
        print(f"  ğŸ“Š Total chunks: {len(chunks)}")
        
        # ===== é˜¶æ®µ1: è‡ªåŠ¨ç¿»è¯‘æ‰€æœ‰chunks =====
        print(f"\n  ğŸ”„ Phase 1: Auto-translating all chunks...")
        chunk_results = []
        for chunk_id, chunk_text in enumerate(chunks):
            task = {
                "input": {
                    "book_id": book_id,
                    "chapter_id": chapter_id,
                    "chunk_id": chunk_id,
                    "source_text": chunk_text,
                    "thread_id": f"ch{chapter_id}_ck{chunk_id}",
                }
            }
            result = agent.run_chunk_auto(task)
            chunk_results.append(result)
        
        # ===== é˜¶æ®µ2: æ”¶é›†æ•´ä¸ªchapterçš„æœ¯è¯­è¡¨å¹¶äººå·¥å®¡æŸ¥ =====
        print(f"\n  ğŸ›‘ Phase 2: Human review for Chapter {chapter_id}...")
        chapter_glossary = collect_chapter_glossaries(book_id, chapter_id, len(chunks))
        
        if chapter_glossary:
            print(f"  ğŸ“‹ Found {len(chapter_glossary)} unique terms in this chapter")
            reviewed_glossary = review_glossary(chapter_glossary)
            print(f"  âœ… Reviewed glossary: {len(reviewed_glossary)} terms")
            
            # ===== æ›´æ–°æ‰€æœ‰chunkæ–‡ä»¶ä¸­çš„æœ¯è¯­è¡¨ =====
            print(f"\n  ğŸ’¾ Updating glossary in chunk files...")
            update_chunks_with_reviewed_glossary(book_id, chapter_id, len(chunks), reviewed_glossary)
        else:
            print(f"  â„¹ï¸  No terms found in this chapter")
            reviewed_glossary = []
        
        print(f"\n  âœ… Chapter {chapter_id} completed!")
        print("-" * 60 + "\n")

def main():

    config = ConfigLoader("agents/config.yml")
    config.validate()

    logger = setup_logger(
        "Agent",
        config.get("logging")["log_file"]
    )

    state = StateManager(config.get("agent")["memory_size"])
    executor = ActionExecutor(**config.get("execution"))
    learner = LearningEngine()

    agent = BaseAgent(
        name=config.get("agent")["name"],
        state_manager=state,
        executor=executor,
        learner=learner,
        logger=logger,
        max_steps=config.get("agent")["max_steps"]
    )
    
    # åŸæ¥çš„ EPUB è·¯å¾„ï¼ˆå·²æ³¨é‡Šï¼‰
    # path = "D:/hw/translation-proj/5Chapter_output/Artificial Intelligence_ A Modern Approach 4th Ed.epub"
    # run_book_translation(path, agent)
    
    # æ–°çš„ JSON æ–‡ä»¶è·¯å¾„
    json_path = "D:/hw/translation-proj/data/3_en.json"
    run_book_translation(str(json_path), agent, book_id="YOLO")

if __name__ == "__main__":
    main()

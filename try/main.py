from utils.config_loader import ConfigLoader
from utils.logger import setup_logger
from utils.human import review_glossary
from utils.book_cut import split_epub_by_chapter
from utils.book_cut import split_chapter_into_chunks
from utils.glossary_storage import load_reviewed_glossary
from utils.memory_storage import (
    get_previous_chapter_summaries,
    save_chapter_summary,
    get_chapter_translation_memory
)
from core.state_manager import StateManager
from core.action_executor import ActionExecutor
from core.learning_engine import LearningEngine
from core.base_agent import BaseAgent
from task import TranslationTask
from pathlib import Path
import json
import argparse
from datetime import datetime

class BaseAgent:
    def __init__(self, name, state_manager, executor, learner, logger, max_steps):
        self.name = name
        self.logger = logger
        self.max_steps = max_steps

    def run(self, task):
        """
        åŸæœ‰çš„é€chunkäººå·¥ä»‹å…¥é€»è¾‘ï¼ˆå·²æ³¨é‡Šï¼Œæ”¹ä¸ºchapterçº§åˆ«å®¡æŸ¥ï¼‰
        """
        # # åˆå§‹åŒ–ä»»åŠ¡å¤„ç†å™¨
        # handler = TranslationTask(self.logger)
        # # --- æ‰“å°ä»»åŠ¡èµ·å§‹è¾¹ç•Œ ---
        # task_input = task.get("input", {})
        # chapter_id = task_input.get("chapter_id", "UNKNOWN")
        # chunk_id = task_input.get("chunk_id", "UNKNOWN")
        # print("\n" + "="*60)
        # print(f"ğŸ“Œ Task: Chapter {chapter_id} - Chunk {chunk_id}")
        # print("="*60)
        # # 1. è¿è¡Œä»»åŠ¡åˆ°ä¸­æ–­ç‚¹ï¼Œè·å–å½“å‰çš„ã€çŠ¶æ€æ•°æ®å­—å…¸ã€‘
        # # æ³¨æ„ï¼šè¿™é‡Œæ”¹åä¸º state_valuesï¼Œé¿å…å’Œ thread_id é…ç½®æ··æ·†
        # state_values = handler.run(task["input"]) 
        # 
        # # 2. æå–è‡ªåŠ¨ç”Ÿæˆçš„æœ¯è¯­è¡¨
        # auto_glossary = handler.get_glossary(state_values)
        # 
        # # 3. â€”â€” äººå·¥ä¿®æ­£ â€”â€”
        # reviewed_glossary = review_glossary(auto_glossary)
        # 
        # # 4. ç»§ç»­æ‰§è¡Œå‰©ä½™æµç¨‹
        # # æ³¨æ„ï¼šå¿…é¡»åŒ¹é… resume(updated_glossary, state_dict) çš„å‚æ•°é¡ºåº
        # print(f"\nResuming translation for Chunk {chunk_id}...")
        # final_result = handler.resume(reviewed_glossary, state_values)
        # quality = final_result["result"].get("quality_score", "N/A")
        # print(f"âˆš Chunk {chunk_id} Finished. Score: {quality}")
        # print("-" * 60 + "\n") 
        # return final_result
        pass
    
    def run_chunk_auto(self, task):
        """
        è‡ªåŠ¨ç¿»è¯‘å•ä¸ªchunkï¼Œä¸ä¸­æ–­ï¼ˆç”¨äºchapterçº§åˆ«å®¡æŸ¥æ¨¡å¼ï¼‰
        """
        handler = TranslationTask(self.logger)
        task_input = task.get("input", {})
        chapter_id = task_input.get("chapter_id", "UNKNOWN")
        chunk_id = task_input.get("chunk_id", "UNKNOWN")
        print(f"  Translating Chunk {chunk_id}...")
        
        # å®Œæ•´æ‰§è¡Œç¿»è¯‘æµç¨‹ï¼Œä¸ä¸­æ–­
        state_values = handler.run(task["input"])
        
        quality = state_values.get("quality_score", "N/A")
        print(f"  âˆš Chunk {chunk_id} Finished. Score: {quality}")
        
        return state_values
    
# def run_book_translation(epub_path, agent):
#     chapters = split_epub_by_chapter(epub_path)
# 
#     for chapter_id, chap in enumerate(chapters):
#         chunks = split_chapter_into_chunks(chap["content"])
# 
#         for chunk_id, chunk_text in enumerate(chunks):
#             task = {
#                 "input": {
#                     "book_id": "AIMA_4th",
#                     "chapter_id": chapter_id,
#                     "chunk_id": chunk_id,
#                     "source_text": chunk_text,
#                     "thread_id": f"ch{chapter_id}_ck{chunk_id}",
#                 }
#             }
# 
#             agent.run(task)

def collect_chapter_glossaries(book_id, chapter_id, num_chunks):
    """
    æ”¶é›†æ•´ä¸ªchapteræ‰€æœ‰chunkçš„æœ¯è¯­è¡¨å’ŒåŸæ–‡
    """
    import json
    import os
    all_glossaries = []
    chapter_source_text = []  # æ”¶é›†æ‰€æœ‰åŸæ–‡ï¼Œç”¨äºæ˜¾ç¤ºä¸Šä¸‹æ–‡
    
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if os.path.exists(chunk_file):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'glossary' in data:
                    all_glossaries.extend(data['glossary'])
                if 'source_text' in data:
                    chapter_source_text.append(data['source_text'])
    
    # å»é‡ï¼šç›¸åŒsrcçš„æœ¯è¯­åªä¿ç•™ä¸€ä¸ªï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„ï¼‰
    seen_src = set()
    unique_glossaries = []
    for term in all_glossaries:
        src = term.get('src', '')
        if src and src not in seen_src:
            seen_src.add(src)
            unique_glossaries.append(term)
    
    # åˆå¹¶æ‰€æœ‰åŸæ–‡
    full_source_text = "\n\n".join(chapter_source_text)
    
    return unique_glossaries, full_source_text

def update_chunks_with_reviewed_glossary(book_id, chapter_id, num_chunks, reviewed_glossary):
    """
    å°†äººå·¥å®¡æŸ¥åçš„æœ¯è¯­è¡¨æ›´æ–°åˆ°æ‰€æœ‰chunkæ–‡ä»¶ä¸­ï¼Œå¹¶æ›´æ–°è¯‘æ–‡ä¸­çš„æœ¯è¯­ç¿»è¯‘
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_id: ç« èŠ‚ID
        num_chunks: chunkæ•°é‡
        reviewed_glossary: å®¡æŸ¥åçš„æœ¯è¯­åˆ—è¡¨
    """
    import json
    import os
    import re
    
    # åˆ›å»ºæœ¯è¯­å­—å…¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
    reviewed_dict = {term.get('src', ''): term for term in reviewed_glossary if term.get('src')}
    
    # æ‰¾å‡ºæ‰€æœ‰è¢«äººå·¥ä¿®æ”¹çš„æœ¯è¯­ï¼ˆéœ€è¦æ›´æ–°è¯‘æ–‡çš„ï¼‰
    translation_updates = {}  # {original_trans: new_trans}
    for term in reviewed_glossary:
        if term.get('human_modified', False) and 'original_suggested_trans' in term:
            original_trans = term['original_suggested_trans']
            new_trans = term.get('suggested_trans', '')
            if original_trans and new_trans and original_trans != new_trans:
                translation_updates[original_trans] = new_trans
    
    updated_count = 0
    translation_updated_count = 0
    
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if not os.path.exists(chunk_file):
            continue
        
        try:
            # è¯»å–chunkæ–‡ä»¶
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            translation_updated = False
            
            # æ›´æ–°æœ¯è¯­è¡¨
            if 'glossary' in data and isinstance(data['glossary'], list):
                updated_glossary = []
                for term in data['glossary']:
                    src = term.get('src', '')
                    if src in reviewed_dict:
                        # ä½¿ç”¨å®¡æŸ¥åçš„æœ¯è¯­ä¿¡æ¯
                        updated_term = reviewed_dict[src].copy()
                        # ä¿ç•™åŸæœ‰çš„ä¸€äº›å­—æ®µï¼ˆå¦‚æœå®¡æŸ¥åçš„æœ¯è¯­æ²¡æœ‰ï¼‰
                        for key in ['context_meaning']:
                            if key not in updated_term and key in term:
                                updated_term[key] = term[key]
                        updated_glossary.append(updated_term)
                    else:
                        # ä¿ç•™åŸæœ‰æœ¯è¯­
                        updated_glossary.append(term)
                
                data['glossary'] = updated_glossary
                
                # æ·»åŠ äººå·¥å®¡æŸ¥æ ‡è®°
                data['human_reviewed'] = True
                data['reviewed_glossary_count'] = len([t for t in updated_glossary if t.get('human_reviewed', False)])
                
            # æ›´æ–°è¯‘æ–‡ä¸­çš„æœ¯è¯­ç¿»è¯‘
            if 'translation' in data and data['translation'] and translation_updates:
                translation = data['translation']
                # æŒ‰é•¿åº¦é™åºæ’åºï¼Œä¼˜å…ˆæ›¿æ¢è¾ƒé•¿çš„æœ¯è¯­ï¼Œé¿å…çŸ­æœ¯è¯­è¢«é•¿æœ¯è¯­åŒ…å«
                sorted_updates = sorted(translation_updates.items(), key=lambda x: len(x[0]), reverse=True)
                
                for original_trans, new_trans in sorted_updates:
                    # ç›´æ¥æ›¿æ¢ï¼Œå› ä¸ºæœ¯è¯­é€šå¸¸æ˜¯å®Œæ•´çš„è¯æˆ–çŸ­è¯­
                    # å¦‚æœåŸæ–‡ä¸­å­˜åœ¨è¯¥æœ¯è¯­ï¼Œåˆ™æ›¿æ¢
                    if original_trans in translation:
                        # ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢ï¼ˆç®€å•ç›´æ¥ï¼‰
                        translation = translation.replace(original_trans, new_trans)
                        translation_updated = True
                
                data['translation'] = translation
                
                # æ·»åŠ è¯‘æ–‡æ›´æ–°æ ‡è®°
                if translation_updated:
                    data['translation_updated_by_glossary'] = True
                    data['translation_updated_at'] = datetime.now().isoformat()
            
                # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                updated_count += 1
            if translation_updated:
                translation_updated_count += 1
                
                # åŒæ—¶æ›´æ–°ç¿»è¯‘è®°å¿†åº“
                try:
                    from utils.memory_storage import load_translation_memory, save_translation_memory
                    memory_key = f"{book_id}_ch{chapter_id}_ck{chunk_id}"
                    memory = load_translation_memory(book_id)
                    if memory_key in memory:
                        memory[memory_key]['translation'] = data['translation']
                        memory[memory_key]['updated_at'] = datetime.now().isoformat()
                        # ä¿å­˜æ›´æ–°åçš„è®°å¿†åº“
                        memory_file = f"output/{book_id}/translation_memory.json"
                        with open(memory_file, 'w', encoding='utf-8') as f:
                            json.dump(memory, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    print(f"  [WARNING] æ›´æ–°ç¿»è¯‘è®°å¿†åº“å¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"  [WARNING] æ›´æ–° chunk_{chunk_id:03d}.json å¤±è´¥: {e}")
    
    print(f"  âˆš å·²æ›´æ–° {updated_count} ä¸ªchunkæ–‡ä»¶ä¸­çš„æœ¯è¯­è¡¨")
    if translation_updated_count > 0:
        print(f"  âˆš å·²æ›´æ–° {translation_updated_count} ä¸ªchunkæ–‡ä»¶ä¸­çš„è¯‘æ–‡ï¼ˆæ ¹æ®æœ¯è¯­å®¡æŸ¥ç»“æœï¼‰")

def generate_chapter_summary(book_id, chapter_id, chunks_data, enable_human_review=True):
    """
    ç”Ÿæˆç« èŠ‚æ‘˜è¦
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_id: ç« èŠ‚ID
        chunks_data: chunkæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«source_textå’Œtranslation
        enable_human_review: æ˜¯å¦å¯ç”¨äººå·¥å®¡æŸ¥æ¨¡å¼ï¼ˆç”¨äºæ§åˆ¶é€Ÿç‡é™åˆ¶ï¼‰
    """
    try:
        from core.get_llm import llm
        from core.nodes import _rate_limiter
        
        # æ”¶é›†æ‰€æœ‰åŸæ–‡å’Œè¯‘æ–‡
        source_texts = [chunk.get('source_text', '') for chunk in chunks_data]
        translations = [chunk.get('translation', '') for chunk in chunks_data]
        
        combined_source = "\n\n".join(source_texts[:5])  # åªå–å‰5ä¸ªchunk
        combined_translation = "\n\n".join(translations[:5])
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç« èŠ‚ç”Ÿæˆæ‘˜è¦å’Œå…³é”®ç‚¹ã€‚

ã€åŸæ–‡ï¼ˆå‰5ä¸ªchunkï¼‰ã€‘
{combined_source[:2000]}

ã€è¯‘æ–‡ï¼ˆå‰5ä¸ªchunkï¼‰ã€‘
{combined_translation[:2000]}

è¯·ç”Ÿæˆï¼š
1. ç« èŠ‚æ‘˜è¦ï¼ˆ100-200å­—ï¼Œä¸­æ–‡ï¼‰
2. å…³é”®ç‚¹åˆ—è¡¨ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "summary": "ç« èŠ‚æ‘˜è¦",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
}}
"""
        try:
            from pydantic import BaseModel, Field
            class ChapterSummary(BaseModel):
                summary: str = Field(description="ç« èŠ‚æ‘˜è¦")
                key_points: list = Field(description="å…³é”®ç‚¹åˆ—è¡¨")
            
            # é€Ÿç‡é™åˆ¶æ£€æŸ¥ï¼ˆå¦‚æœç¦ç”¨äº†äººå·¥å®¡æŸ¥ï¼‰
            _rate_limiter.wait_if_needed(enable_human_review)
            structured_llm = llm.with_structured_output(ChapterSummary)
            result = structured_llm.invoke(prompt)
            summary_data = result.model_dump()
            
            # ä¿å­˜æ‘˜è¦
            save_chapter_summary(
                book_id=book_id,
                chapter_id=chapter_id,
                summary=summary_data['summary'],
                key_points=summary_data['key_points']
            )
            print(f"  âˆš ç« èŠ‚æ‘˜è¦å·²ç”Ÿæˆå¹¶ä¿å­˜")
            return summary_data
        except Exception as e:
            print(f"  [WARNING] ç”Ÿæˆç« èŠ‚æ‘˜è¦å¤±è´¥: {e}")
            return None
    except Exception as e:
        print(f"  [WARNING] ç”Ÿæˆç« èŠ‚æ‘˜è¦æ—¶å‡ºé”™: {e}")
        return None


def review_chapter_translation(book_id, chapter_id, num_chunks):
    """
    äººå·¥å®¡æŸ¥ç« èŠ‚ç¿»è¯‘è´¨é‡
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_id: ç« èŠ‚ID
        num_chunks: chunkæ•°é‡
    
    Returns:
        å®¡æŸ¥ç»“æœå­—å…¸ï¼ŒåŒ…å« accepted å’Œ feedback å­—æ®µ
    """
    import json
    import os
    
    print("\n" + "="*60)
    print(f"ç« èŠ‚ {chapter_id} ç¿»è¯‘è´¨é‡å®¡æŸ¥")
    print("="*60)
    
    # æ”¶é›†æ‰€æœ‰chunkçš„ç¿»è¯‘ï¼ˆè¿‡æ»¤æ‰source_textä¸ºç©ºçš„ï¼‰
    translations = []
    for chunk_id in range(num_chunks):
        chunk_file = f"output/{book_id}/chapter_{chapter_id}/chunk_{chunk_id:03d}.json"
        if os.path.exists(chunk_file):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                source_text = data.get('source_text', '').strip()
                # åªæ·»åŠ source_textä¸ä¸ºç©ºçš„chunk
                if source_text:
                    translations.append({
                        "chunk_id": chunk_id,
                        "source_text": source_text,
                        "translation": data.get('translation', ''),
                        "quality_score": data.get('quality_score', 0)
                    })
    
    if not translations:
        print("  [WARNING] æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç¿»è¯‘ç»“æœï¼ˆæ‰€æœ‰chunkçš„source_textéƒ½ä¸ºç©ºï¼‰")
        return {"accepted": False, "feedback": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç¿»è¯‘ç»“æœ"}
    
    # æ˜¾ç¤ºç¿»è¯‘ç»Ÿè®¡ï¼ˆåªç»Ÿè®¡æœ‰æ•ˆçš„translationsï¼‰
    scores = [t['quality_score'] for t in translations if t.get('quality_score')]
    avg_score = sum(scores) / len(scores) if scores else 0
    print(f"\n  ç¿»è¯‘ç»Ÿè®¡ï¼ˆå·²æ’é™¤ç©ºæ–‡æœ¬ï¼‰:")
    print(f"     - æœ‰æ•ˆchunkæ•°: {len(translations)}")
    print(f"     - å¹³å‡è´¨é‡åˆ†: {avg_score:.1f}/10")
    
    # ä¿å­˜è´¨é‡è¯„åˆ†åˆ°å•ç‹¬æ–‡ä»¶
    quality_scores_file = f"output/{book_id}/chapter_{chapter_id}/quality_scores.json"
    try:
        os.makedirs(os.path.dirname(quality_scores_file), exist_ok=True)
        quality_data = {
            "book_id": book_id,
            "chapter_id": chapter_id,
            "reviewed_at": datetime.now().isoformat(),
            "statistics": {
                "total_chunks": len(translations),
                "average_score": round(avg_score, 2),
                "min_score": round(min(scores), 2) if scores else None,
                "max_score": round(max(scores), 2) if scores else None,
                "scores_count": len(scores),
                "note": "å·²æ’é™¤source_textä¸ºç©ºçš„chunk"
            },
            "chunk_scores": [
                {
                    "chunk_id": t['chunk_id'],
                    "quality_score": t['quality_score'],
                    "source_preview": t['source_text'][:100] + "..." if len(t['source_text']) > 100 else t['source_text'],
                    "translation_preview": t['translation'][:100] + "..." if len(t['translation']) > 100 else t['translation']
                }
                for t in translations
            ]
        }
        with open(quality_scores_file, 'w', encoding='utf-8') as f:
            json.dump(quality_data, f, ensure_ascii=False, indent=2)
        print(f"     - è´¨é‡è¯„åˆ†å·²ä¿å­˜åˆ°: {quality_scores_file}")
    except Exception as e:
        print(f"     [WARNING] ä¿å­˜è´¨é‡è¯„åˆ†å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºå‰3ä¸ªchunkçš„ç¿»è¯‘ç¤ºä¾‹
    print(f"\n  ç¿»è¯‘ç¤ºä¾‹ï¼ˆå‰3ä¸ªchunkï¼‰:")
    for i, t in enumerate(translations[:3], 1):
        print(f"\n  [ç¤ºä¾‹ {i}] Chunk {t['chunk_id']}")
        print(f"  åŸæ–‡: {t['source_text'][:150]}...")
        print(f"  è¯‘æ–‡: {t['translation'][:150]}...")
        print(f"  è´¨é‡åˆ†: {t['quality_score']}/10")
    
    # è¯¢é—®æ˜¯å¦æ¥å—ï¼ˆå¸¦è¶…æ—¶ï¼‰
    print(f"\n" + "-"*60)
    from utils.input_with_timeout import input_with_timeout
    
    # ç¬¬ä¸€æ¬¡è¯¢é—®ï¼ˆ3åˆ†é’Ÿè¶…æ—¶ï¼Œé»˜è®¤æ¥å—ï¼‰
    action = input_with_timeout(
        "æ˜¯å¦æ¥å—æœ¬ç« èŠ‚ç¿»è¯‘ï¼Ÿ [y=æ¥å— | n=ä¸æ¥å— | s=è·³è¿‡] (3åˆ†é’Ÿè¶…æ—¶è‡ªåŠ¨æ¥å—) > ",
        timeout=180.0,  # 3åˆ†é’Ÿ = 180ç§’
        default="y"
        ).strip().lower()
        
    if action == "y" or action == "":
        print("  âˆš å·²æ¥å—æœ¬ç« èŠ‚ç¿»è¯‘")
        return {"accepted": True, "feedback": ""}
    elif action == "n":
        # å¦‚æœä¸æ¥å—ï¼Œè¯¢é—®ä¿®æ”¹æ„è§ï¼ˆ3åˆ†é’Ÿè¶…æ—¶ï¼‰
        feedback = input_with_timeout(
            "  è¯·è¾“å…¥ä¿®æ”¹æ„è§ï¼ˆå¯ç›´æ¥å›è½¦è·³è¿‡ï¼Œ3åˆ†é’Ÿè¶…æ—¶è‡ªåŠ¨è·³è¿‡ï¼‰: ",
            timeout=180.0,  # 3åˆ†é’Ÿ = 180ç§’
            default=""
        ).strip()
        return {"accepted": False, "feedback": feedback or "éœ€è¦ä¿®æ”¹"}
    elif action == "s":
        print("  è·³è¿‡å®¡æŸ¥")
        return {"accepted": True, "feedback": "è·³è¿‡å®¡æŸ¥"}
    else:
        print("  [WARNING] æ— æ•ˆæ“ä½œï¼Œé»˜è®¤æ¥å—")
        return {"accepted": True, "feedback": ""}


def load_global_glossary(book_id, current_chapter_id):
    """
    åŠ è½½å…¨å±€æœ¯è¯­è¡¨ï¼ˆä¹‹å‰ç« èŠ‚çš„å·²å®¡æŸ¥æœ¯è¯­ï¼‰
    
    Args:
        book_id: ä¹¦ç±ID
        current_chapter_id: å½“å‰ç« èŠ‚ID
    
    Returns:
        å…¨å±€æœ¯è¯­è¡¨å­—å…¸
    """
    try:
        # åŠ è½½å·²å®¡æŸ¥çš„æœ¯è¯­åº“
        reviewed_glossary = load_reviewed_glossary()
        
        # è¿‡æ»¤å‡ºå½“å‰ä¹¦ç±çš„æœ¯è¯­ï¼ˆå¦‚æœæœ‰book_idæ ‡è®°çš„è¯ï¼‰
        # æˆ–è€…ç›´æ¥è¿”å›æ‰€æœ‰å·²å®¡æŸ¥çš„æœ¯è¯­
        global_glossary = {}
        for term_src, term_info in reviewed_glossary.items():
            if isinstance(term_info, dict):
                global_glossary[term_src] = term_info
        
        if global_glossary:
            print(f"  åŠ è½½äº† {len(global_glossary)} ä¸ªå…¨å±€æœ¯è¯­ï¼ˆæ¥è‡ªä¹‹å‰ç« èŠ‚ï¼‰")
        else:
            print(f"  æš‚æ— å…¨å±€æœ¯è¯­è¡¨")
        
        return global_glossary
    except Exception as e:
        print(f"  [WARNING] åŠ è½½å…¨å±€æœ¯è¯­è¡¨å¤±è´¥: {e}")
        return {}


def run_book_translation(json_path, agent, book_id="AlexNet_Paper", enable_human_review=True, use_rag=True):
    """
    ä» JSON æ–‡ä»¶è¯»å–ç« èŠ‚å¹¶ç¿»è¯‘ï¼ˆchapterçº§åˆ«äººå·¥å®¡æŸ¥ï¼‰
    
    Args:
        json_path: JSONæ–‡ä»¶è·¯å¾„
        agent: ç¿»è¯‘ä»£ç†
        book_id: ä¹¦ç±ID
        enable_human_review: æ˜¯å¦å¯ç”¨äººå·¥å®¡æŸ¥ï¼ˆé»˜è®¤Trueï¼‰
        use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ£€ç´¢ï¼ˆé»˜è®¤Trueï¼‰
    """
    # æ ¹æ®é…ç½®ä¿®æ”¹book_idåç¼€
    # å¦‚æœæ²¡æœ‰äººå·¥ä»‹å…¥ï¼Œä¿®æ”¹book_idä¸ºbook_id_nohuman
    if not enable_human_review:
        book_id = f"{book_id}_nohuman"
    # å¦‚æœæ²¡æœ‰ä½¿ç”¨RAGï¼Œåœ¨book_idååŠ ä¸Š_norag
    if not use_rag:
        book_id = f"{book_id}_norag"
    
    
    chapters = split_epub_by_chapter(json_path)

    for chapter_id, chap in enumerate(chapters):
        chapter_title = chap.get("title", f"Chapter {chapter_id}")
        print("\n" + "="*60)
        print(f"Chapter {chapter_id}: {chapter_title}")
        print("="*60)
        
        content = chap.get("content", "")
        if not content:
            print(f"  [WARNING] Chapter {chapter_id} is empty, skipping...")
            continue
        
        # ===== åŠ è½½å…¨å±€æœ¯è¯­è¡¨å’Œç« èŠ‚ä¸Šä¸‹æ–‡ =====
        print(f"\n  Loading global context...")
        global_glossary = load_global_glossary(book_id, chapter_id)
        
        # åŠ è½½ä¹‹å‰ç« èŠ‚çš„æ‘˜è¦
        try:
            prev_summaries = get_previous_chapter_summaries(book_id, chapter_id)
            if prev_summaries:
                print(f"  åŠ è½½äº† {len(prev_summaries)} ä¸ªä¹‹å‰ç« èŠ‚çš„æ‘˜è¦")
        except Exception as e:
            print(f"  [WARNING] åŠ è½½ç« èŠ‚æ‘˜è¦å¤±è´¥: {e}")
        
        # å¦‚æœå†…å®¹è¶…è¿‡ä¸€å®šé•¿åº¦ï¼Œä»ç„¶éœ€è¦åˆ†å‰²
        chunks = split_chapter_into_chunks(content)
        print(f"  Total chunks: {len(chunks)}")
        
        # ===== é˜¶æ®µ1: è‡ªåŠ¨ç¿»è¯‘æ‰€æœ‰chunks =====
        print(f"\n  Phase 1: Auto-translating all chunks...")
        chunk_results = []
        chunks_data = []  # ç”¨äºç”Ÿæˆæ‘˜è¦
        
        for chunk_id, chunk_text in enumerate(chunks):
            task = {
                "input": {
                    "book_id": book_id,
                    "chapter_id": chapter_id,
                    "chunk_id": chunk_id,
                    "source_text": chunk_text,
                    "thread_id": f"ch{chapter_id}_ck{chunk_id}",
                    # ä¼ é€’å…¨å±€æœ¯è¯­è¡¨
                    "global_glossary": global_glossary,
                    # ä¼ é€’äººå·¥å®¡æŸ¥æ¨¡å¼æ ‡å¿—ï¼ˆç”¨äºæ§åˆ¶é€Ÿç‡é™åˆ¶ï¼‰
                    "enable_human_review": enable_human_review,
                    # ä¼ é€’ RAG ä½¿ç”¨æ ‡å¿—
                    "use_rag": use_rag,
                }
            }
            result = agent.run_chunk_auto(task)
            chunk_results.append(result)
            
            # æ”¶é›†chunkæ•°æ®ç”¨äºç”Ÿæˆæ‘˜è¦
            if isinstance(result, dict):
                chunks_data.append({
                    "source_text": result.get("source_text", chunk_text),
                    "translation": result.get("combined_translation", "")
                })
        
        # ===== é˜¶æ®µ2: æ”¶é›†æ•´ä¸ªchapterçš„æœ¯è¯­è¡¨å¹¶äººå·¥å®¡æŸ¥ =====
        if enable_human_review:
            print(f"\n  Phase 2: Human review for Chapter {chapter_id}...")
        else:
            print(f"\n  Phase 2: Auto-accepting glossary for Chapter {chapter_id}...")
        
        chapter_glossary, chapter_source_text = collect_chapter_glossaries(book_id, chapter_id, len(chunks))
        
        if chapter_glossary:
            print(f"  Found {len(chapter_glossary)} unique terms in this chapter")
            if enable_human_review:
                reviewed_glossary = review_glossary(chapter_glossary, chapter_source_text)
            else:
                # è‡ªåŠ¨æ¥å—æ‰€æœ‰æœ¯è¯­
                reviewed_glossary = chapter_glossary
                for term in reviewed_glossary:
                    term["human_reviewed"] = True
                    term["human_modified"] = False
                print(f"  âˆš Auto-accepted {len(reviewed_glossary)} terms (äººå·¥å®¡æŸ¥å·²ç¦ç”¨)")
            
            print(f"  âˆš Reviewed glossary: {len(reviewed_glossary)} terms")
            
            # ===== æ›´æ–°æ‰€æœ‰chunkæ–‡ä»¶ä¸­çš„æœ¯è¯­è¡¨ =====
            print(f"\n  Updating glossary in chunk files...")
            update_chunks_with_reviewed_glossary(book_id, chapter_id, len(chunks), reviewed_glossary)
        else:
            print(f"  No terms found in this chapter")
            reviewed_glossary = []
        
        # ===== é˜¶æ®µ3: ç”Ÿæˆç« èŠ‚æ‘˜è¦ =====
        print(f"\n  Phase 3: Generating chapter summary...")
        if chunks_data:
            generate_chapter_summary(book_id, chapter_id, chunks_data, enable_human_review)
        
        # ===== é˜¶æ®µ4: ç« èŠ‚ç¿»è¯‘è´¨é‡äººå·¥å®¡æŸ¥ï¼ˆå¸¦é‡æ–°ç¿»è¯‘å¾ªç¯ï¼‰ =====
        if enable_human_review:
            max_retry_count = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_count = 0
            chapter_review_result = None
            
            while retry_count <= max_retry_count:
                print(f"\n  Phase 4: Chapter translation quality review (å°è¯• {retry_count + 1}/{max_retry_count + 1})...")
                chapter_review_result = review_chapter_translation(book_id, chapter_id, len(chunks))
                
                if chapter_review_result and chapter_review_result.get('accepted', False):
                    print(f"  âˆš ç« èŠ‚ç¿»è¯‘å·²é€šè¿‡å®¡æŸ¥")
                    break
                else:
                    feedback = chapter_review_result.get('feedback', 'éœ€è¦ä¿®æ”¹') if chapter_review_result else 'éœ€è¦ä¿®æ”¹'
                    print(f"  [WARNING] ç« èŠ‚ç¿»è¯‘æœªé€šè¿‡å®¡æŸ¥")
                    print(f"  ä¿®æ”¹æ„è§: {feedback}")
                    
                    if retry_count < max_retry_count:
                        print(f"\n  >>> å¼€å§‹é‡æ–°ç¿»è¯‘ï¼ˆæ ¹æ®ä¿®æ”¹æ„è§ï¼‰...")
                        
                        # é‡æ–°ç¿»è¯‘æ‰€æœ‰chunksï¼ˆä½¿ç”¨ä¿®æ”¹æ„è§ï¼‰
                        chunk_results = []
                        chunks_data = []
                        
                        for chunk_id, chunk_text in enumerate(chunks):
                            task = {
                                "input": {
                                    "book_id": book_id,
                                    "chapter_id": chapter_id,
                                    "chunk_id": chunk_id,
                                    "source_text": chunk_text,
                                    "thread_id": f"ch{chapter_id}_ck{chunk_id}_retry{retry_count + 1}",
                                    "global_glossary": global_glossary,
                                    "critique": feedback,  # ä¼ é€’ä¿®æ”¹æ„è§åˆ°critiqueå­—æ®µ
                                    "is_retry": True,  # æ ‡è®°ä¸ºé‡æ–°ç¿»è¯‘
                                    # ä¼ é€’äººå·¥å®¡æŸ¥æ¨¡å¼æ ‡å¿—ï¼ˆç”¨äºæ§åˆ¶é€Ÿç‡é™åˆ¶ï¼‰
                                    "enable_human_review": enable_human_review,
                                    # ä¼ é€’ RAG ä½¿ç”¨æ ‡å¿—
                                    "use_rag": use_rag,
                                }
                            }
                            result = agent.run_chunk_auto(task)
                            chunk_results.append(result)
                            
                            # æ”¶é›†chunkæ•°æ®ç”¨äºç”Ÿæˆæ‘˜è¦
                            if isinstance(result, dict):
                                chunks_data.append({
                                    "source_text": result.get("source_text", chunk_text),
                                    "translation": result.get("combined_translation", "")
                                })
                        
                        print(f"  âˆš é‡æ–°ç¿»è¯‘å®Œæˆï¼Œå‡†å¤‡å†æ¬¡å®¡æŸ¥...")
                        retry_count += 1
                    else:
                        print(f"  [WARNING] å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ{max_retry_count}æ¬¡ï¼‰ï¼Œåœæ­¢é‡è¯•")
                        break
        else:
            # è‡ªåŠ¨æ¥å—ç« èŠ‚ç¿»è¯‘
            print(f"\n  Phase 4: Auto-accepting chapter translation (äººå·¥å®¡æŸ¥å·²ç¦ç”¨)...")
            print(f"  âˆš ç« èŠ‚ç¿»è¯‘å·²è‡ªåŠ¨æ¥å—")
        
        print(f"\n  âˆš Chapter {chapter_id} completed!")
        print("-" * 60 + "\n")

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="è®ºæ–‡ç¿»è¯‘å·¥å…·")
    parser.add_argument(
        "--no-human-review",
        action="store_true",
        help="ç¦ç”¨äººå·¥å®¡æŸ¥ï¼ˆè‡ªåŠ¨æ¥å—æ‰€æœ‰æœ¯è¯­å’Œç¿»è¯‘ï¼‰"
    )
    parser.add_argument(
        "--paper-id",
        type=str,
        default="vgg",
        help="è®ºæ–‡IDï¼ˆé»˜è®¤: vggï¼‰"
    )
    parser.add_argument(
        "--json-path",
        type=str,
        default=None,
        help="JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨ --paper-id è‡ªåŠ¨æ„å»ºè·¯å¾„ï¼‰"
    )
    parser.add_argument(
        "--no-rag",
        action="store_true",
        help="ç¦ç”¨ RAG æ£€ç´¢ï¼Œç›´æ¥ç¿»è¯‘ï¼ˆä¸ä½¿ç”¨ç¿»è¯‘è®°å¿†æ£€ç´¢ï¼‰"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦å¯ç”¨äººå·¥å®¡æŸ¥
    enable_human_review = not args.no_human_review
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨ RAG æ£€ç´¢
    use_rag = not args.no_rag
    
    if enable_human_review:
        print("="*60)
        print("äººå·¥å®¡æŸ¥æ¨¡å¼ï¼šå·²å¯ç”¨")
        print("="*60)
    else:
        print("="*60)
        print("è‡ªåŠ¨æ¨¡å¼ï¼šäººå·¥å®¡æŸ¥å·²ç¦ç”¨ï¼Œå°†è‡ªåŠ¨æ¥å—æ‰€æœ‰æœ¯è¯­å’Œç¿»è¯‘")
        print("="*60)
    
    if use_rag:
        print("RAG æ£€ç´¢ï¼šå·²å¯ç”¨")
    else:
        print("RAG æ£€ç´¢ï¼šå·²ç¦ç”¨ï¼ˆç›´æ¥ç¿»è¯‘æ¨¡å¼ï¼‰")
    print("="*60)
    
    config = ConfigLoader("agents/config.yml")
    config.validate()

    logger = setup_logger(
        "Agent",
        config.get("logging")["log_file"]
    )

    state = StateManager(config.get("agent")["memory_size"])
    executor = ActionExecutor(**config.get("execution"))
    learner = LearningEngine()

    agent = BaseAgent(
        name=config.get("agent")["name"],
        state_manager=state,
        executor=executor,
        learner=learner,
        logger=logger,
        max_steps=config.get("agent")["max_steps"]
    )
    
    # ç¡®å®šJSONæ–‡ä»¶è·¯å¾„
    if args.json_path:
        json_path = args.json_path
    else:
        json_path = f"D:/hw/translation-proj/data/{args.paper_id}_en.json"
    
    print(f"\nä½¿ç”¨æ–‡ä»¶: {json_path}")
    print(f"ä¹¦ç±ID: {args.paper_id}\n")
    
    run_book_translation(
        str(json_path), 
        agent, 
        book_id=args.paper_id,
        enable_human_review=enable_human_review,
        use_rag=use_rag
    )

if __name__ == "__main__":
    main()

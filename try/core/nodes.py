from typing import Annotated, List, Dict, TypedDict, Union, Optional
from enum import Enum

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
# from langchain_core.pydantic_v1 import BaseModel, Field
from pydantic import BaseModel, Field

from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver

from .get_llm import llm
from rag.es_retriever import retrieve_translation_memory

from typing import Any

import json
import os
# ============================================
# 1. å®šä¹‰æ•°æ®ç»“æ„ (State & Pydantic Models)
# ============================================

# (A2) é£æ ¼å…ƒæ•°æ®ç»“æ„
class StyleMetadata(BaseModel):
    domain: str = Field(description="æ–‡æœ¬é¢†åŸŸï¼Œå¦‚æ³•å¾‹ã€æ–‡å­¦ã€è¯´å”±")
    tone: str = Field(description="è¯­ä½“é£æ ¼ï¼Œå¦‚æ­£å¼ã€å£è¯­ã€å¹½é»˜")
    complexity: str = Field(description="æ–‡æœ¬å¤æ‚åº¦")

# (B1) æœ¯è¯­æ¡ç›®ç»“æ„
class TermEntry(BaseModel):
    src: str = Field(description="åŸæ–‡è¯æ±‡")
    type: str = Field(description="ç±»å‹: NER/Term/Idiom/Slang")
    context_meaning: Optional[str] = Field(description="è¯­å¢ƒä¸‹çš„å«ä¹‰")
    suggested_trans: str = Field(description="å»ºè®®è¯‘æ³•")
    rationale: str = Field(description="ç¿»è¯‘ç†ç”±æˆ–ç­–ç•¥")

class TermList(BaseModel):
    terms: List[TermEntry]

# (C3) è¯„ä¼°ç»“æœç»“æ„
class QualityReview(BaseModel):
    score: int = Field(description="1-10åˆ†ï¼Œ10åˆ†ä¸ºå®Œç¾")
    critique: str = Field(description="è¯¦ç»†çš„æ‰¹è¯„å’Œä¿®æ”¹å»ºè®®")
    pass_flag: bool = Field(description="æ˜¯å¦è¾¾åˆ°å‡ºç‰ˆæ ‡å‡†")

class Book:
    book_id: str
    meta: dict
    chapters: List["Chapter"]
class Chapter:
    chapter_id: int
    title: str
    chunks: List["Chunk"]
    memory: Dict[str, str]   # æœ¬ç« ç´¯è®¡æ€»ç»“
class Chunk:
    chunk_id: int
    text: str
    translation: Optional[str]

# --- LangGraph å…¨å±€çŠ¶æ€ ---
class TranslationState(BaseModel):
    # ======== æ ¸å¿ƒè¾“å…¥ï¼ˆå¿…é¡»ï¼‰ ========
    book_id: str
    chapter_id: int
    chunk_id: int
    source_text: str
    thread_id: str
    # ===== ä¸Šä¸‹æ–‡ =====
    book_meta: Dict[str, Any] = Field(default_factory=dict)
    chapter_memory: List[str] = Field(default_factory=list)
    global_glossary: Dict[str, Any] = Field(default_factory=dict)      # å…¨ä¹¦æœ¯è¯­è¡¨
    rag_context: List[str] = Field(default_factory=list)              # ES / å¤–éƒ¨æ£€ç´¢ç»“æœ
    # ===== ä¸­é—´ç»“æœ =====
    style_guide: Dict[str, Any] = Field(default_factory=dict)
    raw_terms: List[str] = Field(default_factory=list) # åˆæ­¥è¯†åˆ«çš„éš¾è¯
    glossary: List[Dict[str, Any]] = Field(default_factory=list) # (B3) ç»è¿‡æŸ¥è¯å’Œäººå·¥ç¡®è®¤çš„æœ¯è¯­è¡¨
    # ===== ç¿»è¯‘ç»“æœ =====
    draft_versions: List[str] = Field(default_factory=list) # ç›´è¯‘/æ„è¯‘/é£æ ¼åŒ–ç‰ˆæœ¬
    combined_translation: Optional[str] = None # èåˆåçš„è¯‘æ–‡
    back_translation: Optional[str] = None # å›è¯‘æ–‡
    # ===== æ§åˆ¶ä¿¡å· =====
    need_human_review: bool = True
    critique: Optional[str] = None
    quality_score: Optional[float] = None
    revision_count: int = 0

# ============================================
# 2. èŠ‚ç‚¹å®ç° (Node Functions)
# ============================================

# --- Node A: é£æ ¼ä¸é¢„å¤„ç† ---
def node_analyze_style(state: TranslationState):
    print("\nğŸ”¹ [Phase A] Analyzing Style & Domain...")
    
    chapter_ctx = "\n".join(state.chapter_memory) if state.chapter_memory else "æ— "
    
    prompt = f"""
    åˆ†æä»¥ä¸‹æ–‡æœ¬çš„é¢†åŸŸã€è¯­ä½“é£æ ¼å’Œå¤æ‚åº¦ã€‚
    å‚è€ƒä¸Šæ–‡è„‰ç»œï¼š{chapter_ctx}
    å½“å‰æ–‡æœ¬ï¼š{state.source_text}
    """
    # ç»“æ„åŒ–è¾“å‡º
    structured_llm = llm.with_structured_output(StyleMetadata)
    res = structured_llm.invoke(prompt)
    print("----------------------------", res)
    
    # ç›´æ¥æ›´æ–°çŠ¶æ€å±æ€§
    state.style_guide = res.model_dump()
    return {"style_guide": state.style_guide}

# --- Node B1: æœ¯è¯­è¯†åˆ« (Term Miner) ---
def node_extract_terms(state: TranslationState):
    print("\nğŸ”¹ [Phase B1] Mining Terms & Entities...")
    
    domain = state.style_guide.get('domain', 'æœªçŸ¥é¢†åŸŸ')
    
    prompt = f"""
    ä½ æ˜¯æœ¯è¯­ä¸“å®¶ã€‚è¯·è¯†åˆ«æ–‡æœ¬ä¸­çš„ï¼š
    1. å‘½åå®ä½“ (NER)
    2. é¢†åŸŸæœ¯è¯­ (Domain Terms)
    3. æ–‡åŒ–è´Ÿè½½è¯/ä¿šè¯­ (Idioms/Slang)

    ä»…è¾“å‡ºéœ€è¦æŸ¥è¯æˆ–ç»Ÿä¸€è¯‘åçš„è¯æ±‡åˆ—è¡¨ã€‚
    æ–‡æœ¬ï¼š{state.source_text}
    é¢†åŸŸï¼š{domain}
    """
    class RawTerms(BaseModel):
        terms: List[str]

    structured_llm = llm.with_structured_output(RawTerms)
    res = structured_llm.invoke(prompt)
    print("----------------------------", res)
    
    state.raw_terms = res.terms
    return {"raw_terms": state.raw_terms}

# --- Node B2: çŸ¥è¯†æŸ¥è¯ (RAG/Search) ---
def node_search_and_consolidate(state: TranslationState):
    print("\nğŸ”¹ [Phase B2] Searching & Standardizing Terms (RAG)...")
    
    consolidated = []
    
    for term in state.raw_terms:
        search_result = retrieve_translation_memory(term, top_k=3)
        term_prompt = f"""
        You are a terminology expert.

        Term: "{term}"
        Source text: "{state.source_text}"

        Retrieved translation memory:
        {search_result}

        Output a JSON object with ALL fields:
        {{
        "src": string,
        "suggested_trans": string,
        "type": string,
        "context_meaning": string,
        "rationale": string
        }}
        """
        try:
            entry = llm.with_structured_output(TermEntry).invoke(term_prompt)
            consolidated.append(entry.model_dump())
        except Exception as e:
            consolidated.append({
                "src": term,
                "suggested_trans": term,
                "type": "Unknown",
                "context_meaning": "Insufficient context from retrieval.",
                "rationale": f"Fallback due to error: {e}"
            })
    
    state.glossary = consolidated
    return {"glossary": state.glossary}



# --- Node C1: å¤šç­–ç•¥ç¿»è¯‘ä¸èåˆ (The Translator) ---
def node_translate_fusion(state: TranslationState):
    iteration = state.revision_count
    print(f"\nğŸ”¸ [Phase 2] Translation Generation (Iter {iteration+1})...")
    
    glossary_text = "\n".join([f"- {t['src']} -> {t['suggested_trans']} ({t['rationale']})" for t in state.glossary])
    style_str = str(state.style_guide)
    prev_feedback = state.critique or "æ— "
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªé«˜çº§ç¿»è¯‘å¼•æ“ã€‚è¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

    1. **ç†è§£ä¸è§£æ„**ï¼šåˆ†æå¥å­ç»“æ„ã€‚
    2. **å¤šç‰ˆæœ¬ç”Ÿæˆ**ï¼š
    - ç›´è¯‘ç‰ˆ (Literal)
    - æ„è¯‘ç‰ˆ (Liberal)
    3. **èåˆä¸æ¶¦è‰²**ï¼šç»“åˆæœ€ä½³è¡¨è¾¾ï¼Œç”Ÿæˆæœ€ç»ˆè¯‘æ–‡ã€‚

    [çº¦æŸæ¡ä»¶]
    - ä¸¥æ ¼éµå®ˆé£æ ¼ï¼š{style_str}
    - å¼ºåˆ¶ä½¿ç”¨æœ¯è¯­è¡¨ï¼š
    {glossary_text}
    - ä¸Šä¸€è½®åé¦ˆï¼ˆå¦‚æœ‰ï¼‰ï¼š{prev_feedback}

    [åŸæ–‡]
    {state.source_text}

    è¯·åªè¾“å‡ºæœ€ç»ˆèåˆåçš„è¯‘æ–‡ã€‚
    """
    response = llm.invoke(prompt)
    state.combined_translation = response.content
    state.revision_count += 1
    print("----------------------------", response.content)
    return {
        "combined_translation": state.combined_translation,
        "revision_count": state.revision_count
    }


# --- Node C2: å›è¯‘ä¸ TEaR è¯„ä¼° ---
def node_tear_evaluation(state: TranslationState):
    print("\nğŸ”¸ [Phase 3] TEaR Evaluation (Back-translation & Scoring)...")
    
    bt_prompt = f"Translate the following text back to the source language (English) strictly:\n{state.combined_translation}"
    bt_res = llm.invoke(bt_prompt)
    state.back_translation = bt_res.content
    
    eval_prompt = f"""
    ä½ æ˜¯ç¿»è¯‘è´¨é‡è¯„ä¼°ç³»ç»Ÿã€‚

    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™æ–‡æœ¬ï¼š

    {{
    "score": 0-10 çš„æ•´æ•°,
    "pass_flag": true æˆ– false,
    "critique": "ç®€è¦è¯„ä¼°æ„è§"
    }}

    ã€åŸæ–‡ã€‘
    {state.source_text}

    ã€å½“å‰è¯‘æ–‡ã€‘
    {state.combined_translation}

    ã€å›è¯‘ã€‘
    {state.back_translation}
    """
    eval_res = llm.with_structured_output(QualityReview).invoke(eval_prompt)
    state.quality_score = eval_res.score
    state.critique = eval_res.critique
    
    print(f"   >>> Score: {eval_res.score}/10 | Pass: {eval_res.pass_flag}")
    return {
        "back_translation": state.back_translation,
        "quality_score": state.quality_score,
        "critique": state.critique
    }
# --- Node D: æŒä¹…åŒ–ä¿å­˜ ---
def node_persistence(state: TranslationState):
    """ä¿å­˜æœ€ç»ˆç¿»è¯‘ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶"""
    
    # æ—¢ç„¶ state æ˜¯ TranslationState ç±»å‹ï¼Œç›´æ¥ç‚¹å·è®¿é—®æœ€å®‰å…¨
    try:
        book_id = state.book_id
        chapter_id = state.chapter_id
        chunk_id = state.chunk_id
        translation = state.combined_translation
        source_text = state.source_text
        quality_score = state.quality_score
        glossary = state.glossary
    except AttributeError:
        # ä¸‡ä¸€ LangGraph ä¼ è¿›æ¥çš„æ˜¯ä¸ª dictï¼ˆé€šå¸¸ä¸ä¼šï¼Œé™¤éé…ç½®æ”¹äº†ï¼‰
        book_id = state.get("book_id")
        chapter_id = state.get("chapter_id")
        chunk_id = state.get("chunk_id")
        translation = state.get("combined_translation")
        source_text = state.get("source_text")
        quality_score = state.get("quality_score")
        glossary = state.get("glossary")

    print(f"ğŸ’¾ [Persistence] Writing data for Chunk {chunk_id}...")

    # è·¯å¾„æ„é€ 
    base_dir = f"./output/{book_id}/chapter_{chapter_id}"
    os.makedirs(base_dir, exist_ok=True)
    
    # å»ºè®® chunk_id æ ¼å¼åŒ–ä¸º 3 ä½æˆ– 4 ä½æ•°å­—ï¼Œæ–¹ä¾¿æ’åº
    file_path = os.path.join(base_dir, f"chunk_{int(chunk_id):03d}.json")
    
    data_to_save = {
        "chunk_id": chunk_id,
        "source_text": source_text,
        "translation": translation,
        "quality_score": quality_score,
        "glossary": glossary
    }

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data_to_save, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“‚ File saved: {file_path}")
    return {"need_human_review": False}
from typing import Annotated, List, Dict, TypedDict, Union, Optional
from enum import Enum

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
# from langchain_core.pydantic_v1 import BaseModel, Field
from pydantic import BaseModel, Field

from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver

from .get_llm import llm
from rag.es_retriever import retrieve_translation_memory
from utils.memory_storage import (
    get_previous_chapters_memory,
    get_similar_translation_examples,
    get_previous_chapter_summaries,
    get_chapter_translation_memory
)

from typing import Any

import json
import os
import time

# å°è¯•å¯¼å…¥RateLimitErrorï¼ˆä¸åŒç‰ˆæœ¬çš„openaiå¯èƒ½ä½ç½®ä¸åŒï¼‰
try:
    from openai import RateLimitError
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨å¼‚å¸¸å¤„ç†
    RateLimitError = Exception  # ä½¿ç”¨é€šç”¨Exceptionï¼Œåœ¨ä»£ç ä¸­é€šè¿‡é”™è¯¯æ¶ˆæ¯åˆ¤æ–­
# ============================================
# 1. å®šä¹‰æ•°æ®ç»“æ„ (State & Pydantic Models)
# ============================================

# (A2) é£æ ¼å…ƒæ•°æ®ç»“æ„
class StyleMetadata(BaseModel):
    domain: str = Field(description="æ–‡æœ¬é¢†åŸŸï¼Œå¦‚æ³•å¾‹ã€æ–‡å­¦ã€è¯´å”±")
    tone: str = Field(description="è¯­ä½“é£æ ¼ï¼Œå¦‚æ­£å¼ã€å£è¯­ã€å¹½é»˜")
    complexity: str = Field(description="æ–‡æœ¬å¤æ‚åº¦")

# (B1) æœ¯è¯­æ¡ç›®ç»“æ„
class TermEntry(BaseModel):
    src: str = Field(description="åŸæ–‡è¯æ±‡ï¼ˆè‹±æ–‡ï¼‰")
    type: str = Field(description="ç±»å‹: NER/Term/Idiom/Slang/Acronym/Proper Noun")
    context_meaning: Optional[str] = Field(description="è¯­å¢ƒä¸‹çš„å«ä¹‰ï¼ˆä¸­æ–‡ï¼‰")
    suggested_trans: str = Field(description="å»ºè®®è¯‘æ³•ï¼ˆå¿…é¡»æ˜¯ä¸­æ–‡ç®€ä½“ï¼‰")
    rationale: str = Field(description="ç¿»è¯‘ç†ç”±æˆ–ç­–ç•¥ï¼ˆä¸­æ–‡ï¼‰")

class TermList(BaseModel):
    terms: List[TermEntry]

# (C3) è¯„ä¼°ç»“æœç»“æ„
class QualityReview(BaseModel):
    score: int = Field(description="1-10åˆ†ï¼Œ10åˆ†ä¸ºå®Œç¾")
    critique: str = Field(description="è¯¦ç»†çš„æ‰¹è¯„å’Œä¿®æ”¹å»ºè®®")
    pass_flag: bool = Field(description="æ˜¯å¦è¾¾åˆ°å‡ºç‰ˆæ ‡å‡†")

class Book:
    book_id: str
    meta: dict
    chapters: List["Chapter"]
class Chapter:
    chapter_id: int
    title: str
    chunks: List["Chunk"]
    memory: Dict[str, str]   # æœ¬ç« ç´¯è®¡æ€»ç»“
class Chunk:
    chunk_id: int
    text: str
    translation: Optional[str]

# --- LangGraph å…¨å±€çŠ¶æ€ ---
class TranslationState(BaseModel):
    # ======== æ ¸å¿ƒè¾“å…¥ï¼ˆå¿…é¡»ï¼‰ ========
    book_id: str
    chapter_id: int
    chunk_id: int
    source_text: str
    thread_id: str
    # ===== ä¸Šä¸‹æ–‡ =====
    book_meta: Dict[str, Any] = Field(default_factory=dict)
    chapter_memory: List[str] = Field(default_factory=list)
    global_glossary: Dict[str, Any] = Field(default_factory=dict)      # å…¨ä¹¦æœ¯è¯­è¡¨
    rag_context: List[str] = Field(default_factory=list)              # ES / å¤–éƒ¨æ£€ç´¢ç»“æœ
    # ===== ä¸­é—´ç»“æœ =====
    style_guide: Dict[str, Any] = Field(default_factory=dict)
    raw_terms: List[str] = Field(default_factory=list) # åˆæ­¥è¯†åˆ«çš„éš¾è¯
    glossary: List[Dict[str, Any]] = Field(default_factory=list) # (B3) ç»è¿‡æŸ¥è¯å’Œäººå·¥ç¡®è®¤çš„æœ¯è¯­è¡¨
    # ===== ç¿»è¯‘ç»“æœ =====
    draft_versions: List[str] = Field(default_factory=list) # ç›´è¯‘/æ„è¯‘/é£æ ¼åŒ–ç‰ˆæœ¬
    combined_translation: Optional[str] = None # èåˆåçš„è¯‘æ–‡
    back_translation: Optional[str] = None # å›è¯‘æ–‡
    # ===== æ§åˆ¶ä¿¡å· =====
    need_human_review: bool = True
    critique: Optional[str] = None
    quality_score: Optional[float] = None
    revision_count: int = 0

# ============================================
# 2. èŠ‚ç‚¹å®ç° (Node Functions)
# ============================================

# --- Node A: é£æ ¼ä¸é¢„å¤„ç† ---
def node_analyze_style(state: TranslationState):
    print("\nğŸ”¹ [Phase A] Analyzing Style & Domain...")
    
    # åŠ è½½ç« èŠ‚ä¸Šä¸‹æ–‡ï¼ˆä¹‹å‰çš„ç« èŠ‚æ‘˜è¦å’Œç¿»è¯‘è®°å¿†ï¼‰
    chapter_context_parts = []
    
    # 1. åŠ è½½ä¹‹å‰ç« èŠ‚çš„æ‘˜è¦
    try:
        prev_summaries = get_previous_chapter_summaries(state.book_id, state.chapter_id)
        if prev_summaries:
            summary_text = "\n".join([
                f"ç¬¬{summ['chapter_id']}ç« æ‘˜è¦: {summ['summary']}" 
                for summ in prev_summaries[-2:]  # åªå–æœ€è¿‘2ç« 
            ])
            chapter_context_parts.append(f"ä¹‹å‰ç« èŠ‚æ‘˜è¦:\n{summary_text}")
    except Exception as e:
        print(f"  âš ï¸  åŠ è½½ç« èŠ‚æ‘˜è¦å¤±è´¥: {e}")
    
    # 2. åŠ è½½å½“å‰ç« èŠ‚å·²ç¿»è¯‘çš„chunkï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
    try:
        current_chapter_memories = get_chapter_translation_memory(
            state.book_id, state.chapter_id
        )
        # åªå–å½“å‰chunkä¹‹å‰çš„ç¿»è¯‘è®°å¿†
        prev_chunks = [
            mem for mem in current_chapter_memories 
            if mem.get('chunk_id', -1) < state.chunk_id
        ]
        if prev_chunks:
            context_text = "\n".join([
                f"Chunk {mem['chunk_id']}: {mem['source_text'][:100]}... â†’ {mem['translation'][:100]}..."
                for mem in prev_chunks[-3:]  # åªå–æœ€è¿‘3ä¸ªchunk
            ])
            chapter_context_parts.append(f"æœ¬ç« å·²ç¿»è¯‘å†…å®¹:\n{context_text}")
    except Exception as e:
        print(f"  âš ï¸  åŠ è½½ç« èŠ‚ç¿»è¯‘è®°å¿†å¤±è´¥: {e}")
    
    # 3. ä½¿ç”¨stateä¸­çš„chapter_memoryï¼ˆå¦‚æœæœ‰ï¼‰
    if state.chapter_memory:
        chapter_context_parts.append("\n".join(state.chapter_memory))
    
    chapter_ctx = "\n\n".join(chapter_context_parts) if chapter_context_parts else "æ— "
    
    prompt = f"""
    åˆ†æä»¥ä¸‹æ–‡æœ¬çš„é¢†åŸŸã€è¯­ä½“é£æ ¼å’Œå¤æ‚åº¦ã€‚
    å‚è€ƒä¸Šä¸‹æ–‡è„‰ç»œï¼š{chapter_ctx}
    å½“å‰æ–‡æœ¬ï¼š{state.source_text}
    
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
    {{
        "domain": "é¢†åŸŸ",
        "tone": "è¯­ä½“é£æ ¼",
        "complexity": "å¤æ‚åº¦"
    }}
    """
    # ç»“æ„åŒ–è¾“å‡º
    try:
        structured_llm = llm.with_structured_output(StyleMetadata)
        res = structured_llm.invoke(prompt)
        print("----------------------------", res)
        style_data = res.model_dump()
    except Exception as e:
        print(f"âš ï¸  Structured output failed: {e}")
        print("   Using default style metadata...")
        # å›é€€åˆ°é»˜è®¤å€¼
        style_data = {
            "domain": "é€šç”¨",
            "tone": "æ­£å¼",
            "complexity": "ä¸­ç­‰"
        }
    
    # ç›´æ¥æ›´æ–°çŠ¶æ€å±æ€§
    state.style_guide = style_data
    return {"style_guide": state.style_guide}

# --- Node B1: æœ¯è¯­è¯†åˆ« (Term Miner) ---
def node_extract_terms(state: TranslationState):
    print("\nğŸ”¹ [Phase B1] Mining Terms & Entities...")
    
    domain = state.style_guide.get('domain', 'æœªçŸ¥é¢†åŸŸ')
    
    prompt = f"""
    ä½ æ˜¯æœ¯è¯­ä¸“å®¶ã€‚è¯·è¯†åˆ«æ–‡æœ¬ä¸­çš„ï¼š
    1. å‘½åå®ä½“ (NER) - äººåã€åœ°åã€æœºæ„åç­‰
    2. é¢†åŸŸæœ¯è¯­ (Domain Terms) - ä¸“ä¸šæœ¯è¯­ã€æŠ€æœ¯è¯æ±‡
    3. æ–‡åŒ–è´Ÿè½½è¯/ä¿šè¯­ (Idioms/Slang) - ä¹ è¯­ã€ä¿šè¯­ç­‰

    é‡è¦è¦æ±‚ï¼š
    - åªè¯†åˆ«è‹±æ–‡åŸæ–‡ä¸­çš„è¯æ±‡ï¼Œä¸è¦è¯†åˆ«ä¸­æ–‡
    - åªè¾“å‡ºè‹±æ–‡åŸæ–‡è¯æ±‡ï¼Œä¸è¦è¾“å‡ºç¿»è¯‘
    - ä»…è¾“å‡ºéœ€è¦æŸ¥è¯æˆ–ç»Ÿä¸€è¯‘åçš„è¯æ±‡åˆ—è¡¨
    
    æ–‡æœ¬ï¼š{state.source_text}
    é¢†åŸŸï¼š{domain}
    
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™æ–‡æœ¬ï¼š
    {{
        "terms": ["term1", "term2", "term3"]
    }}
    
    æ³¨æ„ï¼štermsæ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ å¿…é¡»æ˜¯è‹±æ–‡åŸæ–‡ï¼Œä¸èƒ½æ˜¯ä¸­æ–‡ç¿»è¯‘ã€‚
    """
    class RawTerms(BaseModel):
        terms: List[str]

    try:
        structured_llm = llm.with_structured_output(RawTerms)
        res = structured_llm.invoke(prompt)
        print("----------------------------", res)
        terms_list = res.terms
    except Exception as e:
        print(f"âš ï¸  Structured output failed: {e}")
        print("   Falling back to manual JSON parsing...")
        # å›é€€æ–¹æ¡ˆï¼šæ™®é€šè°ƒç”¨ + æ‰‹åŠ¨è§£æ
        response = llm.invoke(prompt)
        content = response.content.strip()
        
        # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å« markdown ä»£ç å—ï¼‰
        import re
        json_match = re.search(r'\{[^{}]*"terms"[^{}]*\[[^\]]*\][^{}]*\}', content, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå†…å®¹
            json_str = content
            if json_str.startswith('```'):
                json_str = json_str.split('```')[1]
                if json_str.startswith('json'):
                    json_str = json_str[4:]
            json_str = json_str.strip()
        
        try:
            import json
            parsed = json.loads(json_str)
            terms_list = parsed.get("terms", [])
        except json.JSONDecodeError as je:
            print(f"âš ï¸  JSON parsing failed: {je}")
            print(f"   Raw content: {content[:200]}...")
            # æœ€åçš„å›é€€ï¼šå°è¯•ä»æ–‡æœ¬ä¸­æå–å¯èƒ½çš„æœ¯è¯­
            terms_list = []
            # ç®€å•æå–ï¼šæŸ¥æ‰¾å¼•å·ä¸­çš„å†…å®¹æˆ–å¤§å†™å•è¯
            import re
            # æå–å¼•å·ä¸­çš„å†…å®¹
            quoted_terms = re.findall(r'"([^"]+)"', content)
            terms_list.extend(quoted_terms)
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç©ºåˆ—è¡¨
            if not terms_list:
                print("   âš ï¸  Could not extract terms, using empty list")
    
    state.raw_terms = terms_list
    return {"raw_terms": state.raw_terms}

# --- Node B2: çŸ¥è¯†æŸ¥è¯ (RAG/Search) ---
def node_search_and_consolidate(state: TranslationState):
    print("\nğŸ”¹ [Phase B2] Searching & Standardizing Terms (RAG)...")
    
    consolidated = []
    
    for term in state.raw_terms:
        search_result = retrieve_translation_memory(term, top_k=3)
        term_prompt = f"""
        You are a terminology expert specializing in English-to-Chinese translation.

        Task: Translate the following English term into Chinese (Simplified Chinese).

        Term: "{term}"
        Source text: "{state.source_text}"

        Retrieved translation memory:
        {search_result}

        IMPORTANT: 
        - The "suggested_trans" field MUST be in Chinese (Simplified Chinese), not any other language.
        - If the term is a proper noun or acronym (like "YOLO"), you may keep it as is or provide a Chinese explanation.
        - The "rationale" field should explain your translation choice in Chinese.

        Output a JSON object with ALL fields:
        {{
        "src": string (the original English term),
        "suggested_trans": string (MUST be in Chinese/Simplified Chinese),
        "type": string (e.g., "Terminology", "Acronym", "Proper Noun"),
        "context_meaning": string (explain the meaning in the context, in Chinese),
        "rationale": string (explain translation rationale, in Chinese)
        }}
        """
        try:
            entry = llm.with_structured_output(TermEntry).invoke(term_prompt)
            consolidated.append(entry.model_dump())
        except Exception as e:
            consolidated.append({
                "src": term,
                "suggested_trans": term,
                "type": "Unknown",
                "context_meaning": "Insufficient context from retrieval.",
                "rationale": f"Fallback due to error: {e}"
            })
    
    state.glossary = consolidated
    return {"glossary": state.glossary}



# --- Node C1: å¤šç­–ç•¥ç¿»è¯‘ä¸èåˆ (The Translator) ---
def node_translate_fusion(state: TranslationState):
    iteration = state.revision_count
    print(f"\nğŸ”¸ [Phase 2] Translation Generation (Iter {iteration+1})...")
    
    # ç›´æ¥ä½¿ç”¨åŸæ–‡ï¼Œä¸å†æå–LaTeXå…¬å¼
    source_text_cleaned = state.source_text
    
    # åŠ è½½å…¨å±€æœ¯è¯­è¡¨ï¼ˆè·¨ç« èŠ‚ï¼‰
    global_glossary_text = ""
    if state.global_glossary:
        global_terms = []
        for term_key, term_info in state.global_glossary.items():
            if isinstance(term_info, dict):
                src = term_info.get('src', term_key)
                trans = term_info.get('suggested_trans', '')
                if src and trans:
                    global_terms.append(f"- {src} -> {trans}")
        if global_terms:
            global_glossary_text = "\n".join(global_terms[:20])  # é™åˆ¶æ•°é‡
            print(f"  ğŸ“š åŠ è½½äº† {len(global_terms)} ä¸ªå…¨å±€æœ¯è¯­")
    
    # å½“å‰chunkçš„æœ¯è¯­è¡¨
    current_glossary_text = "\n".join([
        f"- {t['src']} -> {t['suggested_trans']} ({t.get('rationale', '')})" 
        for t in state.glossary
    ])
    
    # åˆå¹¶æœ¯è¯­è¡¨
    all_glossary_text = ""
    if global_glossary_text:
        all_glossary_text += f"ã€å…¨å±€æœ¯è¯­è¡¨ï¼ˆæ¥è‡ªä¹‹å‰ç« èŠ‚ï¼‰ã€‘\n{global_glossary_text}\n\n"
    if current_glossary_text:
        all_glossary_text += f"ã€å½“å‰ç« èŠ‚æœ¯è¯­è¡¨ã€‘\n{current_glossary_text}"
    
    style_str = str(state.style_guide)
    prev_feedback = state.critique or "æ— "
    
    # åŠ è½½ç›¸ä¼¼çš„ç¿»è¯‘ç¤ºä¾‹ï¼ˆä»å·²ç¿»è¯‘çš„æ–‡æœ¬ä¸­ï¼‰
    translation_examples = []
    try:
        similar_examples = get_similar_translation_examples(
            state.source_text, state.book_id, top_k=3
        )
        if similar_examples:
            print(f"  ğŸ“– æ‰¾åˆ° {len(similar_examples)} ä¸ªç›¸ä¼¼çš„ç¿»è¯‘ç¤ºä¾‹")
            translation_examples = similar_examples
    except Exception as e:
        print(f"  âš ï¸  åŠ è½½ç¿»è¯‘ç¤ºä¾‹å¤±è´¥: {e}")
    
    # åŠ è½½ä¹‹å‰ç« èŠ‚çš„ç¿»è¯‘è®°å¿†ï¼ˆç”¨äºé£æ ¼å‚è€ƒï¼‰
    previous_memories = []
    try:
        prev_memories = get_previous_chapters_memory(
            state.book_id, state.chapter_id, top_k=3
        )
        if prev_memories:
            print(f"  ğŸ“ åŠ è½½äº† {len(prev_memories)} ä¸ªä¹‹å‰ç« èŠ‚çš„ç¿»è¯‘è®°å¿†")
            previous_memories = prev_memories
    except Exception as e:
        print(f"  âš ï¸  åŠ è½½ä¹‹å‰ç« èŠ‚è®°å¿†å¤±è´¥: {e}")
    
    # æ„å»ºç¿»è¯‘ç¤ºä¾‹æ–‡æœ¬
    examples_text = ""
    if translation_examples:
        examples_text = "\nã€ç›¸ä¼¼ç¿»è¯‘ç¤ºä¾‹ï¼ˆå‚è€ƒè¿™äº›å·²ç¿»è¯‘çš„æ–‡æœ¬å¯¹ï¼‰ã€‘\n"
        for i, ex in enumerate(translation_examples, 1):
            examples_text += f"\nç¤ºä¾‹{i}:\nåŸæ–‡: {ex['source_text'][:200]}...\nè¯‘æ–‡: {ex['translation'][:200]}...\n"
    
    if previous_memories:
        examples_text += "\nã€ä¹‹å‰ç« èŠ‚çš„ç¿»è¯‘é£æ ¼å‚è€ƒã€‘\n"
        for i, mem in enumerate(previous_memories, 1):
            examples_text += f"\nå‚è€ƒ{i}:\nåŸæ–‡: {mem['source_text'][:150]}...\nè¯‘æ–‡: {mem['translation'][:150]}...\n"
    
    # å¤šæ­¥éª¤å¼•å¯¼ç¿»è¯‘çš„prompt
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé«˜çº§ç¿»è¯‘å¼•æ“ï¼Œéœ€è¦å‚è€ƒå·²ç¿»è¯‘çš„æ–‡æœ¬å¯¹æ¥ä¿æŒç¿»è¯‘é£æ ¼çš„ä¸€è‡´æ€§ã€‚

ã€ç¿»è¯‘æ­¥éª¤ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

æ­¥éª¤1ï¼šç†è§£ä¸è§£æ„
- ä»”ç»†åˆ†æåŸæ–‡çš„å¥å­ç»“æ„ã€è¯­æ³•å…³ç³»å’Œè¯­ä¹‰å±‚æ¬¡
- è¯†åˆ«å…³é”®ä¿¡æ¯ç‚¹å’Œé€»è¾‘è¿æ¥
- æ³¨æ„ï¼šæ–‡æœ¬ä¸­çš„ __LATEX_PLACEHOLDER_X__ æ˜¯LaTeXå…¬å¼å ä½ç¬¦ï¼Œè¯·ä¿æŒåŸæ ·

æ­¥éª¤2ï¼šå‚è€ƒå·²ç¿»è¯‘æ–‡æœ¬
- ä»”ç»†ç ”ç©¶ä¸‹é¢æä¾›çš„å·²ç¿»è¯‘æ–‡æœ¬å¯¹
- å­¦ä¹ å…¶ç¿»è¯‘é£æ ¼ã€æœ¯è¯­ä½¿ç”¨å’Œè¡¨è¾¾æ–¹å¼
- ç¡®ä¿å½“å‰ç¿»è¯‘ä¸å·²æœ‰ç¿»è¯‘ä¿æŒé£æ ¼ä¸€è‡´

æ­¥éª¤3ï¼šå¤šç‰ˆæœ¬ç”Ÿæˆï¼ˆåœ¨è„‘æµ·ä¸­ï¼‰
- ç›´è¯‘ç‰ˆï¼šå°½å¯èƒ½è´´è¿‘åŸæ–‡ç»“æ„ï¼Œä¿ç•™åŸæ–‡çš„è¡¨è¾¾æ–¹å¼
- æ„è¯‘ç‰ˆï¼šæ ¹æ®ç›®æ ‡è¯­è¨€ä¹ æƒ¯è°ƒæ•´è¡¨è¾¾ï¼Œä½¿è¯‘æ–‡æ›´è‡ªç„¶æµç•…
- é£æ ¼åŒ–ç‰ˆï¼šç»“åˆå·²ç¿»è¯‘æ–‡æœ¬çš„é£æ ¼ï¼Œä¿æŒå…¨ä¹¦ä¸€è‡´æ€§

æ­¥éª¤4ï¼šèåˆä¸æ¶¦è‰²
- ç»“åˆç›´è¯‘å’Œæ„è¯‘çš„ä¼˜ç‚¹
- å‚è€ƒå·²ç¿»è¯‘æ–‡æœ¬çš„é£æ ¼å’Œæœ¯è¯­ä½¿ç”¨
- ç¡®ä¿æœ¯è¯­ä½¿ç”¨ä¸æœ¯è¯­è¡¨å®Œå…¨ä¸€è‡´
- ç”Ÿæˆæœ€ç»ˆè¯‘æ–‡

ã€çº¦æŸæ¡ä»¶ã€‘
- ä¸¥æ ¼éµå®ˆé£æ ¼ï¼š{style_str}
- å¼ºåˆ¶ä½¿ç”¨æœ¯è¯­è¡¨ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
{all_glossary_text if all_glossary_text else "æ— æœ¯è¯­è¡¨"}
    - ä¸Šä¸€è½®åé¦ˆï¼ˆå¦‚æœ‰ï¼‰ï¼š{prev_feedback}
    - æ³¨æ„ï¼šå¦‚æœæ–‡æœ¬ä¸­åŒ…å«LaTeXå…¬å¼ï¼ˆå¦‚ $...$ æˆ– $$...$$ï¼‰ï¼Œè¯·ä¿æŒåŸæ ·ï¼Œä¸è¦ç¿»è¯‘

{examples_text if examples_text else ""}

ã€å¾…ç¿»è¯‘åŸæ–‡ã€‘
{source_text_cleaned}

è¯·åªè¾“å‡ºæœ€ç»ˆèåˆåçš„è¯‘æ–‡ï¼Œä¸è¦è¾“å‡ºä¸­é—´æ­¥éª¤ã€‚
"""
    
    # æ·»åŠ é‡è¯•æœºåˆ¶
    max_retries = 3
    retry_delay = 2  # ç§’
    for attempt in range(max_retries):
        try:
            response = llm.invoke(prompt)
            translated_text = response.content
            state.combined_translation = translated_text
            state.revision_count += 1
            print("----------------------------", translated_text)
            break
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
            error_str = str(e)
            is_rate_limit = "RateLimitError" in str(type(e).__name__) or "rate_limit" in error_str.lower() or "429" in error_str
            
            if is_rate_limit and attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1)
                print(f"  âš ï¸  é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
            elif attempt < max_retries - 1:
                print(f"  âš ï¸  ç¿»è¯‘é”™è¯¯: {e}ï¼Œç­‰å¾… {retry_delay} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
            else:
                print(f"  âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨åŸæ–‡ä½œä¸ºç¿»è¯‘")
                state.combined_translation = state.source_text
                state.revision_count += 1
    
    return {
        "combined_translation": state.combined_translation,
        "revision_count": state.revision_count
    }


# --- Node C2: å›è¯‘ä¸ TEaR è¯„ä¼° ---
def node_tear_evaluation(state: TranslationState):
    print("\nğŸ”¸ [Phase 3] TEaR Evaluation (Back-translation & Scoring)...")
    
    # ç›´æ¥ä½¿ç”¨ç¿»è¯‘ç»“æœï¼Œä¸å†æå–LaTeXå…¬å¼
    translation_cleaned = state.combined_translation
    
    bt_prompt = f"""Translate the following text back to the source language (English) strictly.
Note: If the text contains LaTeX formulas (like $...$ or $$...$$), keep them unchanged, do not translate them.

Text to translate:
{translation_cleaned}"""
    
    # æ·»åŠ é‡è¯•æœºåˆ¶å¤„ç†é€Ÿç‡é™åˆ¶
    max_retries = 3
    retry_delay = 2
    back_translation = None
    
    for attempt in range(max_retries):
        try:
            bt_res = llm.invoke(bt_prompt)
            back_translation = bt_res.content
            state.back_translation = back_translation
            break
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
            error_str = str(e)
            is_rate_limit = "RateLimitError" in str(type(e).__name__) or "rate_limit" in error_str.lower() or "429" in error_str
            
            if is_rate_limit and attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1)
                print(f"  âš ï¸  é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
            elif attempt < max_retries - 1:
                print(f"  âš ï¸  å›è¯‘é”™è¯¯: {e}ï¼Œç­‰å¾… {retry_delay} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
            else:
                print(f"  âŒ å›è¯‘å¤±è´¥ï¼Œè·³è¿‡å›è¯‘æ­¥éª¤")
                state.back_translation = state.source_text  # ä½¿ç”¨åŸæ–‡ä½œä¸ºå›è¯‘ç»“æœ
    
    eval_prompt = f"""
    ä½ æ˜¯ç¿»è¯‘è´¨é‡è¯„ä¼°ç³»ç»Ÿã€‚

    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™æ–‡æœ¬ï¼š

    {{
    "score": 0-10 çš„æ•´æ•°,
    "pass_flag": true æˆ– false,
    "critique": "ç®€è¦è¯„ä¼°æ„è§"
    }}

    ã€åŸæ–‡ã€‘
    {state.source_text}

    ã€å½“å‰è¯‘æ–‡ã€‘
    {state.combined_translation}

    ã€å›è¯‘ã€‘
    {state.back_translation}
    """
    try:
        eval_res = llm.with_structured_output(QualityReview).invoke(eval_prompt)
        quality_score = eval_res.score
        critique = eval_res.critique
        pass_flag = eval_res.pass_flag
    except Exception as e:
        print(f"âš ï¸  Structured output failed: {e}")
        print("   Using default quality scores...")
        # å›é€€åˆ°é»˜è®¤å€¼
        quality_score = 7.0
        critique = "è¯„ä¼°ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†"
        pass_flag = True
    
    state.quality_score = quality_score
    state.critique = critique
    
    print(f"   >>> Score: {quality_score}/10 | Pass: {pass_flag}")
    return {
        "back_translation": state.back_translation,
        "quality_score": state.quality_score,
        "critique": state.critique
    }
# --- Node D: æŒä¹…åŒ–ä¿å­˜ ---
def node_persistence(state: TranslationState):
    """ä¿å­˜æœ€ç»ˆç¿»è¯‘ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶"""
    
    # æ—¢ç„¶ state æ˜¯ TranslationState ç±»å‹ï¼Œç›´æ¥ç‚¹å·è®¿é—®æœ€å®‰å…¨
    try:
        book_id = state.book_id
        chapter_id = state.chapter_id
        chunk_id = state.chunk_id
        translation = state.combined_translation
        source_text = state.source_text
        quality_score = state.quality_score
        glossary = state.glossary
    except AttributeError:
        # ä¸‡ä¸€ LangGraph ä¼ è¿›æ¥çš„æ˜¯ä¸ª dictï¼ˆé€šå¸¸ä¸ä¼šï¼Œé™¤éé…ç½®æ”¹äº†ï¼‰
        book_id = state.get("book_id")
        chapter_id = state.get("chapter_id")
        chunk_id = state.get("chunk_id")
        translation = state.get("combined_translation")
        source_text = state.get("source_text")
        quality_score = state.get("quality_score")
        glossary = state.get("glossary")

    print(f"ğŸ’¾ [Persistence] Writing data for Chunk {chunk_id}...")

    # è·¯å¾„æ„é€ 
    base_dir = f"./output/{book_id}/chapter_{chapter_id}"
    os.makedirs(base_dir, exist_ok=True)
    
    # å»ºè®® chunk_id æ ¼å¼åŒ–ä¸º 3 ä½æˆ– 4 ä½æ•°å­—ï¼Œæ–¹ä¾¿æ’åº
    file_path = os.path.join(base_dir, f"chunk_{int(chunk_id):03d}.json")
    
    data_to_save = {
        "chunk_id": chunk_id,
        "source_text": source_text,
        "translation": translation,
        "quality_score": quality_score,
        "glossary": glossary
    }

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data_to_save, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“‚ File saved: {file_path}")
    
    # ä¿å­˜ç¿»è¯‘è®°å¿†åˆ°Memoryç³»ç»Ÿ
    try:
        from utils.memory_storage import save_translation_memory
        save_translation_memory(
            book_id=book_id,
            chapter_id=chapter_id,
            chunk_id=chunk_id,
            source_text=source_text,
            translation=translation,
            quality_score=quality_score
        )
        print(f"  âœ… ç¿»è¯‘è®°å¿†å·²ä¿å­˜åˆ°Memoryç³»ç»Ÿ")
    except Exception as e:
        print(f"  âš ï¸  ä¿å­˜ç¿»è¯‘è®°å¿†å¤±è´¥: {e}")
    
    return {"need_human_review": False}
from elasticsearch import Elasticsearch
import hashlib
import json
import os
from datetime import datetime

es = Elasticsearch("http://localhost:9200")
INDEX_NAME = "zh_en_translation_memory"

def retrieve_translation_memory(term: str, top_k: int = 3, include_context: bool = True) -> str:
    """
    ç”¨æœ¯è¯­æ£€ç´¢ç¿»è¯‘è®°å¿†ï¼Œè¿”å›å¯ç›´æ¥å–‚ç»™ LLM çš„æ–‡æœ¬
    
    Args:
        term: æ£€ç´¢å…³é”®è¯ï¼ˆå¯ä»¥æ˜¯æœ¯è¯­æˆ–å¥å­ç‰‡æ®µï¼‰
        top_k: è¿”å›æœ€ç›¸å…³çš„kä¸ªç»“æœ
        include_context: æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ç« èŠ‚ç­‰ï¼‰
    """
    try:
        # æ£€æŸ¥ESè¿æ¥
        if not es.ping():
            return "No relevant translation memory found (ES not available)."
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if not es.indices.exists(index=INDEX_NAME):
            return "No relevant translation memory found (index not exists)."
        
        # æ„å»ºæŸ¥è¯¢
        try:
            # å°è¯•æ–°ç‰ˆæœ¬APIï¼ˆç›´æ¥ä¼ å‚ï¼‰
            resp = es.search(
                index=INDEX_NAME,
                size=top_k,
                query={
                    "multi_match": {
                        "query": term,
                        "fields": ["en^2", "zh", "title^0.5"],  # è‹±æ–‡æƒé‡æ›´é«˜ï¼Œæ ‡é¢˜æƒé‡è¾ƒä½
                        "type": "best_fields",
                        "fuzziness": "AUTO"  # å…è®¸æ¨¡ç³ŠåŒ¹é…
                    }
                }
            )
        except (TypeError, AttributeError):
            # å›é€€åˆ°æ—§ç‰ˆæœ¬APIï¼ˆä½¿ç”¨bodyå‚æ•°ï¼‰
            resp = es.search(
                index=INDEX_NAME,
                size=top_k,
                body={
                    "query": {
                        "multi_match": {
                            "query": term,
                            "fields": ["en^2", "zh", "title^0.5"],
                            "type": "best_fields",
                            "fuzziness": "AUTO"
                        }
                    }
                }
            )

        hits = resp["hits"]["hits"]
        if not hits:
            return "No relevant translation memory found."

        snippets = []
        for h in hits:
            src = h["_source"].get("en", "")
            tgt = h["_source"].get("zh", "")
            
            if include_context:
                # åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
                title = h["_source"].get("title", "")
                source = h["_source"].get("source", "")
                pair_type = h["_source"].get("pair_type", "")
                
                context_info = []
                if title:
                    context_info.append(f"ç« èŠ‚: {title}")
                if source:
                    context_info.append(f"æ¥æº: {source}")
                if pair_type:
                    context_info.append(f"ç±»å‹: {pair_type}")
                
                context_str = f" ({', '.join(context_info)})" if context_info else ""
                snippets.append(f"- {src} â†’ {tgt}{context_str}")
            else:
                snippets.append(f"- {src} â†’ {tgt}")

        return "\n".join(snippets)
    
    except Exception as e:
        print(f"[WARNING] æ£€ç´¢ç¿»è¯‘è®°å¿†å¤±è´¥: {e}")
        return "No relevant translation memory found."


def update_term_to_es(term_dict: dict) -> bool:
    """
    å°†å•ä¸ªæœ¯è¯­æ·»åŠ åˆ°æˆ–æ›´æ–°åˆ°Elasticsearch
    
    Args:
        term_dict: æœ¯è¯­å­—å…¸ï¼ŒåŒ…å« 'src' (è‹±æ–‡) å’Œ 'suggested_trans' (ä¸­æ–‡) ç­‰å­—æ®µ
    
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        en_text = term_dict.get('src', '').strip()
        zh_text = term_dict.get('suggested_trans', '').strip()
        
        if not en_text or not zh_text:
            print(f"[WARNING] æœ¯è¯­æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡: {term_dict}")
            return False
        
        # ç”Ÿæˆæ–‡æ¡£IDï¼ˆåŸºäºè‹±æ–‡æ–‡æœ¬çš„SHA1å“ˆå¸Œï¼‰
        doc_id = hashlib.sha1(en_text.encode('utf-8')).hexdigest()
        
        # æ„å»ºæ–‡æ¡£å†…å®¹
        doc = {
            "en": en_text,
            "zh": zh_text
        }
        
        # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'type' in term_dict:
            doc['term_type'] = term_dict['type']
        if 'rationale' in term_dict:
            doc['rationale'] = term_dict['rationale']
        if 'human_reviewed' in term_dict:
            doc['human_reviewed'] = term_dict['human_reviewed']
        if 'human_modified' in term_dict:
            doc['human_modified'] = term_dict['human_modified']
        if 'reviewed_at' in term_dict:
            doc['reviewed_at'] = term_dict['reviewed_at']
        
        # ä½¿ç”¨ upsert æ“ä½œï¼ˆå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰
        # Elasticsearch 7.x+ ä½¿ç”¨ body å‚æ•°ï¼Œ8.x+ å¯ä»¥ç›´æ¥ä¼ å‚
        try:
            # å°è¯•æ–°ç‰ˆæœ¬APIï¼ˆç›´æ¥ä¼ å‚ï¼‰
            response = es.update(
                index=INDEX_NAME,
                id=doc_id,
                doc=doc,
                doc_as_upsert=True
            )
        except TypeError:
            # å›é€€åˆ°æ—§ç‰ˆæœ¬APIï¼ˆä½¿ç”¨bodyå‚æ•°ï¼‰
            response = es.update(
                index=INDEX_NAME,
                id=doc_id,
                body={
                    "doc": doc,
                    "doc_as_upsert": True
                }
            )
        
        return response.get('result') in ['created', 'updated']
        
    except Exception as e:
        print(f"[WARNING] æ›´æ–°æœ¯è¯­åˆ°ESå¤±è´¥: {e}")
        return False


def batch_update_terms_to_es(terms: list[dict]) -> dict:
    """
    æ‰¹é‡å°†æœ¯è¯­æ·»åŠ åˆ°æˆ–æ›´æ–°åˆ°Elasticsearch
    
    Args:
        terms: æœ¯è¯­åˆ—è¡¨ï¼Œæ¯ä¸ªæœ¯è¯­åŒ…å« 'src' å’Œ 'suggested_trans' ç­‰å­—æ®µ
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯ï¼š{"success": æˆåŠŸæ•°é‡, "failed": å¤±è´¥æ•°é‡, "total": æ€»æ•°}
    """
    if not terms:
        return {"success": 0, "failed": 0, "total": 0}
    
    success_count = 0
    failed_count = 0
    
    print(f"\nğŸ“¤ å¼€å§‹æ‰¹é‡æ›´æ–° {len(terms)} ä¸ªæœ¯è¯­åˆ°Elasticsearch...")
    
    for term in terms:
        if update_term_to_es(term):
            success_count += 1
        else:
            failed_count += 1
    
    result = {
        "success": success_count,
        "failed": failed_count,
        "total": len(terms)
    }
    
    print(f"âˆš ESæ›´æ–°å®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªï¼Œæ€»è®¡ {len(terms)} ä¸ª")
    
    return result


def export_rag_data_to_file(output_dir: str = "output/rag_backups") -> str:
    """
    å¯¼å‡ºElasticsearchä¸­çš„æ‰€æœ‰RAGæ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    try:
        # æ£€æŸ¥ESè¿æ¥
        if not es.ping():
            print(f"  [WARNING] æ— æ³•è¿æ¥åˆ°Elasticsearchï¼Œè·³è¿‡RAGæ•°æ®å¯¼å‡º")
            return ""
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if not es.indices.exists(index=INDEX_NAME):
            print(f"  [WARNING] ç´¢å¼• {INDEX_NAME} ä¸å­˜åœ¨ï¼Œè·³è¿‡RAGæ•°æ®å¯¼å‡º")
            return ""
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼šå¹´æœˆæ—¥æ—¶åˆ†ç§’æ ¼å¼
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        filename = f"rag_backup_{timestamp}.json"
        file_path = os.path.join(output_dir, filename)
        
        # ä½¿ç”¨scroll APIè·å–æ‰€æœ‰æ•°æ®ï¼ˆå¤„ç†å¤§é‡æ•°æ®ï¼‰
        all_docs = []
        scroll_size = 1000  # æ¯æ¬¡è·å–1000æ¡
        scroll_id = None
        
        try:
            # åˆå§‹æœç´¢ - å°è¯•æ–°ç‰ˆæœ¬APIï¼ˆç›´æ¥ä¼ å‚ï¼‰
            try:
                response = es.search(
                    index=INDEX_NAME,
                    query={"match_all": {}},
                    size=scroll_size,
                    scroll='2m'
                )
            except (TypeError, AttributeError):
                # å›é€€åˆ°æ—§ç‰ˆæœ¬APIï¼ˆä½¿ç”¨bodyå‚æ•°ï¼‰
                response = es.search(
                    index=INDEX_NAME,
                    body={
                        "query": {"match_all": {}},
                        "size": scroll_size
                    },
                    scroll='2m'
                )
            
            # è·å–ç¬¬ä¸€æ‰¹æ•°æ®
            scroll_id = response.get('_scroll_id')
            hits = response['hits']['hits']
            all_docs.extend([hit['_source'] for hit in hits])
            
            # ç»§ç»­æ»šåŠ¨è·å–å‰©ä½™æ•°æ®
            while len(hits) > 0:
                try:
                    # å°è¯•æ–°ç‰ˆæœ¬API
                    try:
                        response = es.scroll(
                            scroll_id=scroll_id,
                            scroll='2m'
                        )
                    except (TypeError, AttributeError):
                        # å›é€€åˆ°æ—§ç‰ˆæœ¬API
                        response = es.scroll(
                            scroll_id=scroll_id,
                            scroll='2m'
                        )
                    
                    scroll_id = response.get('_scroll_id')
                    hits = response['hits']['hits']
                    all_docs.extend([hit['_source'] for hit in hits])
                except Exception as scroll_error:
                    print(f"  [WARNING] Scrollè·å–æ•°æ®æ—¶å‡ºé”™: {scroll_error}")
                    break
            
            # æ¸…ç†scrollä¸Šä¸‹æ–‡
            if scroll_id:
                try:
                    es.clear_scroll(scroll_id=scroll_id)
                except:
                    pass
            
        except Exception as search_error:
            print(f"  [WARNING] æœç´¢ESæ•°æ®æ—¶å‡ºé”™: {search_error}")
            # å¦‚æœscrollå¤±è´¥ï¼Œå°è¯•ç®€å•æœç´¢ï¼ˆä»…é€‚ç”¨äºæ•°æ®é‡å°çš„æƒ…å†µï¼‰
            try:
                response = es.search(
                    index=INDEX_NAME,
                    body={"query": {"match_all": {}}, "size": 10000}
                )
                all_docs = [hit['_source'] for hit in response['hits']['hits']]
            except:
                raise search_error
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        if all_docs:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(all_docs, f, ensure_ascii=False, indent=2)
            
            print(f"  RAGæ•°æ®å·²å¯¼å‡º: {file_path} (å…± {len(all_docs)} æ¡è®°å½•)")
            return file_path
        else:
            print(f"  [WARNING] æœªæ‰¾åˆ°ä»»ä½•RAGæ•°æ®")
            return ""
        
    except Exception as e:
        print(f"  [WARNING] å¯¼å‡ºRAGæ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return ""
